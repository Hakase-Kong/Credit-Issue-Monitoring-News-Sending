import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

import streamlit as st
import requests
import re
import os
from datetime import datetime
import telepot
from openai import OpenAI
import newspaper  # newspaper4k

# --- ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” (í•­ìƒ ìœ„ì ¯ë³´ë‹¤ ë¨¼ì €!) ---
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def extract_article_text(url):
    try:
        article = newspaper.article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

def summarize_and_sentiment_with_openai(text):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            "ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ìš”ì•½í•˜ê³  ê°ì„±ë¶„ì„ì„ í•´ì¤˜.\n\n"
            "- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n"
            "- [ìš”ì•½ë³¸]: ê¸°ì‚¬ ë‚´ìš©ì„ 2~3 ë¬¸ë‹¨(ê° ë¬¸ë‹¨ 2~4ë¬¸ì¥)ìœ¼ë¡œ, í•µì‹¬ ë‚´ìš©ì„ ì¶©ë¶„íˆ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ìš”ì•½\n"
            "- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. "
            "ë§Œì•½ íŒŒì‚°, ìê¸ˆë‚œ, íšŒìƒ, ì ì, êµ¬ì¡°ì¡°ì •, ì˜ì—…ì†ì‹¤, ë¶€ë„, ì±„ë¬´ë¶ˆì´í–‰, ê²½ì˜ ìœ„ê¸° ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n"
            "ê´‘ê³ , ë°°ë„ˆ, ì¶”ì²œê¸°ì‚¬, ì„œë¹„ìŠ¤ ì•ˆë‚´ ë“± ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì€ ëª¨ë‘ ìš”ì•½ê³¼ ê°ì„±ë¶„ì„ì—ì„œ ì œì™¸.\n\n"
            "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n"
            "[ìš”ì•½ë³¸]: (ì—¬ê¸°ì— ì—¬ëŸ¬ ë¬¸ë‹¨ ìš”ì•½)\n"
            "[ê°ì„±]: (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
            "[ê¸°ì‚¬ ë³¸ë¬¸]\n" + text
        )
    else:
        prompt = (
            "Summarize the following news article and analyze its sentiment.\n\n"
            "- [One-line Summary]: Summarize the entire article in one sentence.\n"
            "- [Summary]: Summarize the article in 2â€“3 paragraphs (each 2â€“4 sentences), so that the main content is well understood.\n"
            "- [Sentiment]: Classify the overall sentiment as one of: positive, negative, or neutral. "
            "If the article centers on bankruptcy, financial distress, restructuring, insolvency, operating loss, default, or management crisis, you must answer 'negative'.\n"
            "Exclude any advertisements, banners, recommended articles, or unrelated content.\n\n"
            "Respond in this format:\n"
            "[One-line Summary]: (your one-line summary)\n"
            "[Summary]: (your multi-paragraph summary)\n"
            "[Sentiment]: (positive/negative/neutral only)\n\n"
            "[ARTICLE]\n" + text
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m2 = re.search(r"\[ìš”ì•½ë³¸\]:\s*([\s\S]+?)(?:\[ê°ì„±\]:|$)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m2 = re.search(r"\[Summary\]:\s*([\s\S]+?)(?:\[Sentiment\]:|$)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if m1 else ""
    summary = m2.group(1).strip() if m2 else answer
    sentiment = m3.group(1).strip() if m3 else ""
    return one_line, summary, sentiment, text

# --- ëŒ€ë¶„ë¥˜(ì‚°ì—…) & ì†Œë¶„ë¥˜(í•„í„° í‚¤ì›Œë“œ) êµ¬ì¡° ---
favorite_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": ["í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ"],
    "5ëŒ€ì‹œì¤‘ì€í–‰": ["ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"],
    "ì¹´ë“œì‚¬": ["KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"],
    "ìºí”¼íƒˆ": ["í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": ["SKì´ë…¸ë² ì´ì…˜", "GSì—ë„ˆì§€", "SK", "GS"],
    "ì—ë„ˆì§€": ["SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "SKì•¤ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"],
    "ë°œì „": ["GSíŒŒì›Œ", "GSEPS", "ì‚¼ì²œë¦¬"],
    "ìë™ì°¨": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´"],
    "ì „ê¸°/ì „ì": ["SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "LSì¼ë ‰íŠ¸ë¦­"],
    "ì†Œë¹„ì¬": ["ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"],
    "ë¹„ì² /ì² ê°•": ["í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"],
    "ì„ìœ í™”í•™": ["LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"],
    "ê±´ì„¤": ["í¬ìŠ¤ì½”ì´ì•¤ì”¨"],
    "íŠ¹ìˆ˜ì±„": ["ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"]
}
major_categories = list(favorite_categories.keys())
sub_categories = {cat: favorite_categories[cat] for cat in major_categories}

# --- UI: í‚¤ì›Œë“œ ì…ë ¥ì°½ ---
st.set_page_config(layout="wide")
st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)
col1, col2, col3 = st.columns([6, 1, 1])
with col1:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="", on_change=lambda: st.session_state.__setitem__('search_triggered', True))
with col2:
    st.write("")
    search_clicked = st.button("ê²€ìƒ‰", use_container_width=True)
with col3:
    st.write("")
    fav_add_clicked = st.button("â­ ì¦ê²¨ì°¾ê¸° ì¶”ê°€", use_container_width=True)
    if fav_add_clicked:
        new_keywords = {kw.strip() for kw in keywords_input.split(",") if kw.strip()}
        st.session_state.favorite_keywords.update(new_keywords)
        st.success("ì¦ê²¨ì°¾ê¸°ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ (í‚¤ì›Œë“œ ì…ë ¥ì°½ ë°”ë¡œ ì•„ë˜) ---
st.markdown("**ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ**")
cat_col, btn_col = st.columns([5, 1])
with cat_col:
    selected_categories = st.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ì¦ê²¨ì°¾ê¸° í‚¤ì›Œë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.", major_categories)
    for cat in selected_categories:
        st.session_state.favorite_keywords.update(favorite_categories[cat])
with btn_col:
    st.write("")
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", use_container_width=True)

# --- ì¦ê²¨ì°¾ê¸°ì—ì„œ ê²€ìƒ‰ (ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ ë°”ë¡œ ì•„ë˜) ---
fav_col1, fav_col2 = st.columns([5, 1])
with fav_col1:
    fav_selected = st.multiselect("â­ ì¦ê²¨ì°¾ê¸°ì—ì„œ ê²€ìƒ‰", sorted(st.session_state.favorite_keywords))
with fav_col2:
    st.write("")
    fav_search_clicked = st.button("ì¦ê²¨ì°¾ê¸°ë¡œ ê²€ìƒ‰", use_container_width=True)

# --- ë‚ ì§œ ì…ë ¥ ---
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼")
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼")

# --- ì‹ ìš©ìœ„í—˜ í•„í„° ì˜µì…˜ ---
with st.expander("ğŸ›¡ï¸ ì‹ ìš©ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    credit_keywords = [
        "ì‹ ìš©ë“±ê¸‰", "ì‹ ìš©í‰ê°€", "í•˜í–¥", "ìƒí–¥", "ê°•ë“±", "ì¡°ì •", "ë¶€ë„",
        "íŒŒì‚°", "ë””í´íŠ¸", "ì±„ë¬´ë¶ˆì´í–‰", "ì ì", "ì˜ì—…ì†ì‹¤", "í˜„ê¸ˆíë¦„", "ìê¸ˆë‚œ",
        "ì¬ë¬´ìœ„í—˜", "ë¶€ì •ì  ì „ë§", "ê¸ì •ì  ì „ë§", "ê¸°ì—…íšŒìƒ", "ì›Œí¬ì•„ì›ƒ", "êµ¬ì¡°ì¡°ì •", "ìë³¸ì ì‹"
    ]
    credit_filter_keywords = st.multiselect(
        "ì‹ ìš©ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ (í•˜ë‚˜ ì´ìƒ ì„ íƒ)",
        options=credit_keywords,
        default=credit_keywords,
        key="credit_filter"
    )

# --- í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ (ê¸°ë³¸ í•´ì œ) ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜", expanded=True):
    require_keyword_in_title = st.checkbox("ê¸°ì‚¬ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë³´ê¸°", value=False)

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ë°•ìŠ¤í˜•íƒœ, í•œ ì¤„ì— ë°°ì¹˜, íƒœê·¸ UI) ---
with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜", expanded=True):
    col_major, col_sub = st.columns([1, 2])
    with col_major:
        selected_major = st.selectbox("ëŒ€ë¶„ë¥˜(ì‚°ì—…)", major_categories, key="industry_major")
    with col_sub:
        selected_sub = st.multiselect(
            "ì†Œë¶„ë¥˜(í•„í„° í‚¤ì›Œë“œ)",
            sub_categories[selected_major],
            key="industry_sub"
        )

# --- ì¬ë¬´ìœ„í—˜ í•„í„° ì˜µì…˜ ---
with st.expander("ğŸ’° ì¬ë¬´ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    finance_keywords = ["ìì‚°", "ì´ìì‚°", "ë¶€ì±„", "ìë³¸", "ë§¤ì¶œ", "ë¹„ìš©", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ"]
    finance_filter_keywords = st.multiselect(
        "ì¬ë¬´ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ",
        options=finance_keywords,
        default=[],
        key="finance_filter"
    )

# --- ë²•/ì •ì±… ìœ„í—˜ í•„í„° ì˜µì…˜ ---
with st.expander("âš–ï¸ ë²•/ì •ì±… ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    law_keywords = ["í…ŒìŠ¤íŠ¸1", "í…ŒìŠ¤íŠ¸2", "í…ŒìŠ¤íŠ¸3"]
    law_filter_keywords = st.multiselect(
        "ë²•/ì •ì±… ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ",
        options=law_keywords,
        default=[],
        key="law_filter"
    )

# --- CSS: ë¶‰ì€ìƒ‰ íƒœê·¸ ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
.stMultiSelect [data-baseweb="tag"] {
    background-color: #ff5c5c !important;
    color: white !important;
    border: none !important;
    font-weight: bold;
}
</style>
""", unsafe_allow_html=True)

# --- OR ì¡°ê±´ í•„í„°ë§ í•¨ìˆ˜ ---
def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

# --- í…”ë ˆê·¸ë¨ í´ë˜ìŠ¤ ---
NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

# --- ë‰´ìŠ¤ API í•¨ìˆ˜ (ë„¤ì´ë²„/GNews) ---
def filter_by_issues(title, desc, selected_keywords, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    if enable_credit_filter and not is_credit_risk_news(title + " " + desc, credit_filter_keywords):
        return False
    return True

def is_credit_risk_news(text, keywords):
    return any(kw in text for kw in keywords)

def fetch_naver_news(query, start_date=None, end_date=None, enable_credit_filter=True, credit_filter_keywords=None, limit=100, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 6):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 10,
            "start": (page - 1) * 10 + 1,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], enable_credit_filter, credit_filter_keywords, require_keyword_in_title):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
    return articles[:limit]

def fetch_gnews_news(query, enable_credit_filter=True, credit_filter_keywords=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], enable_credit_filter, credit_filter_keywords, require_keyword_in_title):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def summarize_article_from_url(article_url, title):
    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

def render_articles_with_single_summary_and_telegram(results, show_limit):
    all_articles = []
    article_keys = []
    for keyword, articles in results.items():
        for idx, article in enumerate(articles[:show_limit.get(keyword, 5)]):
            all_articles.append(f"[{keyword}] {article['title']} ({article['date']} | {article['source']})")
            article_keys.append((keyword, idx))

    if not all_articles:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected_idx = st.radio("ìš”ì•½/ê°ì„±ë¶„ì„/í…”ë ˆê·¸ë¨ ì „ì†¡í•  ê¸°ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.", range(len(all_articles)), format_func=lambda i: all_articles[i], key="article_selector")
    selected_keyword, selected_article_idx = article_keys[selected_idx]
    selected_article = st.session_state.search_results[selected_keyword][selected_article_idx]

    st.markdown(f"""
    <div style='margin-bottom: 10px; padding: 10px; border: 1px solid #eee; border-radius: 10px; background-color: #fafafa;'>
        <div style='font-weight: bold; font-size: 15px; margin-bottom: 4px;'>
            <a href="{selected_article['link']}" target="_blank" style='text-decoration: none; color: #1155cc;'>
                {selected_article['title']}
            </a>
        </div>
        <div style='font-size: 12px; color: gray;'>
            {selected_article['date']} | {selected_article['source']}
        </div>
    </div>
    """, unsafe_allow_html=True)

    if st.button("ğŸ” ì„ íƒ ê¸°ì‚¬ ìš”ì•½ ë° ê°ì„±ë¶„ì„"):
        with st.spinner("ê¸°ì‚¬ ìš”ì•½ ë° ê°ì„±ë¶„ì„ ì¤‘..."):
            one_line, summary, sentiment, full_text = summarize_article_from_url(selected_article['link'], selected_article['title'])
            if full_text:
                st.markdown("**[í•œ ì¤„ ìš”ì•½]**")
                st.write(one_line)
                st.markdown("**[ìš”ì•½ë³¸]**")
                st.write(summary)
                st.markdown(f"**[ê°ì„± ë¶„ì„]**: :red[{sentiment}]")
            else:
                st.warning(one_line)

    if st.button("âœˆï¸ ì„ íƒ ê¸°ì‚¬ í…”ë ˆê·¸ë¨ ì „ì†¡"):
        try:
            msg = f"*[{selected_article['title']}]({selected_article['link']})*\n{selected_article['date']} | {selected_article['source']}"
            Telegram().send_message(msg)
            st.success("í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except Exception as e:
            st.warning(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜¤ë¥˜: {e}")

# --- ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰/í•„í„°ë§/ìš”ì•½/ê°ì„±ë¶„ì„ ì‹¤í–‰ ---
search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, True, credit_filter_keywords, require_keyword_in_title)
    st.session_state.search_triggered = False

if fav_search_clicked and fav_selected:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        process_keywords(fav_selected, start_date, end_date, True, credit_filter_keywords, require_keyword_in_title)

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords(
            sorted(keywords),
            start_date,
            end_date,
            True,
            credit_filter_keywords,
            require_keyword_in_title
        )

# --- í•„í„°ë§: OR ì¡°ê±´(ëª¨ë“  í•„í„° ì˜µì…˜ í•©ì‚°) ---
def article_passes_all_filters(article):
    return or_keyword_filter(
        article,
        credit_filter_keywords,
        selected_sub,
        finance_filter_keywords,
        law_filter_keywords
    )

if st.session_state.search_results:
    # OR ì¡°ê±´ í•„í„°ë§ ì ìš©
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(filtered_results, st.session_state.show_limit)
