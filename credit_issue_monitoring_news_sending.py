import nltk

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
import os
from datetime import datetime
import telepot
from openai import OpenAI
import newspaper  # newspaper4k
import openpyxl
from openpyxl import load_workbook

# --- CSS: ì²´í¬ë°•ìŠ¤ì™€ ê¸°ì‚¬ ì‚¬ì´ gap ìµœì†Œí™” ---
st.markdown("""
<style>
[data-testid="column"] > div {
    gap: 0rem !important;
}
.stMultiSelect [data-baseweb="tag"] {
    background-color: #ff5c5c !important;
    color: white !important;
    border: none !important;
    font-weight: bold;
}
.sentiment-badge {
    display: inline-block;
    padding: 0.08em 0.6em;
    margin-left: 0.2em;
    border-radius: 0.8em;
    font-size: 0.85em;
    font-weight: bold;
    vertical-align: middle;
}
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-neutral { background: #0074d9; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False

# ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì¹´í…Œê³ ë¦¬
favorite_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": ["í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ"],
    "5ëŒ€ì‹œì¤‘ì€í–‰": ["ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"],
    "ì¹´ë“œì‚¬": ["KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"],
    "ìºí”¼íƒˆ": ["í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": ["SKì´ë…¸ë² ì´ì…˜", "GSì—ë„ˆì§€", "SK", "GS"],
    "ì—ë„ˆì§€": ["SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "SKì•¤ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"],
    "ë°œì „": ["GSíŒŒì›Œ", "GSEPS", "ì‚¼ì²œë¦¬"],
    "ìë™ì°¨": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´"],
    "ì „ê¸°/ì „ì": ["SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "LSì¼ë ‰íŠ¸ë¦­"],
    "ì†Œë¹„ì¬": ["ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"],
    "ë¹„ì² /ì² ê°•": ["í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"],
    "ì„ìœ í™”í•™": ["LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"],
    "ê±´ì„¤": ["í¬ìŠ¤ì½”ì´ì•¤ì”¨"],
    "íŠ¹ìˆ˜ì±„": ["ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"]
}
major_categories = list(favorite_categories.keys())
sub_categories = {cat: favorite_categories[cat] for cat in major_categories}
all_fav_keywords = sorted(set(
    kw for cat in favorite_categories.values() for kw in cat if kw not in ["í…ŒìŠ¤íŠ¸1", "í…ŒìŠ¤íŠ¸2", "í…ŒìŠ¤íŠ¸3"]
))

st.set_page_config(layout="wide")
st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)

# -- ê²€ìƒ‰ì°½/ê²€ìƒ‰ ë²„íŠ¼ í•œ ì¤„ ë°°ì¹˜
search_col, button_col = st.columns([7, 1])
with search_col:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="", key="keyword_input")
with button_col:
    search_clicked = st.button("ê²€ìƒ‰", use_container_width=True)

# -- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ/ê²€ìƒ‰ ë²„íŠ¼ í•œ ì¤„ ë°°ì¹˜
st.markdown("**â­ ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ**")
cat_col, btn_col = st.columns([5, 1])
with cat_col:
    selected_categories = st.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ì¦ê²¨ì°¾ê¸° í‚¤ì›Œë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.", major_categories)
    for cat in selected_categories:
        st.session_state.favorite_keywords.update(favorite_categories[cat])
with btn_col:
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", use_container_width=True)

# -- ì¦ê²¨ì°¾ê¸°ì—ì„œ ê²€ìƒ‰/ë²„íŠ¼ í•œ ì¤„ ë°°ì¹˜
fav_col, fav_btn_col = st.columns([5, 1])
with fav_col:
    fav_selected = st.multiselect("â­ ì¦ê²¨ì°¾ê¸°ì—ì„œ ê²€ìƒ‰", all_fav_keywords, default=[])
with fav_btn_col:
    fav_search_clicked = st.button("â­ ì¦ê²¨ì°¾ê¸°ë¡œ ê²€ìƒ‰", use_container_width=True)

# ë‚ ì§œ ì…ë ¥
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼")
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼")

# ì‹ ìš©ìœ„í—˜ í•„í„° ì˜µì…˜
with st.expander("ğŸ›¡ï¸ ì‹ ìš©ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    use_credit_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_credit_filter")
    credit_keywords = [
        "ì‹ ìš©ë“±ê¸‰", "ì‹ ìš©í‰ê°€", "í•˜í–¥", "ìƒí–¥", "ê°•ë“±", "ì¡°ì •", "ë¶€ë„",
        "íŒŒì‚°", "ë””í´íŠ¸", "ì±„ë¬´ë¶ˆì´í–‰", "ì ì", "ì˜ì—…ì†ì‹¤", "í˜„ê¸ˆíë¦„", "ìê¸ˆë‚œ",
        "ì¬ë¬´ìœ„í—˜", "ë¶€ì •ì  ì „ë§", "ê¸ì •ì  ì „ë§", "ê¸°ì—…íšŒìƒ", "ì›Œí¬ì•„ì›ƒ", "êµ¬ì¡°ì¡°ì •", "ìë³¸ì ì‹"
    ]
    credit_filter_keywords = st.multiselect(
        "ì‹ ìš©ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ (í•˜ë‚˜ ì´ìƒ ì„ íƒ)",
        options=credit_keywords,
        default=credit_keywords,
        key="credit_filter"
    )

# í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ (ê¸°ë³¸ í•´ì œ)
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜", expanded=True):
    require_keyword_in_title = st.checkbox("ê¸°ì‚¬ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë³´ê¸°", value=False)

# ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ë°•ìŠ¤í˜•íƒœ, í•œ ì¤„ì— ë°°ì¹˜, íƒœê·¸ UI, ëª¨ë‘ ì„ íƒ, ì²´í¬ë°•ìŠ¤)
with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜", expanded=True):
    use_industry_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_industry_filter")
    col_major, col_sub = st.columns([1, 2])
    with col_major:
        selected_major = st.selectbox("ëŒ€ë¶„ë¥˜(ì‚°ì—…)", major_categories, key="industry_major")
    with col_sub:
        selected_sub = st.multiselect(
            "ì†Œë¶„ë¥˜(í•„í„° í‚¤ì›Œë“œ)",
            sub_categories[selected_major],
            default=sub_categories[selected_major],
            key="industry_sub"
        )

# ì¬ë¬´ìœ„í—˜ í•„í„° ì˜µì…˜ (ëª¨ë‘ ì„ íƒ, ì²´í¬ë°•ìŠ¤)
with st.expander("ğŸ’° ì¬ë¬´ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    use_finance_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_finance_filter")
    finance_keywords = ["ìì‚°", "ì´ìì‚°", "ë¶€ì±„", "ìë³¸", "ë§¤ì¶œ", "ë¹„ìš©", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ"]
    finance_filter_keywords = st.multiselect(
        "ì¬ë¬´ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ",
        options=finance_keywords,
        default=finance_keywords,
        key="finance_filter"
    )

# ë²•/ì •ì±… ìœ„í—˜ í•„í„° ì˜µì…˜ (ëª¨ë‘ ì„ íƒ, ì²´í¬ë°•ìŠ¤)
with st.expander("âš–ï¸ ë²•/ì •ì±… ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    use_law_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_law_filter")
    law_keywords = ["í…ŒìŠ¤íŠ¸1", "í…ŒìŠ¤íŠ¸2", "í…ŒìŠ¤íŠ¸3"]
    law_filter_keywords = st.multiselect(
        "ë²•/ì •ì±… ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ",
        options=law_keywords,
        default=law_keywords,
        key="law_filter"
    )

# --- ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜(ìš”ì²­ëŒ€ë¡œ ë‹¨ìˆœí™”) ---
def extract_article_text(url):
    try:
        article = newspaper.article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

# --- OpenAI ìš”ì•½/ê°ì„±ë¶„ì„ í•¨ìˆ˜ ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def summarize_and_sentiment_with_openai(text):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            "ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ìš”ì•½í•˜ê³  ê°ì„±ë¶„ì„ì„ í•´ì¤˜.\n\n"
            "- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n"
            "- [ìš”ì•½ë³¸]: ê¸°ì‚¬ ë‚´ìš©ì„ 2~3 ë¬¸ë‹¨(ê° ë¬¸ë‹¨ 2~4ë¬¸ì¥)ìœ¼ë¡œ, í•µì‹¬ ë‚´ìš©ì„ ì¶©ë¶„íˆ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ìš”ì•½\n"
            "- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. "
            "ë§Œì•½ íŒŒì‚°, ìê¸ˆë‚œ, íšŒìƒ, ì ì, êµ¬ì¡°ì¡°ì •, ì˜ì—…ì†ì‹¤, ë¶€ë„, ì±„ë¬´ë¶ˆì´í–‰, ê²½ì˜ ìœ„ê¸° ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n"
            "ê´‘ê³ , ë°°ë„ˆ, ì¶”ì²œê¸°ì‚¬, ì„œë¹„ìŠ¤ ì•ˆë‚´ ë“± ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì€ ëª¨ë‘ ìš”ì•½ê³¼ ê°ì„±ë¶„ì„ì—ì„œ ì œì™¸.\n\n"
            "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n"
            "[ìš”ì•½ë³¸]: (ì—¬ê¸°ì— ì—¬ëŸ¬ ë¬¸ë‹¨ ìš”ì•½)\n"
            "[ê°ì„±]: (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
            "[ê¸°ì‚¬ ë³¸ë¬¸]\n" + text
        )
    else:
        prompt = (
            "Summarize the following news article and analyze its sentiment.\n\n"
            "- [One-line Summary]: Summarize the entire article in one sentence.\n"
            "- [Summary]: Summarize the article in 2â€“3 paragraphs (each 2â€“4 sentences), so that the main content is well understood.\n"
            "- [Sentiment]: Classify the overall sentiment as one of: positive, negative, or neutral. "
            "If the article centers on bankruptcy, financial distress, restructuring, insolvency, operating loss, default, or management crisis, you must answer 'negative'.\n"
            "Exclude any advertisements, banners, recommended articles, or unrelated content.\n\n"
            "Respond in this format:\n"
            "[One-line Summary]: (your one-line summary)\n"
            "[Summary]: (your multi-paragraph summary)\n"
            "[Sentiment]: (positive/negative/neutral only)\n\n"
            "[ARTICLE]\n" + text
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m2 = re.search(r"\[ìš”ì•½ë³¸\]:\s*([\s\S]+?)(?:\[ê°ì„±\]:|$)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m2 = re.search(r"\[Summary\]:\s*([\s\S]+?)(?:\[Sentiment\]:|$)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if m1 else ""
    summary = m2.group(1).strip() if m2 else answer
    sentiment = m3.group(1).strip() if m3 else ""
    return one_line, summary, sentiment, text

# --- í…”ë ˆê·¸ë¨ í´ë˜ìŠ¤ ---
NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

# --- ë‰´ìŠ¤ API í•¨ìˆ˜ (ë„¤ì´ë²„/GNews) ---
def filter_by_issues(title, desc, selected_keywords, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    if enable_credit_filter and not is_credit_risk_news(title + " " + desc, credit_filter_keywords):
        return False
    return True

def is_credit_risk_news(text, keywords):
    return any(kw in text for kw in keywords)

def fetch_naver_news(query, start_date=None, end_date=None, enable_credit_filter=True, credit_filter_keywords=None, limit=100, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 6):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 10,
            "start": (page - 1) * 10 + 1,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], use_credit_filter, credit_filter_keywords, require_keyword_in_title):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
    return articles[:limit]

def fetch_gnews_news(query, enable_credit_filter=True, credit_filter_keywords=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], use_credit_filter, credit_filter_keywords, require_keyword_in_title):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def summarize_article_from_url(article_url, title):
    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

# OR ì¡°ê±´ í•„í„°ë§ í•¨ìˆ˜
def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

# ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰/í•„í„°ë§/ìš”ì•½/ê°ì„±ë¶„ì„ ì‹¤í–‰
search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, use_credit_filter, credit_filter_keywords, require_keyword_in_title)
    st.session_state.search_triggered = False

if fav_search_clicked and fav_selected:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        process_keywords(fav_selected, start_date, end_date, use_credit_filter, credit_filter_keywords, require_keyword_in_title)

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords(
            sorted(keywords),
            start_date,
            end_date,
            use_credit_filter,
            credit_filter_keywords,
            require_keyword_in_title
        )

# í•„í„°ë§: OR ì¡°ê±´(í•„í„°ë³„ ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ ì ìš©)
def article_passes_all_filters(article):
    filters = []
    if use_credit_filter:
        filters.append(credit_filter_keywords)
    if use_industry_filter:
        filters.append(selected_sub)
    if use_finance_filter:
        filters.append(finance_filter_keywords)
    if use_law_filter:
        filters.append(law_filter_keywords)
    if filters:
        return or_keyword_filter(article, *filters)
    else:
        return True

# --- ì—‘ì…€ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (openpyxl) ---
def update_excel(selected_data, template_path):
    wb = load_workbook(template_path)
    ws = wb.active
    # íšŒì‚¬ëª… â†’ í–‰ë²ˆí˜¸ ë§¤í•‘ (ì—‘ì…€ì˜ 3í–‰ë¶€í„° ë°ì´í„° ì‹œì‘, íšŒì‚¬ëª…ì€ Dì—´(5ë²ˆì§¸))
    company_col = 4  # Dì—´(0ë¶€í„° ì‹œì‘)
    company_to_row = {}
    for row in range(3, ws.max_row + 1):
        name = ws.cell(row=row, column=company_col).value
        if name:
            company_to_row[name.replace(" ", "")] = row
    # J(10), L(12)ì—´ì— í•˜ì´í¼ë§í¬ ì—…ë°ì´íŠ¸
    for item in selected_data:
        name = item["íšŒì‚¬ëª…"].replace(" ", "")
        if name in company_to_row and item["ìš”ì•½"] and item["ë§í¬"]:
            if item["sentiment"] == "ê¸ì •":
                cell = ws.cell(row=company_to_row[name], column=10)  # Jì—´
            elif item["sentiment"] == "ë¶€ì •":
                cell = ws.cell(row=company_to_row[name], column=12)  # Lì—´
            else:
                continue
            cell.value = item["ìš”ì•½"]
            cell.hyperlink = item["ë§í¬"]
            cell.style = "Hyperlink"
    output = BytesIO()
    wb.save(output)
    output.seek(0)
    return output

# --- ìš”ì•½/ê°ì„±ë¶„ì„/ê¸°ì‚¬ì„ íƒ/ì—‘ì…€ ì €ì¥ UI ---
def render_articles_with_single_summary_and_telegram(results, show_limit):
    SENTIMENT_CLASS = {
        "ê¸ì •": "sentiment-positive",
        "ë¶€ì •": "sentiment-negative",
        "ì¤‘ë¦½": "sentiment-neutral"
    }
    summary_data = []
    checked_list = []

    # ì„ íƒ ìƒíƒœë¥¼ ì„¸ì…˜ì— ì €ì¥ (ì²´í¬ë°•ìŠ¤ ìƒíƒœ ìœ ì§€)
    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    # 2ë‹¨ ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ (ì™¼ìª½: ê¸°ì‚¬ë¦¬ìŠ¤íŠ¸, ì˜¤ë¥¸ìª½: ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±)
    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### ê¸°ì‚¬ ìš”ì•½ ê²°ê³¼ (ì—‘ì…€ ì €ì¥í•  ê¸°ì‚¬ ì„ íƒ)")
        for keyword, articles in results.items():
            for idx, article in enumerate(articles[:show_limit.get(keyword, 5)]):
                key = f"{keyword}_{idx}"
                cache_key = f"summary_{key}"
                # ê°ì„±ë¶„ì„ ìºì‹± (ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì— ë°”ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´)
                if cache_key not in st.session_state:
                    one_line, summary, sentiment, full_text = summarize_article_from_url(article['link'], article['title'])
                    st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                else:
                    one_line, summary, sentiment, full_text = st.session_state[cache_key]
                sentiment_label = sentiment if sentiment else "ë¶„ì„ì¤‘"
                sentiment_class = SENTIMENT_CLASS.get(sentiment_label, "sentiment-neutral")
                # ê¸°ì‚¬ ì œëª© ì˜†ì— ê°ì„± ê²°ê³¼ë¥¼ ê´„í˜¸+ìƒ‰ìƒ ë±ƒì§€ë¡œ ë°”ë¡œ í‘œì‹œ
                md_line = (
                    f"[{keyword}] "
                    f"[{article['title']}]({article['link']}) "
                    f"<span class='sentiment-badge {sentiment_class}'>({sentiment_label})</span> "
                    f"({article['date']} | {article['source']})"
                )
                cols = st.columns([0.04, 0.96])
                with cols[0]:
                    checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                with cols[1]:
                    st.markdown(md_line, unsafe_allow_html=True)
                st.session_state.article_checked[key] = checked

    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        selected_articles = []
        for keyword, articles in results.items():
            for idx, article in enumerate(articles[:show_limit.get(keyword, 5)]):
                key = f"{keyword}_{idx}"
                cache_key = f"summary_{key}"
                if st.session_state.article_checked.get(key, False):
                    if cache_key not in st.session_state:
                        one_line, summary, sentiment, full_text = summarize_article_from_url(article['link'], article['title'])
                        st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                    else:
                        one_line, summary, sentiment, full_text = st.session_state[cache_key]
                    selected_articles.append({
                        "íšŒì‚¬ëª…": keyword,
                        "ê¸°ì‚¬ì œëª©": article['title'],
                        "ìš”ì•½": one_line,
                        "full_summary": summary,
                        "sentiment": sentiment,
                        "ë§í¬": article['link'],
                        "date": article['date'],
                        "source": article['source']
                    })
                    # ìš”ì•½/ê°ì„±ë¶„ì„ ê²°ê³¼ ì¶œë ¥
                    st.markdown(f"#### [{article['title']}]({article['link']}) <span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-neutral')}'>({sentiment})</span>", unsafe_allow_html=True)
                    st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {article['date']} | {article['source']}")
                    st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {one_line}")
                    st.markdown(f"- **ìš”ì•½ë³¸:** {summary}")
                    st.markdown("---")
        summary_data = selected_articles

        st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(summary_data)}")

        # 3. ì—‘ì…€ í…œí”Œë¦¿ ì—…ë¡œë“œ ë° ì €ì¥
        st.markdown("#### ê¸°ì¡´ ì—‘ì…€ í…œí”Œë¦¿ ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader("ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”(ê¸°ì¡´ í…œí”Œë¦¿)", type=["xlsx"])
        if uploaded_file is not None and summary_data:
            if st.button("ì„ íƒ ê¸°ì‚¬ ì—‘ì…€ë¡œ ì €ì¥"):
                excel_bytes = update_excel(summary_data, uploaded_file)
                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=excel_bytes.getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ì—…ë°ì´íŠ¸.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        elif uploaded_file is not None:
            st.info("ì—‘ì…€ë¡œ ì €ì¥í•  ê¸°ì‚¬ë¥¼ ë¨¼ì € ì„ íƒí•˜ì„¸ìš”.")

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(filtered_results, st.session_state.show_limit)
