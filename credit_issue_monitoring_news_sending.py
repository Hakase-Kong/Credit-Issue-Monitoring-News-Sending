import os
import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
from datetime import datetime, timedelta
import telepot
from openai import OpenAI
import newspaper
import difflib
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import html
import json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import pandas as pd

# --- config.json ë¡œë“œ ---
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

EXCLUDE_TITLE_KEYWORDS = config["EXCLUDE_TITLE_KEYWORDS"] # --- ì œì™¸ í‚¤ì›Œë“œ ---
ALLOWED_SOURCES = set(config["ALLOWED_SOURCES"]) # í•„í„°ë§í•  ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ (www. ì œê±°ëœ ë„ë©”ì¸ ê¸°ì¤€)
favorite_categories = config["favorite_categories"] # --- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬(ë³€ê²½ ê¸ˆì§€) ---
excel_company_categories = config["excel_company_categories"]
common_filter_categories = config["common_filter_categories"] # --- ê³µí†µ í•„í„° ì˜µì…˜(ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì—†ì´ ëª¨ë‘ ì ìš©) ---
industry_filter_categories = config["industry_filter_categories"] # --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ ---
SYNONYM_MAP = config["synonym_map"]
kiscd_map = config.get("kiscd_map", {})
kr_compcd_map = config.get("kr_COMP_CD_map", {})

# ê³µí†µ í•„í„° í‚¤ì›Œë“œ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)

def extract_file_url(js_href: str) -> str:
    if not js_href or not js_href.startswith("javascript:fn_file"):
        return ""
    m = re.search(r"fn_file\((.*)\)", js_href)
    if not m:
        return ""
    args_str = m.group(1)
    args = [arg.strip().strip("'\"") for arg in args_str.split(",")]
    if len(args) < 4:
        return ""
    file_name = args[3]
    return f"https://www.kisrating.com/common/download.do?filename={file_name}"

def extract_reports_and_research(html: str) -> dict:
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')
    result = {
        "í‰ê°€ë¦¬í¬íŠ¸": [],
        "ê´€ë ¨ë¦¬ì„œì¹˜": [],
        "ì‹ ìš©ë“±ê¸‰ìƒì„¸": []
    }

    # í‰ê°€ë¦¬í¬íŠ¸, ê´€ë ¨ë¦¬ì„œì¹˜ í…Œì´ë¸” ë¡œì§ ê·¸ëŒ€ë¡œ
    tables = soup.select('div.table_ty1 > table')
    for table in tables:
        caption = table.find('caption')
        if not caption:
            continue
        caption_text = caption.text.strip()

        if caption_text == "í‰ê°€ë¦¬í¬íŠ¸":
            rows = table.select('tbody > tr')
            for tr in rows:
                tds = tr.find_all('td')
                if len(tds) < 4:
                    continue
                report_type = tds[0].text.strip()
                a_tag = tds[1].find('a')
                title = a_tag.text.strip() if a_tag else ''
                date = tds[2].text.strip()
                eval_type = tds[3].text.strip()
                result["í‰ê°€ë¦¬í¬íŠ¸"].append({
                    "ì¢…ë¥˜": report_type,
                    "ë¦¬í¬íŠ¸": title,
                    "ì¼ì": date,
                    "í‰ê°€ì¢…ë¥˜": eval_type
                })
        elif caption_text == "ê´€ë ¨ ë¦¬ì„œì¹˜":
            rows = table.select('tbody > tr')
            for tr in rows:
                tds = tr.find_all('td')
                if len(tds) < 4:
                    continue
                category = tds[0].text.strip()
                a_tag = tds[1].find('a')
                title = a_tag.text.strip() if a_tag else ''
                date = tds[2].text.strip()
                result["ê´€ë ¨ë¦¬ì„œì¹˜"].append({
                    "êµ¬ë¶„": category,
                    "ì œëª©": title,
                    "ì¼ì": date
                })

    # ì‹ ìš©ë“±ê¸‰ìƒì„¸ ì¶”ê°€ (ex. í˜„ëŒ€í•´ìƒ ë“±ê¸‰ í…Œì´ë¸”)
    # ê¸°ì¡´ extract_credit_details ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ê°€
    result["ì‹ ìš©ë“±ê¸‰ìƒì„¸"] = extract_credit_details(html)

    return result

# ë³„ë„ í•¨ìˆ˜ë¡œ ì‹ ìš©ë“±ê¸‰ìƒì„¸ ì¶”ì¶œ
def extract_credit_details(html):
    soup = BeautifulSoup(html, 'html.parser')
    results = []
    items = soup.select('div.list li')
    for item in items:
        key_tag = item.find('dt') or item.find('strong')
        kind = key_tag.get_text(strip=True) if key_tag else None
        if not kind:
            continue
        # ë“±ê¸‰
        grade_tag = item.find('span', string='ë“±ê¸‰')
        grade_val = ""
        if grade_tag:
            grade_node = grade_tag.find_next(['a', 'strong'])
            grade_val = grade_node.get_text(strip=True) if grade_node else ""
        # Outlook/Watchlist
        outlook_tag = item.find('span', string=lambda s: s and ('Outlook' in s or 'Watchlist' in s))
        outlook_val = outlook_tag.next_sibling.strip() if outlook_tag and outlook_tag.next_sibling else ""
        # í‰ê°€ì¼
        eval_date_tag = item.find('span', string='í‰ê°€ì¼')
        eval_date_val = eval_date_tag.next_sibling.strip() if eval_date_tag and eval_date_tag.next_sibling else ""
        # í‰ê°€ì˜ê²¬
        eval_opinion_tag = item.find('span', string='í‰ê°€ì˜ê²¬')
        eval_opinion_val = ""
        if eval_opinion_tag:
            next_node = eval_opinion_tag.find_next('a')
            if next_node:
                eval_opinion_val = next_node.get_text(strip=True)
            else:
                eval_opinion_val = eval_opinion_tag.find_next(string=True).strip()
        results.append({
            "ì¢…ë¥˜": kind,
            "ë“±ê¸‰": grade_val,
            "Outlook/Watchlist": outlook_val,
            "í‰ê°€ì¼": eval_date_val,
            "í‰ê°€ì˜ê²¬": eval_opinion_val
        })
    return results

def fetch_and_display_reports(companies_map):
    import pandas as pd
    import requests
    import time
    from bs4 import BeautifulSoup

    def extract_table_after_marker(soup, marker_str):
        marker = None
        for tag in soup.find_all(['b', 'strong', 'h2', 'h3', 'span']):
            if marker_str in tag.get_text():
                marker = tag
                break
        return marker.find_next('table') if marker else None

    def parse_grade_table_html(table_tag):
        try:
            dfs = pd.read_html(str(table_tag), header=[0, 1])
            df = dfs[0]
            df.columns = [
                '_'.join([str(l) for l in col if str(l) not in ['nan', 'None']]).strip()
                for col in df.columns.values
            ]
            if all(('Unnamed' in col or col == '' or col.lower() == 'none') for col in df.columns):
                raise Exception("í—¤ë” íŒŒì‹± ì‹¤íŒ¨ - ë‹¨ì¼ë¼ì¸ í—¤ë” ì‹œë„")
            return df
        except Exception:
            try:
                dfs = pd.read_html(str(table_tag), header=0)
                df = dfs[0]
                df.columns = [str(col).strip() for col in df.columns]
                return df
            except Exception:
                try:
                    rows = [
                        [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]
                        for row in table_tag.find_all('tr')
                    ]
                    df = pd.DataFrame(rows[1:], columns=rows[0])
                    return df
                except Exception:
                    return pd.DataFrame()

    def table_to_list(table):
        rows = []
        if not table:
            return rows
        for row in table.find_all('tr'):
            cells = [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]
            if cells:
                rows.append(cells)
        return rows

    def fetch_nice_rating_data(cmpCd):
        if not cmpCd:
            return {"major_grade_df": pd.DataFrame(), "special_reports": []}
        url = f"https://www.nicerating.com/disclosure/companyGradeInfo.do?cmpCd={cmpCd}"
        try:
            resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=20)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
            major_grade_table_tag = extract_table_after_marker(soup, 'ì£¼ìš” ë“±ê¸‰ë‚´ì—­')
            special_report_table_tag = extract_table_after_marker(soup, 'ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸')
            major_grade_df = parse_grade_table_html(major_grade_table_tag) if major_grade_table_tag else pd.DataFrame()
            special_reports = table_to_list(special_report_table_tag) if special_report_table_tag else []
            return {
                "major_grade_df": major_grade_df,
                "special_reports": special_reports,
            }
        except Exception as e:
            return {
                "major_grade_df": pd.DataFrame(),
                "special_reports": [],
                "error": f"ë‚˜ì´ìŠ¤ ì‹ ìš©í‰ê°€ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}"
            }

    st.markdown("---")
    st.markdown("### ğŸ“‘ ì‹ ìš©í‰ê°€ ë³´ê³ ì„œ ë° ê´€ë ¨ ë¦¬ì„œì¹˜")

    for cat in favorite_categories:
        for company in favorite_categories[cat]:
            kiscd = companies_map.get(company, "")
            cmpcd = config.get("cmpCD_map", {}).get(company, "")
            kr_compcd = kr_compcd_map.get(company, "")
            if not kiscd or not str(kiscd).strip():
                continue

            url_kis = f"https://www.kisrating.com/ratingsSearch/corp_overview.do?kiscd={kiscd}"
            url_nice = f"https://www.nicerating.com/disclosure/companyGradeInfo.do?cmpCd={cmpcd}"
            url_kie = f"https://www.korearatings.com/cms/frDisclosureCon/compView.do?MENU_ID=90&CONTENTS_NO=1&COMP_CD={kr_compcd}"
            with st.expander(
                f"{company} (KISCD: {kiscd} | CMP_CD: {cmpcd} | KIE_CD: {kr_compcd})", expanded=False
            ):
                st.markdown(
                    f"- [í•œêµ­ì‹ ìš©í‰ê°€ (KIS)]({url_kis}) &nbsp;&nbsp; "
                    f"[ë‚˜ì´ìŠ¤ì‹ ìš©í‰ê°€ (NICE)]({url_nice}) &nbsp;&nbsp; "
                    f"[í•œêµ­ê¸°ì—…í‰ê°€ (KIE)]({url_kie})",
                    unsafe_allow_html=True
                )
                try:
                    resp = requests.get(url_kis, timeout=20, headers={"User-Agent": "Mozilla/5.0"})
                    if resp.status_code == 200:
                        html = resp.text
                        report_data = extract_reports_and_research(html)

                        if report_data.get("í‰ê°€ë¦¬í¬íŠ¸"):
                            with st.expander("í‰ê°€ë¦¬í¬íŠ¸", expanded=True):
                                st.markdown("### í•œêµ­ì‹ ìš©í‰ê°€ í‰ê°€ë¦¬í¬íŠ¸")
                                df_report = pd.DataFrame(report_data["í‰ê°€ë¦¬í¬íŠ¸"])
                                df_report = df_report.drop(columns=["ë‹¤ìš´ë¡œë“œ"], errors="ignore")
                                st.dataframe(df_report)

                        if report_data.get("ê´€ë ¨ë¦¬ì„œì¹˜"):
                            with st.expander("ê´€ë ¨ë¦¬ì„œì¹˜", expanded=True):
                                st.markdown("### í•œêµ­ì‹ ìš©í‰ê°€ ê´€ë ¨ ë¦¬ì„œì¹˜")
                                df_research = pd.DataFrame(report_data["ê´€ë ¨ë¦¬ì„œì¹˜"])
                                df_research = df_research.drop(columns=["ë‹¤ìš´ë¡œë“œ"], errors="ignore")
                                st.dataframe(df_research)

                                nice_data = fetch_nice_rating_data(cmpcd)
                                special_reports = nice_data.get("special_reports", [])
                                st.markdown("#### ë‚˜ì´ìŠ¤ ì‹ ìš©í‰ê°€ ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸")
                                if special_reports and len(special_reports) > 1:
                                    header = special_reports[0]
                                    filtered_rows = [row for row in special_reports[1:] if len(row) == len(header)]
                                    if filtered_rows:
                                        df_special = pd.DataFrame(filtered_rows, columns=header)
                                        st.dataframe(df_special)
                                    else:
                                        st.info("í‘œ í˜•ì‹ì´ ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸)")
                                else:
                                    st.info("ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                if nice_data.get("error"):
                                    st.warning(nice_data["error"])

                        credit_detail_list = extract_credit_details(html)
                        with st.expander("ì‹ ìš©ë“±ê¸‰ ìƒì„¸ì •ë³´", expanded=True):
                            if credit_detail_list:
                                st.markdown("### í•œêµ­ì‹ ìš©í‰ê°€ ì‹ ìš©ë“±ê¸‰ ìƒì„¸ì •ë³´")
                                df_credit_detail = pd.DataFrame(credit_detail_list)
                                st.dataframe(df_credit_detail)
                            else:
                                st.info("ì‹ ìš©ë“±ê¸‰ ìƒì„¸ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

                            st.markdown("#### ë‚˜ì´ìŠ¤ ì‹ ìš©í‰ê°€ ì£¼ìš” ë“±ê¸‰ë‚´ì—­")
                            nice_data = fetch_nice_rating_data(cmpcd)
                            major_grade_df = nice_data.get("major_grade_df", pd.DataFrame())
                            if not major_grade_df.empty:
                                st.dataframe(major_grade_df)
                            else:
                                st.info("ì£¼ìš” ë“±ê¸‰ë‚´ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            if nice_data.get("error"):
                                st.warning(nice_data["error"])

                    else:
                        st.warning("í•œêµ­ì‹ ìš©í‰ê°€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.warning(f"ì‹ ìš©í‰ê°€ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")

                time.sleep(1)
            
def expand_keywords_with_synonyms(original_keywords):
    expanded_map = {}
    for kw in original_keywords:
        synonyms = SYNONYM_MAP.get(kw, [])
        expanded_map[kw] = [kw] + synonyms
    return expanded_map

def process_keywords_with_synonyms(favorite_to_expand_map, start_date, end_date, require_keyword_in_title=False):
    """ 
    require_keyword_in_title=True ì¸ ê²½ìš°:
      1ì°¨: ê°•í•œ í•„í„°(ì œëª©/ë³¸ë¬¸ì— í‚¤ì›Œë“œ í¬í•¨)ë¡œ ê²€ìƒ‰
      â†’ ê²°ê³¼(ì¤‘ë³µ ì œê±° í›„)ê°€ 0ê±´ì´ë©´
      2ì°¨: ê°™ì€ í‚¤ì›Œë“œ ì„¸íŠ¸ë¡œ require_keyword_in_title=Falseë¡œ ì¬ê²€ìƒ‰
          ì´ë•Œ ê°€ì ¸ì˜¨ ê¸°ì‚¬ì—ëŠ” relaxed_keyword_match=True í”Œë˜ê·¸ë¥¼ ë‹¬ì•„ì„œ,
          í›„ë‹¨ í•„í„°(article_passes_all_filters)ì—ì„œ 'ê°•ì œ í‚¤ì›Œë“œ í¬í•¨' ì¡°ê±´ì—ì„œ ì œì™¸í•œë‹¤.
    """

    def run_search_for_kw_list(kw_list, start_date, end_date, require_flag, relaxed_flag=False):
        all_articles_local = []
        with ThreadPoolExecutor(max_workers=min(5, len(kw_list))) as executor:
            futures = {
                executor.submit(
                    fetch_naver_news,
                    search_kw,
                    start_date,
                    end_date,
                    require_keyword_in_title=require_flag
                ): search_kw
                for search_kw in kw_list
            }
            for future in as_completed(futures):
                search_kw = futures[future]
                try:
                    fetched = future.result()
                    # ê° ê¸°ì‚¬ì— ê²€ìƒ‰ì–´/ì™„í™” í”Œë˜ê·¸ ì¶”ê°€
                    for a in fetched:
                        a["ê²€ìƒ‰ì–´"] = search_kw
                        if relaxed_flag:
                            a["relaxed_keyword_match"] = True
                    all_articles_local.extend(fetched)
                except Exception as e:
                    st.warning(f"{search_kw} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        # ì¤‘ë³µ ì œê±° ì˜µì…˜
        if st.session_state.get("remove_duplicate_articles", False):
            all_articles_local = remove_duplicates(all_articles_local)
        return all_articles_local

    for main_kw, kw_list in favorite_to_expand_map.items():
        # 1ì°¨: í˜„ì¬ ì²´í¬ë°•ìŠ¤ ê°’(require_keyword_in_title) ê·¸ëŒ€ë¡œ ì ìš©
        articles = run_search_for_kw_list(
            kw_list,
            start_date,
            end_date,
            require_flag=require_keyword_in_title,
            relaxed_flag=False,
        )

        # 2ì°¨ Fallback:
        # - ê°•ë ¥ í•„í„°(require_keyword_in_title=True) ìƒíƒœì´ê³ 
        # - 1ì°¨ ê²€ìƒ‰(ì¤‘ë³µ ì œê±°ê¹Œì§€ ì ìš© í›„) ê²°ê³¼ê°€ 0ê±´ì¸ ê²½ìš°ì—ë§Œ
        #   â†’ í•´ë‹¹ main_kwì— í•œí•´ì„œë§Œ í•„í„°ë¥¼ ì™„í™”í•´ì„œ ë‹¤ì‹œ ê²€ìƒ‰
        if require_keyword_in_title and not articles:
            articles = run_search_for_kw_list(
                kw_list,
                start_date,
                end_date,
                require_flag=False,          # ì œëª©/ë³¸ë¬¸ í‚¤ì›Œë“œ ê°•ì œ ì¡°ê±´ í•´ì œ
                relaxed_flag=True,          # í›„ë‹¨ í•„í„°ì—ì„œ 'ì™„í™” ì¼€ì´ìŠ¤'ë¡œ ì¸ì‹
            )

        st.session_state.search_results[main_kw] = articles
        if main_kw not in st.session_state.show_limit:
            st.session_state.show_limit[main_kw] = 5

# --- CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] { background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold; }
.sentiment-badge { 
    display: inline-block; 
    padding: 0.08em 0.6em; 
    margin-left: 0.2em; 
    border-radius: 0.8em; 
    font-size: 0.85em; 
    font-weight: bold; 
    vertical-align: middle; 
}
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.sentiment-neutral  { background: #6c757d; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
</style>

""", unsafe_allow_html=True)
st.markdown("""
<style>
.news-title { 
    word-break: break-all !important; 
    white-space: normal !important; 
    display: block !important;
    overflow: visible !important;
}
</style>
""", unsafe_allow_html=True)

def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

def init_session_state():
    """Streamlit ì„¸ì…˜ ë³€ìˆ˜ë“¤ì„ ì¼ê´„ ì´ˆê¸°í™”"""
    defaults = {
        "favorite_keywords": set(),
        "search_results": {},
        "show_limit": {},
        "search_triggered": False,
        "selected_articles": [],
        "cat_multi": [],
        "cat_major_autoset": [],
        "important_articles_preview": [],
        "important_selected_index": [],
        "article_checked_left": {},
        "article_checked": {},
        "industry_major_sub_map": {},
        "end_date": datetime.today().date(),
        "start_date": datetime.today().date() - timedelta(days=7),
        "remove_duplicate_articles": True,
        "require_exact_keyword_in_title_or_content": True,
        "filter_allowed_sources_only": False,
        "use_industry_filter": True,
        "show_sentiment_badge": False,
        "enable_summary": True,
        "keyword_input": ""
    }
    for key, default_val in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_val

# --- UI ì‹œì‘ ---
st.set_page_config(layout="wide")

# âœ… ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™” í˜¸ì¶œ
init_session_state()

col_title, col_option1, col_option2 = st.columns([0.5, 0.2, 0.3])

# --- ì¹´í…Œê³ ë¦¬-ì‚°ì—… ëŒ€ë¶„ë¥˜ ë§¤í•‘ í•¨ìˆ˜ ---
def get_industry_majors_from_favorites(selected_categories):
    favorite_to_industry_major = config["favorite_to_industry_major"]
    majors = set()
    for cat in selected_categories:
        for major in favorite_to_industry_major.get(cat, []):
            majors.add(major)
    return list(majors)

col_title, col_option1, col_option2 = st.columns([0.5, 0.2, 0.3])
with col_title:
    st.markdown(
        "<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>"
        "<a href='https://credit-issue-monitoring-news-sending.onrender.com/' target='_blank' style='text-decoration:none; color:#1a1a1a;'>"
        "ğŸ“Š Credit Issue Monitoring</a></h1>",
        unsafe_allow_html=True
    )
with col_option1:
    show_sentiment_badge = st.checkbox("ê°ì„±ë¶„ì„ ë°°ì§€í‘œì‹œ", key="show_sentiment_badge")
with col_option2:
    enable_summary = st.checkbox("ìš”ì•½ ê¸°ëŠ¥", key="enable_summary")
    
col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input(label="", value="", key="keyword_input", label_visibility="collapsed")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

st.markdown("**â­ ì‚°ì—…êµ° ì„ íƒ**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        "",
        list(favorite_categories.keys()), key="cat_multi", label_visibility="collapsed"
        )
if selected_categories:
    auto_selected_majors = get_industry_majors_from_favorites(selected_categories)
    st.session_state.cat_major_autoset = auto_selected_majors.copy()
else:
    st.session_state.cat_major_autoset = []
with col_cat_btn:
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", key="cat_search_btn", help="ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰", use_container_width=True)
for cat in selected_categories:
    st.session_state.favorite_keywords.update(favorite_categories[cat])

# ë‚ ì§œ ì…ë ¥ (ê¸°ë³¸ ì„¸íŒ…: ì¢…ë£Œì¼=ì˜¤ëŠ˜, ì‹œì‘ì¼=ì˜¤ëŠ˜-7ì¼)
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼", value=st.session_state["start_date"], key="start_date_input")
    st.session_state["start_date"] = start_date
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼", value=st.session_state["end_date"], key="end_date_input")
    st.session_state["end_date"] = end_date

with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©ë¨)"):
    for major, subs in common_filter_categories.items():
        st.markdown(f"**{major}**: {', '.join(subs)}")

with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ëŒ€ë¶„ë¥˜ë³„ ì†Œë¶„ë¥˜ í•„í„°ë§)"):
    use_industry_filter = st.checkbox("ì´ í•„í„° ì ìš©", key="use_industry_filter")

    # UI: ì„ íƒëœ ì‚°ì—…êµ°ì—ì„œ ìë™ ë§¤í•‘ëœ ëŒ€ë¶„ë¥˜ ì¶”ì¶œ
    selected_major_map = get_industry_majors_from_favorites(selected_categories)

    updated_map = {}
    for major in selected_major_map:
        options = industry_filter_categories.get(major, [])
        default_selected = options if major not in st.session_state.industry_major_sub_map else st.session_state.industry_major_sub_map[major]
        selected_sub = st.multiselect(
            f"{major} ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ",
            options,
            default=default_selected,
            key=f"subfilter_{major}"
        )
        updated_map[major] = selected_sub

    st.session_state.industry_major_sub_map = updated_map
    
# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ê¸°ëŠ¥ ì²´í¬ë°•ìŠ¤ í¬í•¨ëœ í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    require_exact_keyword_in_title_or_content = st.checkbox("í‚¤ì›Œë“œê°€ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", key="require_exact_keyword_in_title_or_content")
    remove_duplicate_articles = st.checkbox("ì¤‘ë³µ ê¸°ì‚¬ ì œê±°", key="remove_duplicate_articles", help="í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.")
    filter_allowed_sources_only = st.checkbox(
        "íŠ¹ì • ì–¸ë¡ ì‚¬ë§Œ ê²€ìƒ‰", 
        key="filter_allowed_sources_only", 
        help="ì„ íƒëœ ë©”ì´ì € ì–¸ë¡ ì‚¬ë§Œ í•„í„°ë§í•˜ê³ , ê·¸ ì™¸ ì–¸ë¡ ì€ ì œì™¸í•©ë‹ˆë‹¤."
    )

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def get_industry_credit_keywords():
    return """
ë³´í—˜ì‚¬: ìˆ˜ìµì„±, ìë³¸ì ì •ì„±, IFRS17, K-ICS, ë¦¬ìŠ¤í¬ê´€ë¦¬, ì†í•´ìœ¨, ì¬ë³´í—˜, ìœ ë™ì„±, íˆ¬ììì‚°, ìŠ¤íŠ¸ë ˆìŠ¤í…ŒìŠ¤íŠ¸, ê²½ì˜íˆ¬ëª…ì„±, ë‚´ë¶€í†µì œ, ì‹œì¥ì§€ìœ„, ìê¸ˆì¡°ë‹¬, ì •ì±…, ê·œì œ, ëŒ€ì²´íˆ¬ì, ì†ìµë³€ë™, ì§€ê¸‰ì—¬ë ¥, ê³„ì•½ìœ ì§€ìœ¨, ìœ„í—˜ì§‘ì¤‘, ì²´ì¦ë¥ , ë³´í—˜ê¸ˆì§€ê¸‰
5ëŒ€ê¸ˆìœµì§€ì£¼ ë° ì€í–‰: ìíšŒì‚¬ ì‹ ìš©ë„, ë°°ë‹¹, ìì‚°ê±´ì „ì„±, ì •ë¶€ì§€ì›, ìë³¸ë¹„ìœ¨, ìœ ë™ì„±ë¹„ìœ¨, ëŒ€ì†ì¶©ë‹¹ê¸ˆ, ë ˆë²„ë¦¬ì§€, ìŠ¤íŠ¸ë ˆìŠ¤, ì‹œì¥ìœ„í—˜, ê¸ˆë¦¬ìœ„í—˜, ë¹„ì´ììˆ˜ìµ, ë‹¤ê°í™”, ê±°ë²„ë„ŒìŠ¤, ê·œì œì¤€ìˆ˜, ìš´ì˜ìœ„í—˜, ë‹¨ê¸°ë¶€ì±„, êµ¬ì¡°ì¡°ì •, ë¶€ì‹¤ì±„ê¶Œ, ì¡°ê¸°ê²½ë³´, ìœ ê°€ì¦ê¶Œ
ì¹´ë“œì‚¬: ì‹œì¥ì ìœ ìœ¨, ìˆ˜ìˆ˜ë£Œìœ¨, ëŒ€ì†ë¹„ìš©, ìì‚°ê±´ì „ì„±, ì‹ ìš©ë¦¬ìŠ¤í¬, ëŒ€ì†ìœ¨, ìƒí™˜ëŠ¥ë ¥, í¬íŠ¸í´ë¦¬ì˜¤, ìˆ˜ìµì„±, ê±°ë˜ëŸ‰, ìš´ì˜ë¦¬ìŠ¤í¬, ë²•ë¥ , íŒŒíŠ¸ë„ˆì‹­, ë¹„ìš©, ê¸ˆìœµì¡°ë‹¬, ì‹ ìš©ì§€ì›, ê²½ìŸë ¥, ê°€ê²©ì±…ì •, ìŠ¹ì¸ê±°ë˜ì•¡, ë¶€ì •ì‚¬ìš©, ê²°ì œì—°ì²´
ìºí”¼íƒˆ: ì‚¬ì—…í†µí•©, ìˆ˜ìµì•ˆì •ì„±, ìì‚°ê±´ì „ì„±, í•´ì™¸ì‹œì¥, ë¶€ì‹¤ë¥ , ìê¸ˆì¡°ë‹¬, ìœ ë™ì„±, ì´ìµì°½ì¶œë ¥, ì„±ì¥ì„±, ì‹ ìš©ë¦¬ìŠ¤í¬, ì‹œì¥ë¦¬ìŠ¤í¬, ë²•ì ì œì•½, ë‚´ë¶€í†µì œ, ì±„ê¶Œí¬íŠ¸í´ë¦¬ì˜¤, íŒŒìƒìƒí’ˆ, ê·¸ë£¹ì§€ì›, ì‚¬ì—…ë‹¤ê°í™”, ë¦¬ìŠ¤í¬ì§‘ì¤‘ë„, ëŒ€ì¶œì±„ê¶Œ, ë¶€ì‹¤ì±„ê¶Œë¹„ìœ¨, íšŒìˆ˜ìœ¨
ì§€ì£¼ì‚¬: ìíšŒì‚¬ ì‹ ìš©ë„, ë°°ë‹¹ì•ˆì •ì„±, ì¬ë¬´ë¶€ë‹´, ê·¸ë£¹ì‹ ìš©, ì§€ë°°êµ¬ì¡°, ì¬ë¬´ë ˆë²„ë¦¬ì§€, ë¶€ì±„ë§Œê¸°, ì‹ ìš©ì§€ì›, ìˆ˜ìµì•ˆì •ì„±, ìë³¸ì¡°ë‹¬, ìì‚°ê±´ì „ì„±, í˜„ê¸ˆíë¦„, ìë³¸ì„±ì¦ê¶Œ, íˆ¬ìë¦¬ìŠ¤í¬, ì „ëµì§€ì›, ì§€ë¶„ìœ¨, ë‚´ë¶€ê±°ë˜, ê²½ì˜ê¶Œìœ„í—˜
ì—ë„ˆì§€: ì‹œì¥ê²½ìŸ, ì‚¬ì—…ë‹¤ê°í™”, í•´ì™¸ì‹¤ì , íˆ¬ìê·œëª¨, ê°€ê²©ë³€ë™ì„±, ì¬ë¬´ì•ˆì •ì„±, ì •ì±…ë³€í™”, í™˜ê²½ê·œì œ, í˜„ê¸ˆíë¦„, í”„ë¡œì íŠ¸ì§‘í–‰, ì¬ë¬´íŒŒìƒìƒí’ˆë¦¬ìŠ¤í¬, ë¶€ì±„êµ¬ì¡°, ìë³¸ì¡°ë‹¬, ê³µê¸‰ë§, ê¸°ìˆ ì „í™˜, ê¸€ë¡œë²Œê²½ì œ, íƒ„ì†Œë°°ì¶œê¶Œ, ì—ë„ˆì§€ìˆ˜ê¸‰, ì •ë¶€ì§€ì›
ë°œì „: ì „ë ¥ê¸°ë°˜, ì„¤ë¹„íˆ¬ì, ì „ë ¥ê°€ê²©, ê°€ë™ë¥ , ê³„ì•½, ì—°ë£Œë¹„, ë¶€ì±„, ìë³¸êµ¬ì¡°, ë°°ë‹¹ì •ì±…, ì¬ë¬´ìœ ì—°ì„±, ì •ë¶€ê·œì œ, í™˜ê²½ë²•ê·œ, í˜„ê¸ˆíë¦„, íˆ¬ìê³„íš, ì°¨ì…ê¸ˆ, ê¸°ìˆ ë¦¬ìŠ¤í¬, ì‚¬ì—…ë‹¤ê°í™”, ì‹œì¥ìˆ˜ìš”, ë°œì „íš¨ìœ¨, ì‹ ì¬ìƒì—ë„ˆì§€, ì •ë¶€ë³´ì¡°ê¸ˆ
ìë™ì°¨: ë°°í„°ë¦¬ì‹œì¥, ì „ê¸°ì°¨ìˆ˜ìš”, ì„¤ë¹„íˆ¬ì, ìˆ˜ìµì„±, ì‹œì¥ì ìœ ìœ¨, ê¸°ìˆ ê²½ìŸë ¥, ë§¤ì¶œë‹¤ê°í™”, ë ˆë²„ë¦¬ì§€, ê³ ì •ë¹„, ìƒì‚°ëŠ¥ë ¥, ì‹ ì œí’ˆê°œë°œ, ì •ë¶€ì •ì±…, ê³µê¸‰ë§, ìë³¸ì§€ì¶œ, ì—°êµ¬ê°œë°œ, í˜„ê¸ˆíë¦„, ì„±ì¥ì „ë§, ê²½ìŸí™˜ê²½, ì¹œí™˜ê²½ì°¨, ê´€ì„¸ì •ì±…
ì „ê¸°ì „ì: ë°˜ë„ì²´ì‹œì¥, AIìˆ˜ìš”, ë¬´ì—­ê·œì œ, ê¸°ìˆ ìš°ìœ„, ì œí’ˆìˆ˜ìš”, ê´€ì„¸, íˆ¬ìê³„íš, ìƒì‚°ì‹œì„¤, ì¬ë¬´ì•ˆì •ì„±, ì—°êµ¬ê°œë°œ, ê³µê¸‰ë§, ì§„ì…ì¥ë²½, ìš´ì˜íš¨ìœ¨, í™˜ìœ¨, ë³´ì•ˆ, ê°€ê²©ê²½ìŸë ¥, ì¸ì¬í™•ë³´, ì¬ë¬´ì •ì±…, ê¸°ìˆ íŠ¹í—ˆ, ë³´ì•ˆìœ„í˜‘
ì†Œë¹„ì¬: ìœ í†µë³€í™”, M&Aì¬ë¬´ë¶€ë‹´, ì˜¨ë¼ì¸ì‚¬ì—…, ìœ í†µì±„ë„, ë¸Œëœë“œ, ì‹œì¥ì ìœ ìœ¨, ì˜ì—…ì´ìµë¥ , í˜„ê¸ˆíë¦„, ì¬ë¬´ê±´ì „ì„±, ì¬ê³ ê´€ë¦¬, ê²½ìŸì••ë ¥, í˜ì‹ , ê³ ê°ì¶©ì„±ë„, ë¹„ìš©, ê³µê¸‰ë§, ì‹ ìš©ì§€ì›, ë§¤ì¶œì„±ì¥, ì‹ ì œí’ˆëŸ°ì¹­, ê³ ê°í™•ë³´
ë¹„ì² ì² ê°•: ìˆ˜ìš”ê³µê¸‰, ê°€ê²©ë³€ë™, í•´ì™¸í”„ë¡œì íŠ¸, ì¹œí™˜ê²½ì„¤ë¹„, ë¹„ìš©, ìë³¸ì§€ì¶œ, ì‹¤í–‰ë ¥, í™˜ê²½ê·œì œ, ë¶€ì±„, í˜„ê¸ˆíë¦„, ì‹œì¥ë‹¤ë³€í™”, ìƒí’ˆí¬íŠ¸í´ë¦¬ì˜¤, ê²½ìŸ, ê³µê¸‰ë§, ê¸°ìˆ ì „í™˜, ì›ìì¬ê°€ê²©, ìˆ˜ì¶œë¹„ì¤‘
ì„ìœ í™”í•™: ê²½ìŸë ¥, í¬íŠ¸í´ë¦¬ì˜¤, íˆ¬ì, ì°¨ì…ê¸ˆ, ì„¸ì œ, ì¬ë¬´ê´€ë¦¬, ì—…í™©ë¯¼ê°ë„, ì°¨ì…ê¸ˆë¹„ìœ¨, ìê¸ˆì¡°ë‹¬, ì¸ìˆ˜í•©ë³‘, ìˆ˜ìµì„±, í˜„ê¸ˆíë¦„, ìì‚°ìœ ë™í™”, ë¦¬ìŠ¤í¬ë¶„ì‚°, ì‹œì¥ì ìœ ìœ¨, ë¹„ìš©, ë¹„í•µì‹¬ìì‚°, í”„ë¡œì íŠ¸ê´€ë¦¬, ì„¸ì œí˜œíƒ
íŠ¹ìˆ˜ì±„: ì¤€ì •ë¶€ê¸°ê´€, ë³´ì¦ì‹œì¥, ë³´ì¦ì‚¬ê³ , ìë³¸í™•ì¶©, ì •ë¶€ì§€ì›, ì‹ ìš©ì—°ê³„, ë³´ì¦ì”ì•¡, ë¦¬ìŠ¤í¬, í˜„ê¸ˆì„±ìì‚°, ë‹¨ê¸°ë¶€ì±„, ë¯¸íšŒìˆ˜ì±„ê¶Œ, ìì‚°ê±´ì „ì„±, ìš´ì˜ì•ˆì •ì„±, ë³´ì¦í•œë„, ì¬ë¬´ì•ˆì •ì„±, ì‹œì¥ì§€ìœ„, ê´€ë¦¬ì²´ê³„, ì •ì±…, ì±„ê¶Œë°œí–‰, ì§€ê¸‰ìœ ì˜ˆ, ë¶ˆí™•ì‹¤ì„±
"""
# --- REPLACE: summarizer with clearer roles (ì‚¬ì‹¤ vs ì˜í–¥) ---
def summarize_and_sentiment_with_openai(text, do_summary=True, target_keyword=None):
    """
    ë°˜í™˜: (one_line_summary, keywords, sentiment, detailed_implication, short_implication, original_text)
    - í•œ ì¤„ ìš”ì•½: 'ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ëŠ”ê°€' (ì‚¬ì‹¤ ì¤‘ì‹¬)
    - ì‹¬ì¸µ ì‹œì‚¬ì : ì‹ ìš©í‰ê°€ ì½”ë©˜íŠ¸ í˜•ì‹(ë“±ê¸‰/ì „ë§/ìœ ë™ì„±/í˜„ê¸ˆíë¦„ ë“± ì˜í–¥) 3ë¬¸ì¥ ì´ìƒ
    - í•œ ì¤„ ì‹œì‚¬ì : ì˜í–¥ì˜ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ì¶•ì•½
    - ê°ì„±: ê¸ì •/ë¶€ì •/ì¤‘ë¦½
    """
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", text
    if not text or "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜" in text:
        return "ê¸°ì‚¬ ë³¸ë¬¸ì´ ì¶”ì¶œ ì‹¤íŒ¨", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", text

    industry_keywords = get_industry_credit_keywords()

    prompt = f"""
[ì°¸ê³ : ì‚°ì—…êµ°ë³„ ì‹ ìš©í‰ê°€ í‚¤ì›Œë“œ(ì°¸ê³ ìš©)]
{industry_keywords}

ì•„ë˜ [ê¸°ì‚¬ ë³¸ë¬¸]ì„ ë¶„ì„í•´ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì‹œì˜¤.
ëŒ€ìƒ ê¸°ì—…: "{target_keyword or 'N/A'}"

ìš”êµ¬ í˜•ì‹:
1. [í•œ ì¤„ ìš”ì•½]: ì‚¬ì‹¤ ì¤‘ì‹¬. ëˆ„ê°€/ë¬´ì—‡ì„/ì–¸ì œ/ì–´ë–»ê²Œ í•œ ì¼ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ.
2. [ì‹¬ì¸µ ì‹œì‚¬ì ]: ì‹ ìš©í‰ê°€ì‚¬ì˜ ì½”ë©˜íŠ¸ í˜•ì‹ìœ¼ë¡œ ë“±ê¸‰/ì „ë§/ì¬ë¬´ì•ˆì •ì„±/í˜„ê¸ˆíë¦„/ìœ ë™ì„±/ì‚¬ì—…Â·ê·œì œ í™˜ê²½ ì˜í–¥ ë¶„ì„(3ë¬¸ì¥ ì´ìƒ, ê³¼ë„í•œ ì¼ë°˜í™” ê¸ˆì§€).
3. [í•œ ì¤„ ì‹œì‚¬ì ]: ì˜í–¥ì˜ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ì••ì¶•(ì˜ˆ: 'ì°¨ì… í™•ëŒ€ë¡œ ë‹¨ê¸°ìœ ë™ì„± ë¶€ë‹´ ìƒìŠ¹').
4. [ê°ì„±]: ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜.
5. [ê²€ìƒ‰ í‚¤ì›Œë“œ]: ëŒ€ìƒ ê¸°ì—…ëª… ë˜ëŠ” ì£¼ìš” ì—”í‹°í‹° ìœ„ì£¼ë¡œ ì½¤ë§ˆ êµ¬ë¶„.
6. [ì£¼ìš” í‚¤ì›Œë“œ]: ì¸ë¬¼/ê¸°ì—…/ê¸°ê´€ëª… ì¤‘ì‹¬ìœ¼ë¡œ ì½¤ë§ˆ êµ¬ë¶„. ì—†ìœ¼ë©´ 'ì—†ìŒ'.

[ê¸°ì‚¬ ë³¸ë¬¸]
{text}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",              # ê¸°ì¡´ gpt-3.5-turbo â†’ gpt-4o-mini
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì‹ ìš©í‰ê°€ì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤. ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨í•˜ê³  ê³¼ì¥/ì¶”ì¸¡ì„ í”¼í•œë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=900,
            temperature=0.3                   # 0 â†’ 0.3: ì–µì§€ìŠ¤ëŸ¬ì›€ ì™„í™”, ë¬¸ì¥ ìì—°ìŠ¤ëŸ¬ì›€ ê°œì„ 
        )
        answer = response.choices[0].message.content.strip()
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", text

    # --- parser ---
    def extract_group(tag):
        # íƒœê·¸ë³„ ë¸”ë¡ ì¶”ì¶œ
        pattern = rf"\[{tag}\]:\s*([\s\S]+?)(?=\n\[\w+\]:|\n\d+\. \[|$)"
        m = re.search(pattern, answer)
        return m.group(1).strip() if m else ""

    one_line = extract_group("í•œ ì¤„ ìš”ì•½") or "ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨"
    detailed_implication = extract_group("ì‹¬ì¸µ ì‹œì‚¬ì ") or "ì‹œì‚¬ì  ì¶”ì¶œ ì‹¤íŒ¨"
    short_implication = extract_group("í•œ ì¤„ ì‹œì‚¬ì ") or "í•œ ì¤„ ì‹œì‚¬ì  ìš”ì•½ ì‹¤íŒ¨"
    sentiment = extract_group("ê°ì„±") or "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨"
    keywords = extract_group("ê²€ìƒ‰ í‚¤ì›Œë“œ") or ""
    key_entities = extract_group("ì£¼ìš” í‚¤ì›Œë“œ") or ""

    # ê°ì„± í‘œì¤€í™”
    s = sentiment.strip().lower()
    if "ê¸" in s or "positive" in s:
        sentiment = "ê¸ì •"
    elif "ë¶€" in s or "negative" in s:
        sentiment = "ë¶€ì •"
    elif "ì¤‘ë¦½" in s or "neutral" in s:
        sentiment = "ì¤‘ë¦½"
    else:
        sentiment = "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨"

    return one_line, keywords, sentiment, detailed_implication, short_implication, text

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)  # ì´ë¯¸ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜
        self.chat_id = TELEGRAM_CHAT_ID
    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)
        
def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        for item in items:
            title = html.unescape(re.sub("<.*?>", "", item["title"]))
            desc  = html.unescape(re.sub("<.*?>", "", item["description"]))
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()

            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
                continue

            source = item.get("source") or infer_source_from_url(item.get("originallink", "")) or "Naver"
            source_domain = source.lower()
            if source_domain.startswith("www."):
                source_domain = source_domain[4:]

            real_link = item.get("originallink") or item["link"]

            articles.append({
                "title": title,
                "description": desc,  # í˜¹ì‹œ ì—‘ì…€ì— ì„¤ëª…ë„ ì“¸ ê²½ìš° ëŒ€ë¹„
                "link": real_link,
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": source_domain
            })

        if len(items) < 100:
            break
    return articles[:limit]

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

# --- OPTIONAL: keep existing function, just ensure fallback args are passed ---
def summarize_article_from_url(article_url, title, do_summary=True, target_keyword=None, description=None):
    cache_key_base = re.sub(r"\W+", "", article_url)[-16:]
    summary_key = f"summary_{cache_key_base}"

    if summary_key in st.session_state:
        return st.session_state[summary_key]

    try:
        full_text = extract_article_text(article_url, fallback_desc=description, fallback_title=title)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            result = (full_text, "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", full_text)
        else:
            one_line, summary, sentiment, implication, short_implication, text = summarize_and_sentiment_with_openai(
                full_text, do_summary=do_summary, target_keyword=target_keyword
            )
            result = (one_line, summary, sentiment, implication, short_implication, text)
    except Exception as e:
        result = (f"ìš”ì•½ ì˜¤ë¥˜: {e}", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", "")

    st.session_state[summary_key] = result
    return result

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    """ì œëª©/ë³¸ë¬¸(ìš”ì•½/ë³¸ë¬¸ ìºì‹œ í¬í•¨)ì— í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ ì •í™•íˆ í¬í•¨ë˜ë©´ True"""
    if not keywords:
        return False

    # ê¸°ë³¸ í…ìŠ¤íŠ¸: ì œëª© + ë„¤ì´ë²„ description
    text_parts = [
        article.get("title", "") or "",
        article.get("description", "") or "",
    ]

    # ìš”ì•½/ë³¸ë¬¸ì´ ì´ë¯¸ ìºì‹œì— ìˆìœ¼ë©´ ê°™ì´ ê²€ìƒ‰ì— í¬í•¨
    link = article.get("link", "") or ""
    if link:
        cache_key_base = re.sub(r"\W+", "", link)[-16:]
        summary_key = f"summary_{cache_key_base}"
        cached = st.session_state.get(summary_key)
        if isinstance(cached, tuple):
            # (one_line, summary, sentiment, implication, short_implication, full_text)
            one_line = cached[0] if len(cached) > 0 else ""
            summary = cached[1] if len(cached) > 1 else ""
            full_text = cached[5] if len(cached) > 5 else ""
            text_parts.extend([one_line or "", summary or "", full_text or ""])

    text = " ".join(text_parts)

    for kw in keywords:
        if kw and kw in text:
            return True
    return False

def article_passes_all_filters(article):
    # 1) ì œëª©ì— ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ ì œì™¸
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False

    # 2) ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
    try:
        pub_date = datetime.strptime(article['date'], '%Y-%m-%d').date()
        if pub_date < st.session_state.get("start_date") or pub_date > st.session_state.get("end_date"):
            return False
    except Exception:
        return False

    # 3) í‚¤ì›Œë“œ ì§‘í•© êµ¬ì„±: ì§ì ‘ ì…ë ¥ + ì„ íƒ ì¹´í…Œê³ ë¦¬ì˜ íšŒì‚¬ ë¦¬ìŠ¤íŠ¸
    all_keywords = []
    if "keyword_input" in st.session_state and st.session_state["keyword_input"]:
        all_keywords.extend([k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()])
    if "cat_multi" in st.session_state:
        for cat in st.session_state["cat_multi"]:
            all_keywords.extend(favorite_categories.get(cat, []))

    # ê°•ë ¥ í‚¤ì›Œë“œ í•„í„° ì—¬ë¶€
    require_kw = st.session_state.get("require_exact_keyword_in_title_or_content", False)
    # ì´ ê¸°ì‚¬ê°€ fallback(í•„í„° ì™„í™”) ê²€ìƒ‰ìœ¼ë¡œ ê°€ì ¸ì˜¨ ê¸°ì‚¬ì¸ì§€ ì—¬ë¶€
    relaxed = article.get("relaxed_keyword_match", False)

    # 4) í‚¤ì›Œë“œ í•„í„° í†µê³¼ ì—¬ë¶€
    #    - ê°•ë ¥í•„í„° ON & fallback ì•„ë‹˜ & í‚¤ì›Œë“œ ì¡´ì¬ â†’ ì œëª©/ë³¸ë¬¸ì— ë°˜ë“œì‹œ í¬í•¨
    #    - ê·¸ ì™¸(ì²´í¬ë°•ìŠ¤ OFF, í‚¤ì›Œë“œ ì—†ìŒ, fallback ê¸°ì‚¬)ëŠ” True ë¡œ ì™„í™”
    if require_kw and not relaxed and all_keywords:
        keyword_passed = article_contains_exact_keyword(article, all_keywords)
    else:
        keyword_passed = True

    # 5) ì–¸ë¡ ì‚¬ ë„ë©”ì¸ í•„í„°
    if st.session_state.get("filter_allowed_sources_only", True):
        source = (article.get('source', '') or '').lower()
        if source.startswith("www."):
            source = source[4:]
        if source not in ALLOWED_SOURCES:
            return False

    # 6) ê³µí†µ í•„í„°(í•­ìƒ AND)
    common_passed = or_keyword_filter(article, ALL_COMMON_FILTER_KEYWORDS)
    if not common_passed:
        return False

    # 7) ì‚°ì—…ë³„ í•„í„° (ì„ íƒì )
    industry_passed = True
    if st.session_state.get("use_industry_filter", False):
        industry_passed = False
        keyword_for_mapping = article.get("í‚¤ì›Œë“œ")
        matched_major = None
        for cat, companies in favorite_categories.items():
            if keyword_for_mapping in companies:
                majors = get_industry_majors_from_favorites([cat])
                if majors:
                    matched_major = majors[0]
                    break
        if matched_major:
            sub_keyword_filter = st.session_state.industry_major_sub_map.get(matched_major, [])
            if sub_keyword_filter:
                industry_passed = or_keyword_filter(article, sub_keyword_filter)

    # 8) ìµœì¢… íŒë‹¨
    #    - ê°•ë ¥ í‚¤ì›Œë“œ í•„í„°ê°€ ON ì¸ ê²½ìš°:
    #        â†’ keyword_passed=False ì´ë©´ ë¬´ì¡°ê±´ ì œì™¸
    #        â†’ ì‚°ì—…í•„í„°ëŠ” ì¶”ê°€ ì¡°ê±´(ì²´í¬ ì‹œ í†µê³¼ í•„ìš”)
    if require_kw:
        if not keyword_passed:
            return False
        if st.session_state.get("use_industry_filter", False) and not industry_passed:
            return False
        return True

    #    - ê°•ë ¥ í‚¤ì›Œë“œ í•„í„°ê°€ OFF ì¸ ê²½ìš°:
    #        â†’ ê³µí†µí•„í„°ëŠ” ì´ë¯¸ í†µê³¼í–ˆìœ¼ë¯€ë¡œ,
    #        â†’ ì‚°ì—…í•„í„° ë˜ëŠ” í‚¤ì›Œë“œí•„í„° ì¤‘ í•˜ë‚˜ë§Œ í†µê³¼í•´ë„ í—ˆìš©
    if not (industry_passed or keyword_passed):
        return False

    return True

# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° í•¨ìˆ˜ ---
def is_similar(title1, title2, threshold=0.5):
    ratio = difflib.SequenceMatcher(None, title1, title2).ratio()
    return ratio >= threshold

def remove_duplicates(articles):
    unique_articles = []
    titles = []
    for article in articles:
        title = article.get("title", "")
        if all(not is_similar(title, existing_title) for existing_title in titles):
            unique_articles.append(article)
            titles.append(title)
    return unique_articles

# í•­ìƒ ë¨¼ì € ì„ ì–¸í•´ ì—ëŸ¬ ë°©ì§€
keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()] if keywords_input else []
search_clicked = False

if keyword_list:
        search_clicked = True

if keyword_list and (search_clicked or st.session_state.get("search_triggered")):
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        # ë™ì˜ì–´ í™•ì¥
        expanded = expand_keywords_with_synonyms(sorted(keyword_list))
        process_keywords_with_synonyms(
            expanded,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )
    st.session_state.search_triggered = False


if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])

        expanded = expand_keywords_with_synonyms(sorted(keywords))
        process_keywords_with_synonyms(
            expanded,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def clean_excel_formula_text(text):
    """ì—‘ì…€ ìˆ˜ì‹(HYPERLINK)ì—ì„œ ê¹¨ì§ ë°©ì§€ìš© ì „ì²˜ë¦¬"""
    if not isinstance(text, str):  # Noneì´ë‚˜ ìˆ«ìì´ë©´ ë¬¸ì ë³€í™˜
        text = str(text)
    text = text.replace('"', "'")   # í°ë”°ì˜´í‘œ â†’ í™‘ë”°ì˜´í‘œ
    text = text.replace('\n', ' ')  # ì¤„ë°”ê¿ˆ â†’ ê³µë°±
    text = text.replace('\r', '')
    return text[:250]  # ì•ˆì „í•˜ê²Œ 255ì ë¯¸ë§Œìœ¼ë¡œ ì œí•œ

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories, search_results):
    import pandas as pd
    from io import BytesIO

    def clean_text(text):
        if not isinstance(text, str):
            text = str(text)
        text = text.replace('"', "'").replace('\n', ' ').replace('\r', '')
        return text[:200]

    # íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°í•˜ë©° ìˆœì„œ ìœ ì§€)
    sector_list = []
    for cat in favorite_categories:
        sector_list.extend(favorite_categories[cat])
    sector_list = list(dict.fromkeys(sector_list))

    # ê° íšŒì‚¬ì— ëŒ€ì‘í•˜ëŠ” ì—‘ì…€ í‘œê¸°ëª… ë¦¬ìŠ¤íŠ¸
    excel_sector_list = []
    for cat in excel_company_categories:
        excel_sector_list.extend(excel_company_categories[cat])
    excel_sector_list = list(dict.fromkeys(excel_sector_list))

    # ë¹ˆ DataFrameì¼ ê²½ìš° ëŒ€ë¹„
    if summary_data is None or len(summary_data) == 0:
        df_empty = pd.DataFrame(columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df_empty.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
            worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
            worksheet.set_column(0, 5, 30)
        output.seek(0)
        return output

    df = pd.DataFrame(summary_data)

    # â€˜í•œì¤„ì‹œì‚¬ì â€™ ìš°ì„ , ì—†ìœ¼ë©´ â€˜ì‹œì‚¬ì â€™, â€˜implicationâ€™ ì»¬ëŸ¼ìœ¼ë¡œ ì„¤ì •
    if "í•œì¤„ì‹œì‚¬ì " in df.columns:
        implication_col = "í•œì¤„ì‹œì‚¬ì "
    elif "ì‹œì‚¬ì " in df.columns:
        implication_col = "ì‹œì‚¬ì "
    elif "implication" in df.columns:
        implication_col = "implication"
    else:
        implication_col = None

    # í‚¤ì›Œë“œ ê´€ë ¨ ì»¬ëŸ¼ëª… ê²°ì •
    if "í‚¤ì›Œë“œ" in df.columns:
        keyword_col = "í‚¤ì›Œë“œ"
    elif "ê¸°ì—…ëª…" in df.columns:
        keyword_col = "ê¸°ì—…ëª…"
    elif "íšŒì‚¬ëª…" in df.columns:
        keyword_col = "íšŒì‚¬ëª…"
    else:
        keyword_col = df.columns[0] if len(df.columns) > 0 else "ê¸°ì—…ëª…"

    rows = []
    for idx, company in enumerate(sector_list):
        # í•´ë‹¹ íšŒì‚¬ ê´€ë ¨ ëª¨ë“  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        search_articles = search_results.get(company, [])

        # ê³µí†µ í•„í„°ì™€ ì‚°ì—…ë³„ í•„í„° í†µê³¼ ê¸°ì‚¬ë§Œ í•„í„°ë§ (í•„ìš”ì‹œ ì‚°ì—…ë³„ í•„í„° ì¡°ê±´ ì¶”ê°€)
        filtered_articles = []
        for article in search_articles:
            passes_common = any(kw in (article.get("title", "") + article.get("description", "")) for kw in ALL_COMMON_FILTER_KEYWORDS)
            passes_industry = True
            # í•„ìš” ì‹œ ì‚°ì—…ë³„ í•„í„°ë§ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

            if passes_common and passes_industry:
                filtered_articles.append(article)

        # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì˜µì…˜ ì ìš©
        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)

        total_count = len(filtered_articles)

        # í•´ë‹¹ íšŒì‚¬ì˜ ìš”ì•½ ë°ì´í„°(ì¤‘ë³µ ì œê±°, í•„í„°ë§ëœ) ì¤‘ ìµœì‹  2ê°œ ê¸°ì‚¬ ì¶”ì¶œ
        filtered_df = df[df.get(keyword_col, "") == company].sort_values(by='ë‚ ì§œ', ascending=False)
        hl_news = ["", ""]
        implications = ["", ""]
        for i, art in enumerate(filtered_df.itertuples()):
            if i > 1:
                break
            date_val = getattr(art, "ë‚ ì§œ", "") or ""
            title_val = getattr(art, "ê¸°ì‚¬ì œëª©", "") or getattr(art, "ì œëª©", "")
            link_val = getattr(art, "ë§í¬", "") or getattr(art, "link", "")
            display_text = f"({clean_text(date_val)}){clean_text(title_val)}"
            if title_val and link_val:
                hl_news[i] = f'=HYPERLINK("{clean_text(link_val)}", "{display_text}")'
            else:
                hl_news[i] = display_text or ""

            if implication_col:
                implications[i] = getattr(art, implication_col, "") or ""
            else:
                implications[i] = ""

        # â€˜í•œì¤„ ì‹œì‚¬ì â€™ì„ ë²ˆí˜¸ ë§¤ê²¨ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³‘í•© (ìµœëŒ€ 2ê°œ)
        merged_implication = ""
        if implications[0]:
            merged_implication += f"1. {implications[0]}"
        if implications[1]:
            if merged_implication:
                merged_implication += f"\n2. {implications[1]}"
            else:
                merged_implication = f"2. {implications[1]}"

        rows.append({
            "ê¸°ì—…ëª…": company,
            "í‘œê¸°ëª…": excel_sector_list[idx] if idx < len(excel_sector_list) else "",
            "ê±´ìˆ˜": total_count,
            "ì¤‘ìš”ë‰´ìŠ¤1": hl_news[0],
            "ì¤‘ìš”ë‰´ìŠ¤2": hl_news[1],
            "ì‹œì‚¬ì ": merged_implication
        })

    result_df = pd.DataFrame(rows, columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])

    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        result_df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
        worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
        for i, col in enumerate(result_df.columns):
            worksheet.set_column(i, i, 30)
    output.seek(0)
    return output

def generate_important_article_list(search_results, common_keywords, industry_keywords, favorites):
    """
    OpenAIë¥¼ ì´ìš©í•´ 'ì‹ ìš©í‰ê°€ ê´€ì ì—ì„œ ì¤‘ìš”í•œ ê¸°ì‚¬'ë¥¼ ìë™ ì„ ì •.
    - ê° ê¸°ì‚¬ì— ëŒ€í•´ ì‹ ìš©ì˜í–¥ë„(1~5ì )ë¥¼ í‰ê°€í•˜ê²Œ í•˜ê³ 
    - ë°˜ë“œì‹œ 5ì  ê¸°ì‚¬ë§Œ ìë™ ì„ ì • ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©.
    - ê²°ê³¼ëŠ” ê¸°ì‚¬ ë²ˆí˜¸ ê¸°ë°˜ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ ì›ë³¸ ê¸°ì‚¬(dict)ë¥¼ ë°˜í™˜.
    """
    import os
    from openai import OpenAI
    import re

    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    client = OpenAI(api_key=OPENAI_API_KEY)
    result = []

    # ì„¹í„°ë³„ í‚¤ì›Œë“œ íŒŒì‹± (get_industry_credit_keywords() ê¸°ë°˜)
    def parse_industry_keywords():
        raw_text = get_industry_credit_keywords()
        industry_dict = {}
        for line in raw_text.strip().split("\n"):
            if ":" in line:
                sector, keywords = line.split(":", 1)
                industry_dict[sector.strip()] = [
                    kw.strip() for kw in keywords.split(",") if kw.strip()
                ]
        return industry_dict

    industry_keywords_dict = parse_industry_keywords()

    # ---- ê° ì¹´í…Œê³ ë¦¬(ì„¹í„°) / íšŒì‚¬ë³„ë¡œ ì¤‘ìš” ê¸°ì‚¬ ì„ ì • ----
    for category, companies in favorites.items():
        sector_keywords = industry_keywords_dict.get(category, [])

        for comp in companies:
            articles = search_results.get(comp, [])
            if not articles:
                continue

            # ì„¹í„° í‚¤ì›Œë“œê°€ ì œëª©/ì„¤ëª…ì— ì–´ëŠ ì •ë„ í¬í•¨ëœ ê¸°ì‚¬ë§Œ 1ì°¨ í•„í„°
            target_articles = []
            for a in articles:
                text = (a.get("title", "") + " " + a.get("description", "")).lower()
                if sector_keywords and any(kw.lower() in text for kw in sector_keywords):
                    target_articles.append(a)
                elif not sector_keywords:
                    # ì„¹í„° í‚¤ì›Œë“œê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ì „ë¶€ í›„ë³´ë¡œ ì‚¬ìš©
                    target_articles.append(a)

            if not target_articles:
                continue

            # ê¸°ì‚¬ ëª©ë¡ì„ "ë²ˆí˜¸. ì œëª© - ë§í¬" í˜•íƒœë¡œ êµ¬ì„±
            prompt_list = "\n".join(
                [f"{i+1}. {a['title']} - {a['link']}" for i, a in enumerate(target_articles)]
            )

            # --- í”„ë¡¬í”„íŠ¸: 5ì  ê¸°ì‚¬ë§Œ ìë™ ì„ ì • ---
            guideline = f"""
ë‹¹ì‹ ì€ ì‹ ìš©í‰ê°€ì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

[ì‹ ìš©ì˜í–¥ë„ íŒë‹¨ ê¸°ì¤€]
5ì : ì‹ ìš©ë“±ê¸‰/ì „ë§ ë³€í™” ê°€ëŠ¥ì„±, ëŒ€ê·œëª¨ ìë³¸í™•ì¶©Â·ì°¨ì…, ìœ ë™ì„± ìœ„ê¸°, ë¶€ë„Â·ë²•ì •ê´€ë¦¬Â·íšŒìƒ ì‹ ì²­, ì¤‘ëŒ€í•œ ê·œì œÂ·ì œì¬Â·ì†Œì†¡ ë“±
4ì : ëŒ€ê·œëª¨ íˆ¬ìÂ·M&AÂ·ì§€ë¶„ë§¤ê°, ì‹¤ì  ê¸‰ë³€(í° í­ì˜ í‘ì/ì ì ë³€í™”), ë ˆë²„ë¦¬ì§€ ê¸‰ì¦, ê³„ì—´ì‚¬ì˜ ì‹ ìš©ìœ„í—˜ì´ ë³¸ì‚¬ì— ì¤‘ëŒ€í•œ ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±
3ì : ì¼ë°˜ì ì¸ ì‹¤ì  ê°œì„ /ì•…í™”, ì¤‘ê°„ ê·œëª¨ì˜ ìê¸ˆì¡°ë‹¬, ì‚¬ì—… í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì •(ë¹„í•µì‹¬ìì‚° ë§¤ê° ë“±)
2ì : ì‹ ì œí’ˆ ì¶œì‹œ, ë§ˆì¼€íŒ…/í”„ë¡œëª¨ì…˜, ì œíœ´Â·MOU, ì¼ë°˜ì ì¸ ì‚¬ì—… ê³„íš ë“± ì‹ ìš©ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ì œí•œì ì¸ ë‰´ìŠ¤
1ì : ì‚¬íšŒê³µí—Œ/í–‰ì‚¬/ESG í™ë³´, ë‹¨ìˆœ ì´ë¯¸ì§€ ì œê³ , ì—°ì˜ˆÂ·ë¬¸í™”Â·ìŠ¤í¬ì¸  ë“± ì‹ ìš©ê³¼ ê±°ì˜ ë¬´ê´€í•œ ë‚´ìš©

[ê¸°ì‚¬ ëª©ë¡]
{prompt_list}

ë¶„ì„ì˜ ì´ˆì ì€ ë°˜ë“œì‹œ "{comp}" ê¸°ì—…(ë˜ëŠ” í‚¤ì›Œë“œ)ì´ë©°,
"{category}" ì‚°ì—…ì˜ ì‹ ìš©í‰ê°€ ê´€ì ì—ì„œ ê° ë‰´ìŠ¤ê°€ ì‹ ìš©ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ë¥¼ ìœ„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì‹­ì‹œì˜¤.

[ì§€ì‹œì‚¬í•­]
1. ê° ê¸°ì‚¬ ë²ˆí˜¸ë³„ë¡œ ì‹ ìš©ì˜í–¥ë„ ì ìˆ˜(1~5ì )ë¥¼ í•œ ë²ˆì”©ë§Œ ë§¤ê¸°ì‹­ì‹œì˜¤.
2. ë°˜ë“œì‹œ **5ì ì¸ ê¸°ì‚¬ë§Œ** 'ì¤‘ìš” ê¸°ì‚¬ í›„ë³´'ë¡œ ê°„ì£¼í•˜ì‹­ì‹œì˜¤.
3. 5ì ì¸ ê¸°ì‚¬ ì¤‘ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì‚¬ ìµœëŒ€ 2ê±´ì˜ "ë²ˆí˜¸"ë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.
   - 5ì  ê¸°ì‚¬ 2ê±´ ì´ìƒì´ë©´ ê·¸ ì¤‘ì—ì„œ ìƒìœ„ 2ê±´ë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.
   - 5ì  ê¸°ì‚¬ 1ê±´ì´ë©´ ê·¸ 1ê±´ë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.
   - 5ì  ê¸°ì‚¬ 0ê±´ì´ë©´ ì–´ë–¤ ê¸°ì‚¬ë„ ì„ íƒí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
4. ì„ íƒëœ ë²ˆí˜¸ê°€ ì—†ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ë„ ì•„ë˜ [ì„ ì •] í˜•ì‹ì€ ìœ ì§€í•˜ë˜ 'ì—†ìŒ'ì´ë¼ê³  ì ìœ¼ì‹­ì‹œì˜¤.

ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ì„¤ëª… ë¬¸ì¥ì€ ë„£ì§€ ë§ˆì‹­ì‹œì˜¤.

[í‰ê°€]
1ë²ˆ: (ì ìˆ˜)
2ë²ˆ: (ì ìˆ˜)
...

[ì„ ì •]
[ì¤‘ìš”1]: (ê¸°ì‚¬ë²ˆí˜¸ ë˜ëŠ” ì—†ìŒ)
[ì¤‘ìš”2]: (ê¸°ì‚¬ë²ˆí˜¸ ë˜ëŠ” ì—†ìŒ)
"""

            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": guideline}],
                    max_tokens=600,
                    temperature=0,
                )
                answer = response.choices[0].message.content.strip()

                # --- [í‰ê°€]ì—ì„œ ê° ê¸°ì‚¬ ì ìˆ˜ íŒŒì‹± ---
                score_map = {}
                for line in answer.splitlines():
                    m = re.match(r"(\d+)ë²ˆ\s*:\s*([0-9]+)", line.strip())
                    if m:
                        no = int(m.group(1))
                        score = int(m.group(2))
                        score_map[no] = score

                # --- ì„ íƒëœ ê¸°ì‚¬ ë²ˆí˜¸ íŒŒì‹± ---
                sel1_match = re.search(r"\[ì¤‘ìš” ?1\]\s*:\s*(\d+)", answer)
                sel2_match = re.search(r"\[ì¤‘ìš” ?2\]\s*:\s*(\d+)", answer)

                raw_selected = []
                if sel1_match:
                    raw_selected.append(int(sel1_match.group(1)))
                if sel2_match:
                    raw_selected.append(int(sel2_match.group(1)))

                selected_indexes = []
                for no in raw_selected:
                    idx0 = no - 1
                    # âœ… ì‹¤ì œ ì ìˆ˜ê°€ 5ì ì¸ ê²ƒë§Œ ìœ ì§€
                    if score_map.get(no) == 5 and 0 <= idx0 < len(target_articles):
                        if idx0 not in selected_indexes:
                            selected_indexes.append(idx0)

                # âœ… 5ì ì´ ì—†ìœ¼ë©´ skip
                if not selected_indexes:
                    continue
                    
            except Exception:
                # ì—ëŸ¬ ì‹œ ì´ íšŒì‚¬ì— ëŒ€í•´ì„œëŠ” ìë™ì„ ì • ê±´ë„ˆëœ€
                continue

    return result

# --- REPLACE: robust article text extractor ---
def extract_article_text(url, fallback_desc=None, fallback_title=None):
    """
    ìš°ì„  newspaperë¡œ ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ BeautifulSoup <p> ê¸°ë°˜ ìˆ˜ë™ ì¶”ì¶œ â†’ ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´
    title/descriptionì„ ìµœì†Œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì—¬ ìš”ì•½ì´ ë™ì‘í•˜ë„ë¡ ë³´ì¥.
    """
    # 1) newspaper 1ì°¨ ì‹œë„
    try:
        import newspaper
        art = newspaper.Article(url, language="ko")
        art.download()
        art.parse()
        txt = (art.text or "").strip()
        if len(txt) >= 300:
            return txt
    except Exception:
        pass

    # 2) BeautifulSoup fallback (<p> ê¸°ë°˜)
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        # ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ ìš°ì„  íƒìƒ‰
        candidates = [
            "article", ".article", ".news_body", "#articleBodyContents",
            ".content", ".article-body", ".art_txt", ".article_view"
        ]
        blocks = []
        for sel in candidates:
            blocks.extend(soup.select(sel))

        paragraphs = []
        if blocks:
            for b in blocks:
                paragraphs.extend(b.select("p"))
        else:
            paragraphs = soup.select("p")

        text = " ".join(
            p.get_text(strip=True)
            for p in paragraphs
            if len(p.get_text(strip=True)) >= 30
        )
        text = " ".join(text.split())
        if len(text) >= 200:
            return text
    except Exception:
        pass

    # 3) ìµœì†Œ ë³´ì¥ (ì„¤ëª…/ì œëª© ê¸°ë°˜)
    if fallback_desc or fallback_title:
        return f"{(fallback_title or '').strip()} {(fallback_desc or '').strip()}".strip()

    return "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"
    
def extract_keyword_from_link(search_results, article_link):
    """
    ë‰´ìŠ¤ê²€ìƒ‰ê²°ê³¼ dictì™€ ê¸°ì‚¬ ë§í¬ë¡œ í•´ë‹¹ ê¸°ì‚¬ì˜ í‚¤ì›Œë“œ(íšŒì‚¬ëª…/ì¹´í…Œê³ ë¦¬)ë¥¼ ì¶”ì¶œ
    """
    for kw, arts in search_results.items():
        for art in arts:
            if art.get("link") == article_link:
                return kw
    return ""

def build_important_excel_format(important_articles, favorite_categories, excel_categories, search_results):
    import pandas as pd

    df = pd.DataFrame(important_articles)

    # íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°í•˜ë©° ìˆœì„œ ìœ ì§€)
    sector_list = []
    for cat in favorite_categories:
        sector_list.extend(favorite_categories[cat])
    sector_list = list(dict.fromkeys(sector_list))

    excel_sector_list = []
    for cat in excel_categories:
        excel_sector_list.extend(excel_categories[cat])
    excel_sector_list = list(dict.fromkeys(excel_sector_list))

    rows = []

    for idx, company in enumerate(sector_list):
        # ê¸°ì‚¬ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
        all_articles = search_results.get(company, [])

        filtered_articles = []
        for art in all_articles:
            if article_passes_filters(art):  # ë˜ëŠ” article_passes_filters(art) í•¨ìˆ˜ì— ë§ê²Œ ë³€ê²½
                filtered_articles.append(art)

        if 'remove_duplicate_articles' in st.session_state and st.session_state['remove_duplicate_articles']:
            filtered_articles = remove_duplicates(filtered_articles)

        total_count = len(filtered_articles)

        # í•´ë‹¹ íšŒì‚¬ì˜ ì„ íƒëœ ì¤‘ìš”ê¸°ì‚¬ ìš”ì•½ ë°ì´í„°(ì´ë¯¸ ì¤‘ë³µ ì œê±°, í•„í„°ë§ëœ)ë¥¼ ê°€ì ¸ì˜´
        filtered_df = df[df['ê¸°ì—…ëª…'] == company].sort_values(by='ë‚ ì§œ', ascending=False)

        hl_news = []
        for i, art in enumerate(filtered_df.itertuples()):
            if i > 1:
                break
            title = getattr(art, 'ì œëª©', '') or ''
            link = getattr(art, 'ë§í¬', '') or ''
            if title and link:
                hl_news.append(f'=HYPERLINK("{link}", "{title}")')
            else:
                hl_news.append(title or '')
        # 2ê°œê¹Œì§€ ì±„ìš°ê³  ë¶€ì¡±í•˜ë©´ ë¹ˆë¬¸ìì—´ ì±„ì›€
        while len(hl_news) < 2:
            hl_news.append('')

        # ì‹œì‚¬ì  ë³‘í•© (ìµœëŒ€ 2ê°œ)
        implication_col = 'ì‹œì‚¬ì ' if 'ì‹œì‚¬ì ' in df.columns else ('implication' if 'implication' in df.columns else None)
        implications = []
        for i, art in enumerate(filtered_df.itertuples()):
            if i > 1:
                break
            val = getattr(art, implication_col, '') if implication_col else ''
            implications.append(val)
        merged_implication = ''
        if implications:
            merged_implication = '\n'.join(f"{idx+1}. {txt}" for idx, txt in enumerate(implications) if txt)

        rows.append({
            'ê¸°ì—…ëª…': company,
            'í‘œê¸°ëª…': excel_sector_list[idx] if idx < len(excel_sector_list) else '',
            'ê±´ìˆ˜': total_count,
            'ì¤‘ìš”ë‰´ìŠ¤1': hl_news[0],
            'ì¤‘ìš”ë‰´ìŠ¤2': hl_news[1],
            'ì‹œì‚¬ì ': merged_implication
        })

    result_df = pd.DataFrame(rows, columns=['ê¸°ì—…ëª…', 'í‘œê¸°ëª…', 'ê±´ìˆ˜', 'ì¤‘ìš”ë‰´ìŠ¤1', 'ì¤‘ìš”ë‰´ìŠ¤2', 'ì‹œì‚¬ì '])

    from io import BytesIO
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        result_df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
        worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
        for i, col in enumerate(result_df.columns):
            worksheet.set_column(i, i, 30)
    output.seek(0)
    return output
   
def matched_filter_keywords(article, common_keywords, industry_keywords):
    """
    ê¸°ì‚¬ ì œëª©/ìš”ì•½/ë³¸ë¬¸ì—ì„œ ì‹¤ì œë¡œ í¬í•¨ëœ í•„í„° í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    text_candidates = [
        article.get("title", ""),
        article.get("description", ""),
        article.get("ìš”ì•½ë³¸", ""),
        article.get("ìš”ì•½", ""),
        article.get("full_text", ""),
        article.get("content", ""),
    ]
    text_long = " ".join([str(t) for t in text_candidates if t])
    matched_common = [kw for kw in common_keywords if kw in text_long]
    matched_industry = [kw for kw in industry_keywords if kw in text_long]
    return list(set(matched_common + matched_industry))

def render_articles_with_single_summary_and_telegram(
    results, show_limit, show_sentiment_badge=True, enable_summary=True
):
    SENTIMENT_CLASS = {"ê¸ì •": "sentiment-positive", "ë¶€ì •": "sentiment-negative"}
    col_list, col_summary = st.columns([1, 1])

    # ---------------------------- ë‰´ìŠ¤ ëª©ë¡ ì—´ ---------------------------- #
    with col_list:
        st.markdown("### ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼")
        for category_name, company_list in favorite_categories.items():
            companies_with_results = [c for c in company_list if c in results]
            if not companies_with_results:
                continue

            with st.expander(f"ğŸ“‚ {category_name}", expanded=True):
                for company in companies_with_results:
                    articles = results[company]

                    with st.expander(f"[{company}] ({len(articles)}ê±´)", expanded=False):
                        # ì´ íšŒì‚¬ì— ì†í•œ ëª¨ë“  ê¸°ì‚¬ key ìˆ˜ì§‘
                        all_article_keys = []
                        for idx, article in enumerate(articles):
                            uid = re.sub(r"\W+", "", article["link"])[-16:]
                            key = f"{company}_{idx}_{uid}"
                            all_article_keys.append(key)

                        # âœ… ë§ˆìŠ¤í„° ì²´í¬ë°•ìŠ¤ key ë¥¼ ì™„ì „íˆ ìœ ì¼í•˜ê²Œ ìƒì„± (ì¹´í…Œê³ ë¦¬+íšŒì‚¬ ê¸°ë°˜)
                        slug = re.sub(r"\W+", "", f"{category_name}_{company}")
                        master_key = f"left_master_{slug}_select_all"

                        prev_value = all(
                            st.session_state.article_checked.get(k, False)
                            for k in all_article_keys
                        )

                        select_all = st.checkbox(
                            f"ì „ì²´ ê¸°ì‚¬ ì„ íƒ/í•´ì œ ({company})",
                            value=prev_value,
                            key=master_key,
                        )

                        # ë§ˆìŠ¤í„° ì²´í¬ë°•ìŠ¤ ê°’ì´ ë°”ë€ ê²½ìš° â†’ ê°œë³„ ì²´í¬ë°•ìŠ¤ & ìƒíƒœ ë™ê¸°í™”
                        if select_all != prev_value:
                            for k in all_article_keys:
                                st.session_state.article_checked[k] = select_all
                                st.session_state.article_checked_left[k] = select_all
                                # ì‹¤ì œ ê°œë³„ ê¸°ì‚¬ ì²´í¬ë°•ìŠ¤ ìœ„ì ¯ ìƒíƒœë„ ê°™ì´ ë³€ê²½
                                st.session_state[f"news_{k}"] = select_all
                            st.rerun()

                        # ê°œë³„ ê¸°ì‚¬ í‘œì‹œ
                        for idx, article in enumerate(articles):
                            uid = re.sub(r"\W+", "", article["link"])[-16:]
                            key = f"{company}_{idx}_{uid}"
                            cache_key = f"summary_{key}"

                            cols = st.columns([0.04, 0.96])
                            with cols[0]:
                                checked = st.checkbox(
                                    "",
                                    value=st.session_state.article_checked.get(key, False),
                                    key=f"news_{key}",
                                )

                            with cols[1]:
                                sentiment = ""
                                if show_sentiment_badge and cache_key in st.session_state:
                                    _, _, sentiment, _, _ = st.session_state[cache_key]

                                badge_html = (
                                    f"<span class='sentiment-badge "
                                    f"{SENTIMENT_CLASS.get(sentiment, 'sentiment-neutral')}'>{sentiment}</span>"
                                    if sentiment else ""
                                )
                                search_word_info = (
                                    f" | ê²€ìƒ‰ì–´: {article.get('ê²€ìƒ‰ì–´', '')}"
                                    if article.get("ê²€ìƒ‰ì–´") else ""
                                )

                                st.markdown(
                                    f"<span class='news-title'><a href='{article['link']}' "
                                    f"target='_blank'>{article['title']}</a></span> "
                                    f"{badge_html} {article['date']} | {article['source']}{search_word_info}",
                                    unsafe_allow_html=True,
                                )

                            # ì„¸ì…˜ ìƒíƒœ ê°±ì‹ 
                            st.session_state.article_checked_left[key] = checked
                            st.session_state.article_checked[key] = checked


    # ---------------------------- ì„ íƒ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„ ì—´ ---------------------------- #
    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):
            industry_keywords_all = []
            if st.session_state.get("use_industry_filter", False):
                for sublist in st.session_state.industry_major_sub_map.values():
                    industry_keywords_all.extend(sublist)

            grouped_selected = {}
            for cat_name, company_list in favorite_categories.items():
                for company in company_list:
                    if company in results:
                        for idx, article in enumerate(results[company]):
                            uid = re.sub(r"\W+", "", article["link"])[-16:]
                            key = f"{company}_{idx}_{uid}"
                            if st.session_state.article_checked.get(key, False):
                                grouped_selected.setdefault(cat_name, {}).setdefault(company, []).append(
                                    (company, idx, article)
                                )

            def process_article(item):
                keyword, idx, art = item
                cache_key = f"summary_{keyword}_{idx}_" + re.sub(r"\W+", "", art["link"])[-16:]
                if cache_key in st.session_state:
                    one_line, summary, sentiment, implication, short_implication, full_text = st.session_state[cache_key]
                else:
                    one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(
                        art["link"], art["title"], do_summary=enable_summary, target_keyword=keyword
                    )
                    st.session_state[cache_key] = (one_line, summary, sentiment, implication, short_implication, full_text)
                filter_hits = matched_filter_keywords(
                    {"title": art["title"], "ìš”ì•½ë³¸": summary, "ìš”ì•½": one_line, "full_text": full_text},
                    ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords_all
                )
                return {
                    "í‚¤ì›Œë“œ": keyword,
                    "í•„í„°íˆíŠ¸": ", ".join(filter_hits),
                    "ê¸°ì‚¬ì œëª©": safe_title(art["title"]),
                    "ìš”ì•½": one_line,
                    "ìš”ì•½ë³¸": summary,
                    "ê°ì„±": sentiment,
                    "ì‹œì‚¬ì ": implication,
                    "í•œì¤„ì‹œì‚¬ì ": short_implication,  
                    "ë§í¬": art["link"],
                    "ë‚ ì§œ": art["date"],
                    "ì¶œì²˜": art["source"],
                    "full_text": full_text or "",
                }

            from concurrent.futures import ThreadPoolExecutor
            for cat_name, comp_map in grouped_selected.items():
                for company, items in comp_map.items():
                    with ThreadPoolExecutor(max_workers=10) as executor:
                        grouped_selected[cat_name][company] = list(executor.map(process_article, items))

            total_selected_count = 0
            for cat_name, comp_map in grouped_selected.items():
                with st.expander(f"ğŸ“‚ {cat_name}", expanded=True):
                    for company, arts in comp_map.items():
                        with st.expander(f"[{company}] ({len(arts)}ê±´)", expanded=True):
                            for art in arts:
                                total_selected_count += 1
                                st.markdown(
                                    f"#### <span class='news-title'><a href='{art['ë§í¬']}' target='_blank'>{art['ê¸°ì‚¬ì œëª©']}</a></span> "
                                    f"<span class='sentiment-badge {SENTIMENT_CLASS.get(art['ê°ì„±'], 'sentiment-neutral')}'>{art['ê°ì„±']}</span>",
                                    unsafe_allow_html=True
                                )
                                st.markdown(f"- **ê²€ìƒ‰ í‚¤ì›Œë“œ:** `{art['í‚¤ì›Œë“œ']}`")
                                st.markdown(f"- **í•„í„°ë¡œ ì¸ì‹ëœ í‚¤ì›Œë“œ:** `{art['í•„í„°íˆíŠ¸'] or 'ì—†ìŒ'}`")
                                st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {art['ë‚ ì§œ']} | {art['ì¶œì²˜']}")
                                if enable_summary:
                                    st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {art['ìš”ì•½']}")
                                    st.markdown(f"- **í•œ ì¤„ ì‹œì‚¬ì :** {art.get('í•œì¤„ì‹œì‚¬ì ', 'ì—†ìŒ')}")
                                    st.markdown(f"- **ì‹œì‚¬ì :** {art['ì‹œì‚¬ì '] or 'ì—†ìŒ'}")
                                st.markdown(f"- **ê°ì„±ë¶„ì„:** `{art['ê°ì„±']}`")
                                st.markdown("---")

            st.session_state.selected_articles = [
                art for comp_map in grouped_selected.values() for arts in comp_map.values() for art in arts
            ]
            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {total_selected_count}")

            # ë‹¤ìš´ë¡œë“œ / ì „ì²´ í•´ì œ
            col_dl1, col_dl2 = st.columns([0.55, 0.45])
            with col_dl1:
                st.download_button(
                    label="ğŸ“¥ ë§ì¶¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=get_excel_download_with_favorite_and_excel_company_col(
                        st.session_state.selected_articles,
                        favorite_categories,
                        excel_company_categories,
                        st.session_state.search_results
                    ).getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë§ì¶¤í˜•.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            with col_dl2:
                if st.button("ğŸ—‘ ì„ íƒ í•´ì œ (ì „ì²´)"):
                    for key in list(st.session_state.article_checked.keys()):
                        st.session_state.article_checked[key] = False
                    for key in list(st.session_state.article_checked_left.keys()):
                        st.session_state.article_checked_left[key] = False
                    st.rerun()

        render_important_article_review_and_download()

def render_important_article_review_and_download():
    import re
    from collections import defaultdict
    import streamlit as st

    with st.container(border=True):
        st.markdown("### â­ ì¤‘ìš” ê¸°ì‚¬ ë¦¬ë·° ë° í¸ì§‘")

        auto_btn = st.button("ğŸš€ OpenAI ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ìë™ ì„ ì •")
        if auto_btn:
            with st.spinner("OpenAIë¡œ ì¤‘ìš” ë‰´ìŠ¤ ì„ ì • ì¤‘..."):
                filtered_results_for_important = {}
                for keyword, articles in st.session_state.search_results.items():
                    filtered_articles = [a for a in articles if article_passes_all_filters(a)]
                    if st.session_state.get("remove_duplicate_articles", False):
                        filtered_articles = remove_duplicates(filtered_articles)
                    if filtered_articles:
                        filtered_results_for_important[keyword] = filtered_articles

                important_articles = generate_important_article_list(
                    search_results=filtered_results_for_important,
                    common_keywords=ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords=st.session_state.get("industry_sub", []),
                    favorites=favorite_categories
                )
                # key ëª… í†µì¼ ë° ì‹œì‚¬ì  í•„ë“œ í¬í•¨ (ì‹œì‚¬ì ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”, í•„ìš” ì‹œ OpenAI ê²°ê³¼ ë°˜ì˜ ê°€ëŠ¥)
                for i, art in enumerate(important_articles):
                    important_articles[i] = {
                        "í‚¤ì›Œë“œ": art.get("í‚¤ì›Œë“œ") or art.get("íšŒì‚¬ëª…") or art.get("keyword") or "",
                        "ê¸°ì‚¬ì œëª©": art.get("ê¸°ì‚¬ì œëª©") or art.get("ì œëª©") or art.get("title") or "",
                        "ê°ì„±": art.get("ê°ì„±", ""),
                        "ë§í¬": art.get("ë§í¬") or art.get("link", ""),
                        "ë‚ ì§œ": art.get("ë‚ ì§œ") or art.get("date", ""),
                        "ì¶œì²˜": art.get("ì¶œì²˜") or art.get("source", ""),
                        "ì‹œì‚¬ì ": art.get("ì‹œì‚¬ì ", "")  # ì‹œì‚¬ì  í•„ë“œ ì¶”ê°€ (ìë™ì„ ì • ì‹œ ì±„ì›Œì§ˆ ìˆ˜ ìˆìŒ)
                    }
                st.session_state["important_articles_preview"] = important_articles
                st.session_state["important_selected_index"] = []

        articles = st.session_state.get("important_articles_preview", [])
        selected_indexes = st.session_state.get("important_selected_index", [])

        # ëŒ€ë¶„ë¥˜(major) - ì†Œë¶„ë¥˜(minor) ê·¸ë£¹í™”
        major_map = defaultdict(lambda: defaultdict(list))  # major_map[ëŒ€ë¶„ë¥˜][ì†Œë¶„ë¥˜] = [ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸]
        for art in articles:
            keyword = art.get("í‚¤ì›Œë“œ") or art.get("íšŒì‚¬ëª…") or ""
            found_major = None
            for major, minors in favorite_categories.items():
                if keyword in minors:
                    found_major = major
                    break
            if found_major:
                major_map[found_major][keyword].append(art)

        st.markdown("ğŸ¯ **ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡ (êµì²´ ë˜ëŠ” ì‚­ì œí•  í•­ëª©ì„ ì²´í¬í•˜ì„¸ìš”)**")

        from concurrent.futures import ThreadPoolExecutor
        one_line_map = {}
        to_summarize = []

        for major, minor_map in major_map.items():
            for minor, arts in minor_map.items():
                for idx, article in enumerate(arts):
                    link = article.get("ë§í¬", "")
                    cleaned_id = re.sub(r"\W+", "", link)[-16:] if link else ""
                    cache_hit = False
                    for k, v in st.session_state.items():
                        if k.startswith("summary_") and cleaned_id in k and isinstance(v, tuple):
                            one_line_map[(major, minor, idx)] = v
                            cache_hit = True
                            break
                    if not cache_hit and link:
                        to_summarize.append((major, minor, idx, link, article.get("ê¸°ì‚¬ì œëª©", "")))

        if to_summarize:
            with st.spinner("ì¤‘ìš” ê¸°ì‚¬ ìš”ì•½ ìƒì„± ì¤‘..."):
                def get_one_line(args):
                    major, minor, idx, link, title = args
                    one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(link, title, do_summary=True)
                    return (major, minor, idx), (one_line, summary, sentiment, implication, short_implication, full_text)

                with ThreadPoolExecutor(max_workers=10) as executor:
                    for key, data_tuple in executor.map(get_one_line, to_summarize):
                        one_line_map[key] = data_tuple

        new_selection = []
        if to_summarize:
            with st.spinner("ì¤‘ìš” ê¸°ì‚¬ ìš”ì•½ ìƒì„± ì¤‘."):
                def get_one_line(args):
                    major, minor, idx, link, title = args
                    one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(
                        link, title, do_summary=True
                    )
                    return (major, minor, idx), (one_line, summary, sentiment, implication, short_implication, full_text)

                with ThreadPoolExecutor(max_workers=10) as executor:
                    for key, data_tuple in executor.map(get_one_line, to_summarize):
                        one_line_map[key] = data_tuple

        new_selection = []

        for major, minor_map in major_map.items():
            with st.expander(f"ğŸ“Š {major}", expanded=True):
                for minor, arts in minor_map.items():
                    with st.expander(f"{minor} ({len(arts)}ê±´)", expanded=False):
                        for idx, article in enumerate(arts):
                            check_key = f"important_chk_{major}_{minor}_{idx}"

                            # í•œ ì¤„ì— ì²´í¬ë°•ìŠ¤ + ê°ì„± + ê¸°ì‚¬ì œëª© í•˜ì´í¼ë§í¬ ë°°ì¹˜
                            cols = st.columns([0.06, 0.94])

                            # âœ… ì™¼ìª½: ì²´í¬ë°•ìŠ¤
                            with cols[0]:
                                checked = st.checkbox(
                                    "",
                                    key=check_key,
                                    value=((major, minor, idx) in selected_indexes),
                                )

                            # âœ… ì˜¤ë¥¸ìª½: ê¸°ì‚¬ ì •ë³´ ë° ì‹œì‚¬ì 
                            with cols[1]:
                                st.markdown(
                                    f"{article.get('ê°ì„±','')} | "
                                    f"<a href='{article.get('ë§í¬','')}' target='_blank'>"
                                    f"{article.get('ê¸°ì‚¬ì œëª©','ì œëª©ì—†ìŒ')}</a>",
                                    unsafe_allow_html=True,
                                )

                                # ì‹œì‚¬ì  ë° í•œì¤„ ì‹œì‚¬ì  ì¶œë ¥
                                summary_data = one_line_map.get((major, minor, idx))
                                implication_text = ""
                                short_implication_text = ""

                                if summary_data and len(summary_data) == 6:
                                    implication_text = summary_data[3] or ""       # ì‹œì‚¬ì 
                                    short_implication_text = summary_data[4] or ""  # í•œì¤„ ì‹œì‚¬ì 
                                else:
                                    implication_text = article.get("ì‹œì‚¬ì ", "") or ""
                                    short_implication_text = article.get("í•œì¤„ì‹œì‚¬ì ", "") or ""

                                if implication_text:
                                    st.markdown(implication_text)
                                if short_implication_text:
                                    st.markdown(
                                        f"<span style='color:gray;font-style:italic;'>{short_implication_text}</span>",
                                        unsafe_allow_html=True,
                                    )

                                st.markdown(
                                    f"<span style='font-size:12px;color:#99a'>"
                                    f"{article.get('ë‚ ì§œ', '')} | {article.get('ì¶œì²˜', '')}</span>",
                                    unsafe_allow_html=True,
                                )

                                st.markdown(
                                    "<div style='margin:0px;padding:0px;height:4px'></div>",
                                    unsafe_allow_html=True,
                                )

                            # âœ… ì„ íƒ ìƒíƒœ ë°˜ì˜
                            if checked:
                                new_selection.append((major, minor, idx))

        st.session_state["important_selected_index"] = new_selection
        
        # ì¶”ê°€ / ì‚­ì œ / êµì²´ ë²„íŠ¼ ë° í•´ë‹¹ ê¸°ëŠ¥ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        col_add, col_del, col_rep = st.columns([0.3, 0.35, 0.35])
        with col_add:
            if st.button("â• ì„ íƒ ê¸°ì‚¬ ì¶”ê°€"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                if not left_selected_keys:
                    st.warning("ì™¼ìª½ ë‰´ìŠ¤ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì ì–´ë„ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")
                else:
                    added_count = 0
                    important = st.session_state.get("important_articles_preview", [])
                    for from_key in left_selected_keys:
                        m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                        if not m:
                            continue
                        key_tail = m.group(1)
                        selected_article, article_link = None, None
                        for kw, arts in st.session_state.search_results.items():
                            for art in arts:
                                uid = re.sub(r'\W+', '', art['link'])[-16:]
                                if uid == key_tail:
                                    selected_article = art
                                    article_link = art["link"]
                                    break
                            if selected_article:
                                break
                        if not selected_article:
                            continue

                        keyword = extract_keyword_from_link(st.session_state.search_results, article_link)
                        cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                        sentiment = None
                        for k in st.session_state.keys():
                            if k.startswith("summary_") and cleaned_id in k:
                                sentiment = st.session_state[k][2]
                                break
                        if not sentiment:
                            _, _, sentiment, _, _ = summarize_article_from_url(
                                selected_article["link"], selected_article["title"]
                            )
                        new_article = {
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": selected_article["title"],
                            "ê°ì„±": sentiment or "",
                            "ë§í¬": selected_article["link"],
                            "ë‚ ì§œ": selected_article["date"],
                            "ì¶œì²˜": selected_article["source"],
                            "ì‹œì‚¬ì ": ""  # ì‹œì‚¬ì  í•„ë“œ ì´ˆê¸°ê°’ ë¹ˆ ë¬¸ìì—´
                        }
                        if not any(a["ë§í¬"] == new_article["ë§í¬"] for a in important):
                            important.append(new_article)
                            added_count += 1
                        st.session_state.article_checked_left[from_key] = False
                        st.session_state.article_checked[from_key] = False
                    st.session_state["important_articles_preview"] = important
                    if added_count > 0:
                        st.success(f"{added_count}ê±´ì˜ ê¸°ì‚¬ê°€ ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.info("ì¶”ê°€ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.experimental_rerun()

        with col_del:
            if st.button("ğŸ—‘ ì„ íƒ ê¸°ì‚¬ ì‚­ì œ"):
                important = st.session_state.get("important_articles_preview", [])
                remove_links = []
                for major, minor, idx in st.session_state["important_selected_index"]:
                    try:
                        link = major_map[major][minor][idx]["ë§í¬"]
                        remove_links.append(link)
                    except Exception:
                        continue
                important = [a for a in important if a.get("ë§í¬") not in remove_links]
                st.session_state["important_articles_preview"] = important
                st.session_state["important_selected_index"] = []
                st.experimental_rerun()

        with col_rep:
            if st.button("ğŸ” ì„ íƒ ê¸°ì‚¬ êµì²´"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                right_selected_indexes = st.session_state["important_selected_index"]
                if len(left_selected_keys) != 1 or len(right_selected_indexes) != 1:
                    st.warning("ì™¼ìª½ 1ê°œ, ì˜¤ë¥¸ìª½ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    return
                from_key = left_selected_keys[0]
                (target_major, target_minor, target_idx) = right_selected_indexes[0]
                m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                if not m:
                    st.warning("ê¸°ì‚¬ ì‹ë³„ì íŒŒì‹± ì‹¤íŒ¨")
                    return
                key_tail = m.group(1)
                selected_article, article_link = None, None
                for kw, art_list in st.session_state.search_results.items():
                    for art in art_list:
                        uid = re.sub(r'\W+', '', art['link'])[-16:]
                        if uid == key_tail:
                            selected_article = art
                            article_link = art["link"]
                            break
                    if selected_article:
                        break
                if not selected_article:
                    st.warning("ì™¼ìª½ì—ì„œ ì„ íƒí•œ ê¸°ì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return

                keyword = extract_keyword_from_link(st.session_state.search_results, article_link)
                cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                sentiment = None
                for k in st.session_state.keys():
                    if k.startswith("summary_") and cleaned_id in k:
                        sentiment = st.session_state[k][2]
                        break
                if not sentiment:
                    _, _, sentiment, _, _ = summarize_article_from_url(
                        selected_article["link"], selected_article["title"]
                    )
                important = st.session_state.get("important_articles_preview", [])
                remove_link = major_map[target_major][target_minor][target_idx]["ë§í¬"]
                important = [a for a in important if a.get("ë§í¬") != remove_link]
                new_article = {
                    "í‚¤ì›Œë“œ": keyword,
                    "ê¸°ì‚¬ì œëª©": selected_article["title"],
                    "ê°ì„±": sentiment or "",
                    "ë§í¬": selected_article["link"],
                    "ë‚ ì§œ": selected_article["date"],
                    "ì¶œì²˜": selected_article["source"],
                    "ì‹œì‚¬ì ": ""  # ì‹œì‚¬ì  í•„ë“œ ì´ˆê¸°ê°’ ë¹ˆ ë¬¸ìì—´
                }
                important.append(new_article)
                st.session_state["important_articles_preview"] = important
                st.session_state.article_checked_left[from_key] = False
                st.session_state.article_checked[from_key] = False
                st.session_state["important_selected_index"] = []
                st.success("ì¤‘ìš” ê¸°ì‚¬ êµì²´ ì™„ë£Œ")
                st.experimental_rerun()

        st.markdown("---")
        st.markdown("ğŸ“¥ **ë¦¬ë·°í•œ ì¤‘ìš” ê¸°ì‚¬ë“¤ì„ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.**")
        articles_source = st.session_state.get("important_articles_preview", [])
        industry_keywords_all = []
        if st.session_state.get("use_industry_filter", False):
            for sublist in st.session_state.industry_major_sub_map.values():
                industry_keywords_all.extend(sublist)

        def enrich_article_for_excel(raw_article):
            link = raw_article.get("ë§í¬", "")
            keyword = raw_article.get("í‚¤ì›Œë“œ", "")
            cleaned_id = re.sub(r"\W+", "", link)[-16:]

            one_line, summary, sentiment, implication, short_implication, full_text = None, None, None, None, None, None

            for k, v in st.session_state.items():
                if k.startswith("summary_") and cleaned_id in k and isinstance(v, tuple):
                    one_line, summary, sentiment, implication, short_implication, full_text = v
                    break

            if not sentiment:
                one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(
                    link, raw_article.get("ê¸°ì‚¬ì œëª©", "")
                )
            filter_hits = matched_filter_keywords(
                {"title": raw_article.get("ê¸°ì‚¬ì œëª©", ""), "ìš”ì•½ë³¸": summary,
                 "ìš”ì•½": one_line, "full_text": full_text},
                ALL_COMMON_FILTER_KEYWORDS,
                industry_keywords_all
            )
            return {
                "í‚¤ì›Œë“œ": keyword,
                "í•„í„°íˆíŠ¸": ", ".join(filter_hits),
                "ê¸°ì‚¬ì œëª©": safe_title(raw_article.get("ê¸°ì‚¬ì œëª©", "")),
                "ìš”ì•½": one_line,
                "ìš”ì•½ë³¸": summary,
                "ê°ì„±": sentiment,
                "ì‹œì‚¬ì ": implication,
                "í•œì¤„ì‹œì‚¬ì ": short_implication,   # í•œì¤„ ì‹œì‚¬ì  í•„ë“œ ì¶”ê°€
                "ë§í¬": link,
                "ë‚ ì§œ": raw_article.get("ë‚ ì§œ", ""),
                "ì¶œì²˜": raw_article.get("ì¶œì²˜", ""),
                "full_text": full_text or "",
            }
        summary_data = [enrich_article_for_excel(a) for a in articles_source]

        # ì—¬ê¸°ì—ì„œ ì—‘ì…€ ìƒì„± ì‹œ í•œì¤„ì‹œì‚¬ì  ë°˜ì˜í•˜ì—¬ í†µí•©
        def get_excel_with_joined_implications(summary_data, favorite_categories, excel_company_categories, search_results):
            import pandas as pd
            from io import BytesIO

            if not summary_data or len(summary_data) == 0:
                df_empty = pd.DataFrame(columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])
                output = BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    df_empty.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
                    worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
                    worksheet.set_column(0, 5, 30)
                output.seek(0)
                return output

            df = pd.DataFrame(summary_data)

            # íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€)
            sector_list = []
            for cat in favorite_categories:
                sector_list.extend(favorite_categories[cat])
            sector_list = list(dict.fromkeys(sector_list))

            excel_sector_list = []
            for cat in excel_company_categories:
                excel_sector_list.extend(excel_company_categories[cat])
            excel_sector_list = list(dict.fromkeys(excel_sector_list))

            rows = []
            for idx, company in enumerate(sector_list):
                search_articles = search_results.get(company, [])

                filtered_articles = []
                for article in search_articles:
                    passes_common = any(kw in (article.get("title", "") + article.get("description", "")) for kw in ALL_COMMON_FILTER_KEYWORDS)
                    passes_industry = True
                    # í•„ìš” ì‹œ ì‚°ì—…ë³„ í•„í„°ë§ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

                    if passes_common and passes_industry:
                        filtered_articles.append(article)

                if st.session_state.get("remove_duplicate_articles", False):
                    filtered_articles = remove_duplicates(filtered_articles)

                total_count = len(filtered_articles)

                filtered_df = df[df.get("í‚¤ì›Œë“œ", "") == company].sort_values(by='ë‚ ì§œ', ascending=False)

                hl_news = ["", ""]
                implications = ["", ""]
                short_imps = ["", ""]

                for i, art in enumerate(filtered_df.itertuples()):
                    if i > 1:
                        break
                    date_val = getattr(art, "ë‚ ì§œ", "") or ""
                    title_val = getattr(art, "ê¸°ì‚¬ì œëª©", "") or getattr(art, "ì œëª©", "")
                    link_val = getattr(art, "ë§í¬", "") or getattr(art, "link", "")
                    short_imp_val = getattr(art, "í•œì¤„ì‹œì‚¬ì ", "") or ""

                    display_text = f"({clean_excel_formula_text(date_val)}){clean_excel_formula_text(title_val)}"
                    if title_val and link_val:
                        hl_news[i] = f'=HYPERLINK("{clean_excel_formula_text(link_val)}", "{display_text}")'
                    else:
                        hl_news[i] = display_text or ""

                    implications[i] = getattr(art, "ì‹œì‚¬ì ", "") or ""
                    short_imps[i] = short_imp_val

                # ì‹œì‚¬ì  ë° í•œì¤„ì‹œì‚¬ì  ë²ˆí˜¸ ë¶™ì—¬ì„œ ë³‘í•©
                merged_implications = ""
                for n in range(2):
                    if implications[n]:
                        merged_implications += f"{n+1}. {implications[n]}\n"
                for n in range(2):
                    if short_imps[n]:
                        merged_implications += f"{n+1}. {short_imps[n]}\n"

                rows.append({
                    "ê¸°ì—…ëª…": company,
                    "í‘œê¸°ëª…": excel_sector_list[idx] if idx < len(excel_sector_list) else "",
                    "ê±´ìˆ˜": total_count,
                    "ì¤‘ìš”ë‰´ìŠ¤1": hl_news[0],
                    "ì¤‘ìš”ë‰´ìŠ¤2": hl_news[1],
                    "ì‹œì‚¬ì ": merged_implications.strip(),
                })

            result_df = pd.DataFrame(rows, columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])

            output = BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                result_df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
                worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
                for i, col in enumerate(result_df.columns):
                    worksheet.set_column(i, i, 30)
            output.seek(0)
            return output

        excel_data = get_excel_with_joined_implications(summary_data, favorite_categories, excel_company_categories, st.session_state.search_results)

        st.download_button(
            label="ğŸ“¥ ì¤‘ìš” ê¸°ì‚¬ ìµœì¢… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ë§ì¶¤ ì–‘ì‹)",
            data=excel_data.getvalue(),
            file_name=f"ì¤‘ìš”ë‰´ìŠ¤_ìµœì¢…ì„ ì •_ì–‘ì‹_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

if st.session_state.get("search_results"):
    filtered_results = {}
    for keyword, articles in st.session_state["search_results"].items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]

        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)

        if filtered_articles:
            filtered_results[keyword] = filtered_articles

    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )

    selected_companies = []
    for cat in st.session_state.get("cat_multi", []):
        selected_companies.extend(favorite_categories.get(cat, []))
    selected_companies = list(set(selected_companies))

    # kiscd_mapê³¼ cmpCD_map ëª¨ë‘ì—ì„œ íšŒì‚¬ëª…ì— ë§¤ì¹­ë˜ëŠ” í‚¤ ê°’ ê°€ì ¸ì˜¤ê¸°
    kiscd_filtered = {c: kiscd_map[c] for c in selected_companies if c in kiscd_map}
    cmpcd_filtered = {c: config.get("cmpCD_map", {}).get(c, "") for c in selected_companies}

    # ë‘ ë§µì„ í•©ì¹˜ëŠ” í•¨ìˆ˜ (kiscd_filtered ê¸°ë³¸ì— cmpcd_filteredë„ í•©ì¹  ìˆ˜ ìˆë„ë¡)
    # fetch_and_display_reportsê°€ kiscdë§Œ ë°›ìœ¼ë¯€ë¡œ í™•ì¥ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” kiscd_filtered ë„˜ê¸°ê³ , fetch_and_display_reports ë‚´ë¶€ì—ì„œ cmpCD_map ì°¸ì¡° ê¶Œì¥

    fetch_and_display_reports(kiscd_filtered)

else:
    st.info("ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")

