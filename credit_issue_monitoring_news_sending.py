import os
import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
from datetime import datetime, timedelta
import telepot
from openai import OpenAI
import newspaper
import difflib
from urllib.parse import urlparse

# --- CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] { background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold; }
.sentiment-badge { display: inline-block; padding: 0.08em 0.6em; margin-left: 0.2em; border-radius: 0.8em; font-size: 0.85em; font-weight: bold; vertical-align: middle; }
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
</style>
""", unsafe_allow_html=True)

# --- ì œì™¸ í‚¤ì›Œë“œ ---
EXCLUDE_TITLE_KEYWORDS = [
    "ì•¼êµ¬", "ì¶•êµ¬", "ë°°êµ¬", "ë†êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "ì˜¬ë¦¼í”½", "ì›”ë“œì»µ", "Kë¦¬ê·¸", "í”„ë¡œì•¼êµ¬", "í”„ë¡œì¶•êµ¬", "í”„ë¡œë°°êµ¬", "í”„ë¡œë†êµ¬",
    "ë¶€ê³ ", "ë¶€ìŒ", "ì¸ì‚¬", "ìŠ¹ì§„", "ì„ëª…", "ë°œë ¹", "ì¸ì‚¬ë°œë ¹", "ì¸ì‚¬ì´ë™",
    "ë¸Œëœë“œí‰íŒ", "ë¸Œëœë“œ í‰íŒ", "ë¸Œëœë“œ ìˆœìœ„", "ë¸Œëœë“œì§€ìˆ˜",
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì£¼ê°€", "ì£¼ì‹", "ì¦ì‹œ", "ì‹œì„¸", "ë§ˆê°", "ì¥ì¤‘", "ì¥ë§ˆê°", "ê±°ë˜ëŸ‰", "ê±°ë˜ëŒ€ê¸ˆ", "ìƒí•œê°€", "í•˜í•œê°€",
    "ë´‰ì‚¬", "í›„ì›", "ê¸°ë¶€", "ìš°ìŠ¹", "ë¬´ìŠ¹ë¶€", "íŒ¨ë°°", "ìŠ¤í¬ì¸ ", "ìŠ¤í°ì„œ", "ì§€ì†ê°€ëŠ¥", "ESG", "ìœ„ì´‰", "ì´ë²¤íŠ¸", "ì‚¬ì „ì˜ˆì•½", "ì±”í”„ì „",
    "í”„ë¡œëª¨ì…˜", "ì—°ê·¹", "ê³µì—°", "ì–´ë¥´ì‹ "
]
def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

# --- ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” ---
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False
if "selected_articles" not in st.session_state:
    st.session_state.selected_articles = []
if "cat_major_autoset" not in st.session_state:
    st.session_state.cat_major_autoset = []
if "important_articles_preview" not in st.session_state:
    st.session_state.important_articles_preview = []
if "important_selected_index" not in st.session_state:
    st.session_state["important_selected_index"] = []
if "article_checked_left" not in st.session_state:
    st.session_state.article_checked_left = {}

# --- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬(ë³€ê²½ ê¸ˆì§€) ---
favorite_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": ["í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ"],
    "5ëŒ€ì‹œì¤‘ì€í–‰": ["ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"],
    "ì¹´ë“œì‚¬": ["KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"],
    "ìºí”¼íƒˆ": ["í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": ["SKì´ë…¸ë² ì´ì…˜", "GSì—ë„ˆì§€", "SK", "GS"],
    "ì—ë„ˆì§€": ["SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "SKì•¤ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"],
    "ë°œì „": ["GSíŒŒì›Œ", "GSEPS", "ì‚¼ì²œë¦¬"],
    "ìë™ì°¨": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´"],
    "ì „ê¸°/ì „ì": ["SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "LSì¼ë ‰íŠ¸ë¦­"],
    "ì†Œë¹„ì¬": ["ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"],
    "ë¹„ì² /ì² ê°•": ["í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"],
    "ì„ìœ í™”í•™": ["LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"],
    "ê±´ì„¤": ["í¬ìŠ¤ì½”ì´ì•¤ì”¨"],
    "íŠ¹ìˆ˜ì±„": ["ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"]
}

excel_company_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": [
        "í˜„ëŒ€í•´ìƒí™”ì¬ë³´í—˜(í›„)", "ë†í˜‘ìƒëª…ë³´í—˜(í›„)", "ë©”ë¦¬ì¸ í™”ì¬í•´ìƒë³´í—˜(í›„)", "êµë³´ìƒëª…(í›„)",
        "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„(í›„)", "í¥êµ­ìƒëª…ë³´í—˜(í›„)", "ë™ì–‘ìƒëª…ë³´í—˜(í›„)", "ë¯¸ë˜ì—ì…‹ìƒëª…(í›„)"
    ],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": [
        "ì‹ í•œì§€ì£¼", "í•˜ë‚˜ê¸ˆìœµì§€ì£¼", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµì§€ì£¼", "ìš°ë¦¬ê¸ˆìœµì§€ì£¼"
    ],
    "5ëŒ€ì‹œì¤‘ì€í–‰": [
        "ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"
    ],
    "ì¹´ë“œì‚¬": [
        "ì¼€ì´ë¹„ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"
    ],
    "ìºí”¼íƒˆ": [
        "í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"
    ],
    "ì§€ì£¼ì‚¬": [
        "SKì´ë…¸ë² ì´ì…˜", "ì§€ì—ìŠ¤ì—ë„ˆì§€", "SK", "GS"
    ],
    "ì—ë„ˆì§€": [
        "SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "ì—ìŠ¤ì¼€ì´ì—”ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"
    ],
    "ë°œì „": [
        "GSíŒŒì›Œ", "ì§€ì—ìŠ¤ì´í”¼ì—ìŠ¤", "ì‚¼ì²œë¦¬"
    ],
    "ìë™ì°¨": [
        "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´ì•¤í…Œí¬ë†€ë¡œì§€"
    ],
    "ì „ê¸°/ì „ì": [
        "SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "ì—˜ì—ìŠ¤ì¼ë ‰íŠ¸ë¦­"
    ],
    "ì†Œë¹„ì¬": [
        "ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"
    ],
    "ë¹„ì² /ì² ê°•": [
        "í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"
    ],
    "ì„ìœ í™”í•™": [
        "LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"
    ],
    "ê±´ì„¤": [
        "í¬ìŠ¤ì½”ì´ì•¤ì”¨"
    ],
    "íŠ¹ìˆ˜ì±„": [
        "ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"
    ]
}

# --- ê³µí†µ í•„í„° ì˜µì…˜(ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì—†ì´ ëª¨ë‘ ì ìš©) ---
common_filter_categories = {
    "ì‹ ìš©/ë“±ê¸‰": [
        "ì‹ ìš©ë“±ê¸‰", "ë“±ê¸‰ì „ë§", "í•˜ë½", "ê°•ë“±", "í•˜í–¥", "ìƒí–¥", "ë””í´íŠ¸", "ë¶€ì‹¤", "ë¶€ë„", "ë¯¸ì§€ê¸‰", "ìˆ˜ìš” ë¯¸ë‹¬", "ë¯¸ë§¤ê°", "ì œë„ ê°œí¸", "EOD"
    ],
    "ìˆ˜ìš”/ê³µê¸‰": [
        "ìˆ˜ìš”", "ê³µê¸‰", "ìˆ˜ê¸‰", "ë‘”í™”", "ìœ„ì¶•", "ì„±ì¥", "ê¸‰ë“±", "ê¸‰ë½", "ìƒìŠ¹", "í•˜ë½", "ë¶€ì§„", "ì‹¬í™”"
    ],
    "ì‹¤ì /ì¬ë¬´": [
        "ì‹¤ì ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ì ì", "ì†ì‹¤", "ë¹„ìš©", "ë¶€ì±„ë¹„ìœ¨", "ì´ìë³´ìƒë°°ìœ¨"
    ],
    "ìê¸ˆ/ì¡°ë‹¬": [
        "ì°¨ì…", "ì¡°ë‹¬", "ì„¤ë¹„íˆ¬ì", "íšŒì‚¬ì±„", "ë°œí–‰", "ì¸ìˆ˜", "ë§¤ê°"
    ],
    "êµ¬ì¡°/ì¡°ì •": [
        "M&A", "í•©ë³‘", "ê³„ì—´ ë¶„ë¦¬", "êµ¬ì¡°ì¡°ì •", "ë‹¤ê°í™”", "êµ¬ì¡° ì¬í¸"
    ],
    "ê±°ì‹œ/ì •ì±…": [
        "ê¸ˆë¦¬", "í™˜ìœ¨", "ê´€ì„¸", "ë¬´ì—­ì œì¬", "ë³´ì¡°ê¸ˆ", "ì„¸ì•¡ ê³µì œ", "ê²½ìŸ"
    ],
    "ì§€ë°°êµ¬ì¡°/ë²•": [
        "íš¡ë ¹", "ë°°ì„", "ê³µì •ê±°ë˜", "ì˜¤ë„ˆë¦¬ìŠ¤í¬", "ëŒ€ì£¼ì£¼", "ì§€ë°°êµ¬ì¡°"
    ]
}
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ ---
industry_filter_categories = {
    "ì€í–‰ ë° ê¸ˆìœµì§€ì£¼": [
        "ê²½ì˜ì‹¤íƒœí‰ê°€", "BIS", "CET1", "ìë³¸ë¹„ìœ¨", "ìƒê°í˜• ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œ", "ìë³¸í™•ì¶©", "ìë³¸ì—¬ë ¥", "ìë³¸ì ì •ì„±", "LCR",
        "ì¡°ë‹¬ê¸ˆë¦¬", "NIM", "ìˆœì´ìë§ˆì§„", "ê³ ì •ì´í•˜ì—¬ì‹ ë¹„ìœ¨", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ì—°ì²´ìœ¨", "ê°€ê³„ëŒ€ì¶œ", "ì·¨ì•½ì°¨ì£¼"
    ],
    "ë³´í—˜ì‚¬": [
        "ë³´ì¥ì„±ë³´í—˜", "ì €ì¶•ì„±ë³´í—˜", "ë³€ì•¡ë³´í—˜", "í‡´ì§ì—°ê¸ˆ", "ì¼ë°˜ë³´í—˜", "ìë™ì°¨ë³´í—˜", "ALM", "ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨", "K-ICS",
        "ë³´í—˜ìˆ˜ìµì„±", "ë³´í—˜ì†ìµ", "ìˆ˜ì…ë³´í—˜ë£Œ", "CSM", "ìƒê°", "íˆ¬ìì†ìµ", "ìš´ìš©ì„±ê³¼", "IFRS4", "IFRS17", "ë³´í—˜ë¶€ì±„",
        "ì¥ê¸°ì„ ë„ê¸ˆë¦¬", "ìµœì¢…ê´€ì°°ë§Œê¸°", "ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„", "ì‹ ì¢…ìë³¸ì¦ê¶Œ", "í›„ìˆœìœ„ì±„", "ìœ„í—˜ìì‚°ë¹„ì¤‘", "ê°€ì¤‘ë¶€ì‹¤ìì‚°ë¹„ìœ¨"
    ],
    "ì¹´ë“œì‚¬": [
        "ë¯¼ê°„ì†Œë¹„ì§€í‘œ", "ëŒ€ì†ì¤€ë¹„ê¸ˆ", "ê°€ê³„ë¶€ì±„", "ì—°ì²´ìœ¨", "ê°€ë§¹ì ì¹´ë“œìˆ˜ìˆ˜ë£Œ", "ëŒ€ì¶œì„±ìì‚°", "ì‹ ìš©íŒë§¤ìì‚°", "ê³ ì •ì´í•˜ì—¬ì‹ ", "ë ˆë²„ë¦¬ì§€ë°°ìœ¨",
        "ê±´ì „ì„±", "ì¼€ì´ë±…í¬", "ì´íƒˆ"
    ],
    "ìºí”¼íƒˆ": [
        "ì¶©ë‹¹ê¸ˆì»¤ë²„ë¦¬ì§€ë¹„ìœ¨", "ê³ ì •ì´í•˜ì—¬ì‹ ", "PFêµ¬ì¡°ì¡°ì •", "ë¦¬ìŠ¤ìì‚°", "ì†ì‹¤í¡ìˆ˜ëŠ¥ë ¥", "ë¶€ë™ì‚°PFì—°ì²´ì±„ê¶Œ", "ìì‚°í¬íŠ¸í´ë¦¬ì˜¤", "ê±´ì „ì„±",
        "ì¡°ì •ì´ìì‚°ìˆ˜ìµë¥ ", "êµ°ì¸ê³µì œíšŒ"
    ],
    "ì§€ì£¼ì‚¬": [
        "SKì§€ì˜¤ì„¼íŠ¸ë¦­", "SKì—ë„ˆì§€", "SKì—”ë¬´ë¸Œ", "SKì¸ì²œì„ìœ í™”í•™", "GSì¹¼í…ìŠ¤", "GSíŒŒì›Œ", "SKì´ë…¸ë² ì´ì…˜", "SKí…”ë ˆì½¤", "SKì˜¨",
        "GSì—ë„ˆì§€", "GSë¦¬í…Œì¼", "GS E&C", "2ì°¨ì „ì§€", "ì„ìœ í™”í•™", "ìœ¤í™œìœ ", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ì •ìœ ", "ì´ë™í†µì‹ "
    ],
    "ì—ë„ˆì§€": [
        "ì •ìœ ", "ìœ ê°€", "ì •ì œë§ˆì§„", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "ì¬ê³  ì†ì‹¤", "ì¤‘êµ­ ìˆ˜ìš”", "IMO ê·œì œ", "ì €ìœ í™© ì—°ë£Œ", "LNG",
        "í„°ë¯¸ë„", "ìœ¤í™œìœ "
    ],
    "ë°œì „": [
        "LNG", "ì²œì—°ê°€ìŠ¤", "ìœ ê°€", "SMP", "REC", "ê³„í†µì‹œì¥", "íƒ„ì†Œì„¸", "íƒ„ì†Œë°°ì¶œê¶Œ", "ì „ë ¥ì‹œì¥ ê°œí¸", "ì „ë ¥ ììœ¨í™”",
        "ê°€ë™ë¥ ", "ë„ì‹œê°€ìŠ¤"
    ],
    "ìë™ì°¨": [
        "AMPC ë³´ì¡°ê¸ˆ", "IRA ì¸ì„¼í‹°ë¸Œ", "ì¤‘êµ­ ë°°í„°ë¦¬", "EV ìˆ˜ìš”", "ì „ê¸°ì°¨", "ESSìˆ˜ìš”", "ë¦¬íŠ¬", "íƒ€ì´ì–´"
    ],
    "ì „ê¸°ì „ì": [
        "CHIPS ë³´ì¡°ê¸ˆ", "ì¤‘êµ­", "DRAM", "HBM", "ê´‘í• ì†”ë£¨ì…˜", "ì•„ì´í°", "HVAC", "HVTR"
    ],
    "ì² ê°•": [
        "ì² ê´‘ì„", "í›„íŒ", "ê°•íŒ", "ì² ê·¼", "ìŠ¤í”„ë ˆë“œ", "ì² ê°•", "ê°€ë™ë¥ ", "ì œì² ì†Œ", "ì…§ë‹¤ìš´", "ì¤‘êµ­ì‚° ì €ê°€",
        "ì¤‘êµ­ ìˆ˜ì¶œ ê°ì†Œ", "ê±´ì„¤ê²½ê¸°", "ì¡°ì„  ìˆ˜ìš”", "íŒŒì—…"
    ],
    "ë¹„ì² ": [
        "ì—°", "ì•„ì—°", "ë‹ˆì¼ˆ", "ì•ˆí‹°ëª¨ë‹ˆ", "ê²½ì˜ê¶Œ ë¶„ìŸ", "MBK", "ì˜í’"
    ],
    "ì†Œë§¤": [
        "ë‚´ìˆ˜ë¶€ì§„", "ì‹œì¥ì§€ë°°ë ¥", "SKí…”ë ˆì½¤", "SKë§¤ì§", "CLS", "HMR", "ë¼ì´ì‹ ", "ì•„ë¯¸ë…¸ì‚°", "ìŠˆì™„ìŠ¤ì»´í¼ë‹ˆ",
        "ì˜ë¥˜", "ì‹ ì„¸ê³„", "ëŒ€í˜•ë§ˆíŠ¸ ì˜ë¬´íœ´ì—…", "Gë§ˆì¼“", "Wì»¨ì…‰", "ìŠ¤íƒ€í•„ë“œ"
    ],
    "ì„ìœ í™”í•™": [
        "ì„ìœ í™”í•™", "ì„í™”", "ìœ ê°€", "ì¦ì„¤", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "PX", "ë²¤ì  ", "ì¤‘êµ­ ì¦ì„¤", "ì¤‘ë™ COTC",
        "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ë¦¬íŠ¬", "IRA", "AMPC"
    ],
    "ê±´ì„¤": [
        "ì² ê·¼ ê°€ê²©", "ì‹œë©˜íŠ¸ ê°€ê²©", "ê³µì‚¬ë¹„", "SOC ì˜ˆì‚°", "ë„ì‹œì •ë¹„ ì§€ì›", "ìš°ë°œì±„ë¬´", "ìˆ˜ì£¼", "ì£¼ê°„ì‚¬", "ì‚¬ê³ ",
        "ì‹œê³µëŠ¥ë ¥ìˆœìœ„", "ë¯¸ë¶„ì–‘", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ"
    ],
    "íŠ¹ìˆ˜ì±„": [
        "ìë³¸í™•ì¶©", "HUG", "ì „ì„¸ì‚¬ê¸°", "ë³´ì¦ì‚¬ê³ ", "ë³´ì¦ë£Œìœ¨", "íšŒìˆ˜ìœ¨", "ë³´ì¦ì”ì•¡", "ëŒ€ìœ„ë³€ì œì•¡",
        "ì¤‘ì†Œê¸°ì—…ëŒ€ì¶œ", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ë¶ˆë²•", "êµ¬ì†"
    ]
}

# --- ì¹´í…Œê³ ë¦¬-ì‚°ì—… ëŒ€ë¶„ë¥˜ ë§¤í•‘ í•¨ìˆ˜ ---
def get_industry_majors_from_favorites(selected_categories):
    favorite_to_industry_major = {
        "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì€í–‰ ë° ê¸ˆìœµì§€ì£¼"],
        "5ëŒ€ì‹œì¤‘ì€í–‰": ["ì€í–‰ ë° ê¸ˆìœµì§€ì£¼"],
        "ë³´í—˜ì‚¬": ["ë³´í—˜ì‚¬"],
        "ì¹´ë“œì‚¬": ["ì¹´ë“œì‚¬"],
        "ìºí”¼íƒˆ": ["ìºí”¼íƒˆ"],
        "ì§€ì£¼ì‚¬": ["ì§€ì£¼ì‚¬"],
        "ì—ë„ˆì§€": ["ì—ë„ˆì§€"],
        "ë°œì „": ["ë°œì „"],
        "ìë™ì°¨": ["ìë™ì°¨"],
        "ì„ìœ í™”í•™": ["ì„ìœ í™”í•™"],
        "ì „ê¸°/ì „ì": ["ì „ê¸°ì „ì"],
        "ë¹„ì² /ì² ê°•": ["ì² ê°•", "ë¹„ì² "],
        "ì†Œë¹„ì¬": ["ì†Œë§¤"],
        "ê±´ì„¤": ["ê±´ì„¤"],
        "íŠ¹ìˆ˜ì±„": ["íŠ¹ìˆ˜ì±„"],
    }
    majors = set()
    for cat in selected_categories:
        for major in favorite_to_industry_major.get(cat, []):
            majors.add(major)
    return list(majors)

# --- UI ì‹œì‘ ---
st.set_page_config(layout="wide")
col_title, col_option1, col_option2 = st.columns([0.6, 0.2, 0.2])
with col_title:
    st.markdown(
        "<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>"
        "<a href='https://credit-issue-monitoring.onrender.com/' target='_blank' style='text-decoration:none; color:#1a1a1a;'>"
        "ğŸ“Š Credit Issue Monitoring</a></h1>",
        unsafe_allow_html=True
    )
with col_option1:
    show_sentiment_badge = st.checkbox("ê¸°ì‚¬ëª©ë¡ì— ê°ì„±ë¶„ì„ ë°°ì§€ í‘œì‹œ", value=False, key="show_sentiment_badge")
with col_option2:
    enable_summary = st.checkbox("ìš”ì•½ ê¸°ëŠ¥ ì ìš©", value=False, key="enable_summary")

col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input(label="", value="", key="keyword_input", label_visibility="collapsed")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

st.markdown("**â­ ì‚°ì—…êµ° ì„ íƒ**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        "",
        list(favorite_categories.keys()), key="cat_multi", label_visibility="collapsed"
        )
if selected_categories:
    auto_selected_majors = get_industry_majors_from_favorites(selected_categories)
    st.session_state.cat_major_autoset = auto_selected_majors.copy()
else:
    st.session_state.cat_major_autoset = []
with col_cat_btn:
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", key="cat_search_btn", help="ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰", use_container_width=True)
for cat in selected_categories:
    st.session_state.favorite_keywords.update(favorite_categories[cat])

# ë‚ ì§œ ì…ë ¥ (ê¸°ë³¸ ì„¸íŒ…: ì¢…ë£Œì¼=ì˜¤ëŠ˜, ì‹œì‘ì¼=ì˜¤ëŠ˜-7ì¼)
today = datetime.today().date()
if "end_date" not in st.session_state:
    st.session_state["end_date"] = today
if "start_date" not in st.session_state:
    st.session_state["start_date"] = today - timedelta(days=7)
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼", value=st.session_state["start_date"], key="start_date_input")
    st.session_state["start_date"] = start_date
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼", value=st.session_state["end_date"], key="end_date_input")
    st.session_state["end_date"] = end_date

with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©ë¨)"):
    for major, subs in common_filter_categories.items():
        st.markdown(f"**{major}**: {', '.join(subs)}")

with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ëŒ€ë¶„ë¥˜ë³„ ì†Œë¶„ë¥˜ í•„í„°ë§)"):
    use_industry_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=True, key="use_industry_filter")
    
    # ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™”
    if "industry_major_sub_map" not in st.session_state:
        st.session_state.industry_major_sub_map = {}

    # UI: ì„ íƒëœ ì‚°ì—…êµ°ì—ì„œ ìë™ ë§¤í•‘ëœ ëŒ€ë¶„ë¥˜ ì¶”ì¶œ
    selected_major_map = get_industry_majors_from_favorites(selected_categories)

    updated_map = {}
    for major in selected_major_map:
        options = industry_filter_categories.get(major, [])
        default_selected = options if major not in st.session_state.industry_major_sub_map else st.session_state.industry_major_sub_map[major]
        selected_sub = st.multiselect(
            f"{major} ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ",
            options,
            default=default_selected,
            key=f"subfilter_{major}"
        )
        updated_map[major] = selected_sub

    st.session_state.industry_major_sub_map = updated_map
    
# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ê¸°ëŠ¥ ì²´í¬ë°•ìŠ¤ í¬í•¨ëœ í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    require_exact_keyword_in_title_or_content = st.checkbox("í‚¤ì›Œë“œê°€ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", value=True, key="require_exact_keyword_in_title_or_content")
    # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì²´í¬ë°•ìŠ¤ ì¶”ê°€ (ê¸°ë³¸ í•´ì œ)
    remove_duplicate_articles = st.checkbox("ì¤‘ë³µ ê¸°ì‚¬ ì œê±°", value=False, key="remove_duplicate_articles", help="í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.")

def extract_article_text(url):
    try:
        article = newspaper.Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def summarize_and_sentiment_with_openai(text, do_summary=True):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            ("ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°ì„±ë¶„ì„(ê¸ì •/ë¶€ì •ë§Œ)í•˜ê³ " +
             ("\n- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½" if do_summary else "") +
             "\n- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. ì¤‘ë¦½ì€ ì ˆëŒ€ ë‹µí•˜ì§€ ë§ˆ. íŒŒì‚°, ìê¸ˆë‚œ ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n\n"
             "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n" +
             ("[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n" if do_summary else "") +
             "[ê°ì„±]: (ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
             "[ê¸°ì‚¬ ë³¸ë¬¸]\n" + text)
        )
    else:
        prompt = (
            ("Analyze the following news article for sentiment (positive/negative only)." +
             ("\n- [One-line Summary]: Summarize the entire article in one sentence." if do_summary else "") +
             "\n- [Sentiment]: Classify the overall sentiment as either positive or negative ONLY. Never answer 'neutral'. If the article is about bankruptcy, crisis, etc., answer 'negative'.\n\n"
             "Respond in this format:\n" +
             ("[One-line Summary]: (your one-line summary)\n" if do_summary else "") +
             "[Sentiment]: (positive/negative only)\n\n"
             "[ARTICLE]\n" + text)
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if (do_summary and m1) else ""
    summary = ""
    sentiment = m3.group(1).strip() if m3 else ""
    if sentiment.lower() in ['neutral', 'ì¤‘ë¦½', '']:
        sentiment = 'ë¶€ì •' if lang == "ko" else 'negative'
    if lang == "en":
        sentiment = 'ê¸ì •' if sentiment.lower() == 'positive' else 'ë¶€ì •'
    return one_line, summary, sentiment, text

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID
    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()

            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(re.sub("<.*?>", "", title), EXCLUDE_TITLE_KEYWORDS):
                continue

            # ì–¸ë¡ ì‚¬ëª… ê°€ì ¸ì˜¤ê¸° + ê¸°ë³¸ê°’ ì²˜ë¦¬ + ë„ë©”ì¸ ê¸°ë°˜ ì¶”ì¶œ ë³´ì™„
            source = item.get("source")
            if not source or source.strip() == "":
                source = infer_source_from_url(item.get("originallink", ""))
                if not source:
                    source = "Naver"

            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": source
            })

        if len(items) < 100:
            break
    return articles[:limit]

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def summarize_article_from_url(article_url, title, do_summary=True):
    cache_key_base = re.sub(r"\W+", "", article_url)[-16:]
    summary_key = f"summary_{cache_key_base}"

    # ì´ë¯¸ ìš”ì•½ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if summary_key in st.session_state:
        return st.session_state[summary_key]

    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            result = (full_text, None, None, None)
        else:
            one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text, do_summary=do_summary)
            result = (one_line, summary, sentiment, full_text)
    except Exception as e:
        result = (f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None)

    # ìºì‹œì— ì €ì¥
    st.session_state[summary_key] = result
    return result

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    title = article.get("title", "")
    content = ""
    cache_key = article.get("link", "")
    summary_cache_key = None
    for key in st.session_state.keys():
        if key.startswith("summary_") and cache_key in key:
            summary_cache_key = key
            break
    if summary_cache_key and isinstance(st.session_state[summary_cache_key], tuple):
        _, _, _, content = st.session_state[summary_cache_key]
    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° í•¨ìˆ˜ ---
def is_similar(title1, title2, threshold=0.6):
    ratio = difflib.SequenceMatcher(None, title1, title2).ratio()
    return ratio >= threshold

def remove_duplicates(articles):
    unique_articles = []
    titles = []
    for article in articles:
        title = article.get("title", "")
        if all(not is_similar(title, existing_title) for existing_title in titles):
            unique_articles.append(article)
            titles.append(title)
    return unique_articles

search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(
                keyword_list,
                st.session_state["start_date"],
                st.session_state["end_date"],
                require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
            )
    st.session_state.search_triggered = False

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords(
            sorted(keywords),
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )

def article_passes_all_filters(article):
    # ì œì™¸ í‚¤ì›Œë“œ
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False

    # ë‚ ì§œ í•„í„°
    try:
        pub_date = datetime.strptime(article['date'], '%Y-%m-%d').date()
        if pub_date < st.session_state.get("start_date") or pub_date > st.session_state.get("end_date"):
            return False
    except:
        return False

    # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ì²´í¬
    all_keywords = []
    if "keyword_input" in st.session_state:
        all_keywords.extend([k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()])
    if "cat_multi" in st.session_state:
        for cat in st.session_state["cat_multi"]:
            all_keywords.extend(favorite_categories.get(cat, []))
    if not article_contains_exact_keyword(article, all_keywords):
        return False

    # --- ê³µí†µ í•„í„° í‚¤ì›Œë“œ ì ìš© ---
    if not or_keyword_filter(article, ALL_COMMON_FILTER_KEYWORDS):
        return False

    # --- ì‚°ì—…ë³„ í•„í„° (ëŒ€ë¶„ë¥˜ë³„ ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ ì ìš©) ---
    if st.session_state.get("use_industry_filter", False):
        keyword = article.get("í‚¤ì›Œë“œ")
        matched_major = None
        for cat, companies in favorite_categories.items():
            if keyword in companies:
                majors = get_industry_majors_from_favorites([cat])
                if majors:
                    matched_major = majors[0]  # ë³µìˆ˜ ë§¤í•‘ë˜ëŠ” ê²½ìš° ì²« ë²ˆì§¸ë§Œ í™œìš©
                    break

        if matched_major:
            sub_keyword_filter = st.session_state.industry_major_sub_map.get(matched_major, [])
            if sub_keyword_filter:
                if not or_keyword_filter(article, sub_keyword_filter):
                    return False

    return True

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories):
    company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        company_order.extend(favorite_categories.get(cat, []))
    excel_company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        excel_company_order.extend(excel_company_categories.get(cat, []))

    df_articles = pd.DataFrame(summary_data)

    # âœ… ë³´í˜¸ ë¡œì§: 'í‚¤ì›Œë“œ' ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ì—‘ì…€ ë°˜í™˜
    if "í‚¤ì›Œë“œ" not in df_articles.columns:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            pd.DataFrame(columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê¸ì • ë‰´ìŠ¤", "ë¶€ì • ë‰´ìŠ¤"]).to_excel(writer, index=False)
        output.seek(0)
        return output

    result_rows = []
    for idx, company in enumerate(company_order):
        excel_company_name = excel_company_order[idx] if idx < len(excel_company_order) else ""
        comp_articles = df_articles[df_articles["í‚¤ì›Œë“œ"] == company]
        pos_news = comp_articles[comp_articles["ê°ì„±"] == "ê¸ì •"].sort_values(by="ë‚ ì§œ", ascending=False)
        neg_news = comp_articles[comp_articles["ê°ì„±"] == "ë¶€ì •"].sort_values(by="ë‚ ì§œ", ascending=False)

        if not pos_news.empty:
            pos_date = pos_news.iloc[0]["ë‚ ì§œ"]
            pos_title = pos_news.iloc[0]["ê¸°ì‚¬ì œëª©"]
            pos_link = pos_news.iloc[0]["ë§í¬"]
            pos_display = f'({pos_date}) {pos_title}'
            pos_hyperlink = f'=HYPERLINK("{pos_link}", "{pos_display}")'
        else:
            pos_hyperlink = ""

        if not neg_news.empty:
            neg_date = neg_news.iloc[0]["ë‚ ì§œ"]
            neg_title = neg_news.iloc[0]["ê¸°ì‚¬ì œëª©"]
            neg_link = neg_news.iloc[0]["ë§í¬"]
            neg_display = f'({neg_date}) {neg_title}'
            neg_hyperlink = f'=HYPERLINK("{neg_link}", "{neg_display}")'
        else:
            neg_hyperlink = ""

        result_rows.append({
            "ê¸°ì—…ëª…": company,
            "í‘œê¸°ëª…": excel_company_name,
            "ê¸ì • ë‰´ìŠ¤": pos_hyperlink,
            "ë¶€ì • ë‰´ìŠ¤": neg_hyperlink
        })

    df_result = pd.DataFrame(result_rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_result.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
    output.seek(0)
    return output

def build_important_excel_same_format(important_articles, favorite_categories, excel_company_categories):
    """
    ì¤‘ìš”ê¸°ì‚¬ ìë™ ì¶”ì¶œ ê²°ê³¼ë¥¼ ê¸°ì¡´ 'ë§ì¶¤ ì—‘ì…€ ì–‘ì‹'ê³¼ ë™ì¼í•œ í¬ë§·ìœ¼ë¡œ ì €ì¥
    """
    company_order = []
    excel_company_order = []

    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        company_order.extend(favorite_categories.get(cat, []))
        excel_company_order.extend(excel_company_categories.get(cat, []))

    # ê¸°ì—…ë³„ ê¸°ì‚¬ ì •ë¦¬
    rows = []
    for i, comp in enumerate(company_order):
        display_name = excel_company_order[i] if i < len(excel_company_order) else ""
        pos_article = ""
        neg_article = ""

        # ì´ ê¸°ì—…ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë“¤ í•„í„°ë§
        articles = [a for a in important_articles if a["íšŒì‚¬ëª…"] == comp]

        for article in articles:
            link = article["ë§í¬"]
            title = article["ì œëª©"]
            date = article["ë‚ ì§œ"]
            display_text = f"({date}) {title}"
            hyperlink = f'=HYPERLINK("{link}", "{display_text}")'

            if article["ê°ì„±"] == "ê¸ì •":
                pos_article = hyperlink
            elif article["ê°ì„±"] == "ë¶€ì •":
                neg_article = hyperlink

        rows.append({
            "ê¸°ì—…ëª…": comp,
            "í‘œê¸°ëª…": display_name,
            "ê¸ì • ë‰´ìŠ¤": pos_article,
            "ë¶€ì • ë‰´ìŠ¤": neg_article
        })

    df = pd.DataFrame(rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="ì¤‘ìš”ë‰´ìŠ¤_ì–‘ì‹")
    output.seek(0)
    return output

def generate_important_article_list(search_results, common_keywords, industry_keywords, favorites):
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    client = OpenAI(api_key=OPENAI_API_KEY)
    result = []

    for category, companies in favorites.items():
        for comp in companies:
            articles = search_results.get(comp, [])
            filtered_keywords = list(set(common_keywords + industry_keywords))
            target_articles = [a for a in articles if any(kw in a["title"] for kw in filtered_keywords)]

            if not target_articles:
                continue

            prompt_list = "\n".join([f"{i+1}. {a['title']} - {a['link']}" for i, a in enumerate(target_articles)])
            prompt = (
                f"[í•„í„° í‚¤ì›Œë“œ]\n{', '.join(filtered_keywords)}\n\n"
                f"[ê¸°ì‚¬ ëª©ë¡]\n{prompt_list}\n\n"
                "ê° ê¸°ì‚¬ì— ëŒ€í•´ ê°ì„±(ê¸ì •/ë¶€ì •)ì„ íŒë‹¨í•˜ê³ ,\n"
                "ì œëª©ì— í•„í„° í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ ê¸°ì¤€ìœ¼ë¡œ,\n"
                "- ê¸ì •ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 1ê±´\n"
                "- ë¶€ì •ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 1ê±´\n"
                "ì„ ê³¨ë¼ì£¼ì„¸ìš”. ì—†ìœ¼ë©´ ë¹ˆì¹¸ìœ¼ë¡œ:\n\n"
                "[ê¸ì •]: (ë‰´ìŠ¤ ì œëª©)\n[ë¶€ì •]: (ë‰´ìŠ¤ ì œëª©)"
            )

            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=800,
                    temperature=0.3
                )
                answer = response.choices[0].message.content.strip()
                import re
                pos_title = re.search(r"\[ê¸ì •\]:\s*(.+)", answer)
                neg_title = re.search(r"\[ë¶€ì •\]:\s*(.+)", answer)
                pos_title = pos_title.group(1).strip() if pos_title else ""
                neg_title = neg_title.group(1).strip() if neg_title else ""

                for a in target_articles:
                    if pos_title and pos_title in a["title"]:
                        result.append({
                            "íšŒì‚¬ëª…": comp,
                            "ê°ì„±": "ê¸ì •",
                            "ì œëª©": a["title"],
                            "ë§í¬": a["link"],
                            "ë‚ ì§œ": a["date"],
                            "ì¶œì²˜": a["source"]
                        })
                    if neg_title and neg_title in a["title"]:
                        result.append({
                            "íšŒì‚¬ëª…": comp,
                            "ê°ì„±": "ë¶€ì •",
                            "ì œëª©": a["title"],
                            "ë§í¬": a["link"],
                            "ë‚ ì§œ": a["date"],
                            "ì¶œì²˜": a["source"]
                        })
            except Exception:
                continue
    return result

def extract_keyword_from_link(search_results, article_link):
    """
    ì£¼ì–´ì§„ ê¸°ì‚¬ ë§í¬ì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œë¥¼ search_resultsì—ì„œ ì¶”ë¡ í•œë‹¤.
    ë°˜í™˜ê°’: í‚¤ì›Œë“œ ë¬¸ìì—´ (ëª» ì°¾ìœ¼ë©´ "ì•Œìˆ˜ì—†ìŒ")
    """
    for keyword, articles in search_results.items():
        for article in articles:
            if article["link"] == article_link:
                return keyword
    return "ì•Œìˆ˜ì—†ìŒ"

def render_important_article_review_and_download():
    # â–¼ ì´ ì¤„ì„ ì¶”ê°€: ì»¨í…Œì´ë„ˆë¡œ UIë¥¼ í…Œë‘ë¦¬ ì•ˆì— ë„£ê¸°
    with st.container(border=True):
        st.markdown("### â­ ì¤‘ìš” ê¸°ì‚¬ ë¦¬ë·° ë° í¸ì§‘")
        if st.button("ğŸš€ OpenAI ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ìë™ ì„ ì •"):
            with st.spinner("OpenAIë¡œ ì¤‘ìš” ë‰´ìŠ¤ ì„ ì • ì¤‘..."):
                important_articles = generate_important_article_list(
                    search_results=st.session_state.search_results,
                    common_keywords=ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords=st.session_state.get("industry_sub", []),
                    favorites=favorite_categories
                )
                st.session_state.important_articles_preview = important_articles
                st.session_state.important_selected_index = []

        if not st.session_state.get("important_articles_preview"):
            st.info("ì•„ì§ ì¤‘ìš” ê¸°ì‚¬ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìë™ ìƒì„±í•˜ì‹­ì‹œì˜¤.")
            return

        st.markdown("ğŸ¯ **ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡** (êµì²´ ë˜ëŠ” ì‚­ì œí•  í•­ëª©ì„ ì²´í¬í•˜ì„¸ìš”)")
        new_selection = []
        for idx, article in enumerate(st.session_state["important_articles_preview"]):
            checked = st.checkbox(
                f"{article['íšŒì‚¬ëª…']} | {article['ê°ì„±']} | {article['ì œëª©'][:40]}...",
                key=f"important_chk_{idx}",
                value=(idx in st.session_state.important_selected_index)
            )
            if checked:
                new_selection.append(idx)
        st.session_state.important_selected_index = new_selection

        col_action1, col_action2 = st.columns([0.5, 0.5])
        with col_action1:
            if st.button("ğŸ—‘ ì„ íƒí•œ ê¸°ì‚¬ ì‚­ì œ"):
                for idx in sorted(st.session_state.important_selected_index, reverse=True):
                    if 0 <= idx < len(st.session_state["important_articles_preview"]):
                        st.session_state["important_articles_preview"].pop(idx)
                st.session_state.important_selected_index = []
                st.rerun()
        with col_action2:
            if st.button("ğŸ” ì„ íƒí•œ ê¸°ì‚¬ êµì²´ (ì™¼ìª½ì—ì„œ 1ê°œ ì„ íƒ í•„ìš”)"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                right_selected_indexes = st.session_state.important_selected_index
                if len(left_selected_keys) != 1 or len(right_selected_indexes) != 1:
                    st.warning("ì™¼ìª½ì—ì„œ ê¸°ì‚¬ 1ê°œ, ì˜¤ë¥¸ìª½ì—ì„œ ê¸°ì‚¬ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                else:
                    from_key = left_selected_keys[0]
                    selected_link = None
                    m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                    if m:
                        key_tail = m.group(1)
                        for kw, art_list in st.session_state.search_results.items():
                            for art in art_list:
                                uid = re.sub(r'\W+', '', art['link'])[-16:]
                                if uid == key_tail:
                                    selected_link = art['link']
                                    selected_article = art
                                    break
                            if selected_link:
                                break
                    else:
                        st.warning("ì™¼ìª½ ê¸°ì‚¬ ì„ íƒ í‚¤ í•´ì„ ì˜¤ë¥˜")
                        return
                    if not selected_link:
                        st.warning("ì™¼ìª½ì—ì„œ ì„ íƒí•œ ê¸°ì‚¬ì— ëŒ€ì‘í•˜ëŠ” ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return

                    keyword = extract_keyword_from_link(st.session_state.search_results, selected_article["link"])
                    sentiment = None
                    cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                    for k in st.session_state.keys():
                        if k.startswith("summary_") and cleaned_id in k:
                            _, _, sentiment, _ = st.session_state[k]
                            break
                    if not sentiment:
                        _, _, sentiment, _ = summarize_article_from_url(
                            selected_article["link"], selected_article["title"]
                        )
                        summary_key = f"summary_{keyword}_{0}_{cleaned_id}"
                        st.session_state[summary_key] = ("", "", sentiment, "")

                    new_article = {
                        "íšŒì‚¬ëª…": keyword,
                        "ê°ì„±": sentiment,
                        "ì œëª©": selected_article["title"],
                        "ë§í¬": selected_article["link"],
                        "ë‚ ì§œ": selected_article["date"],
                        "ì¶œì²˜": selected_article["source"]
                    }
                    target_idx = right_selected_indexes[0]
                    st.session_state["important_articles_preview"][target_idx] = new_article
                    st.session_state.article_checked_left[from_key] = False
                    st.session_state.article_checked[from_key] = False
                    st.session_state.important_selected_index = []
                    st.success("ê¸°ì‚¬ êµì²´ ì™„ë£Œ: " + new_article["ì œëª©"])
                    st.rerun()

        st.markdown("---")
        st.markdown("ğŸ“¥ **ë¦¬ë·°í•œ ì¤‘ìš” ê¸°ì‚¬ë“¤ì„ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.**")
        output_excel = build_important_excel_same_format(
            st.session_state["important_articles_preview"],
            favorite_categories,
            excel_company_categories
        )
        st.download_button(
            label="ğŸ“¥ ì¤‘ìš” ê¸°ì‚¬ ìµœì¢… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ë§ì¶¤ ì–‘ì‹)",
            data=output_excel.getvalue(),
            file_name="ì¤‘ìš”ë‰´ìŠ¤_ìµœì¢…ì„ ì •_ì–‘ì‹.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True, enable_summary=True):
    SENTIMENT_CLASS = {
        "ê¸ì •": "sentiment-positive",
        "ë¶€ì •": "sentiment-negative"
    }

    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼")
        for keyword, articles in results.items():
            with st.container(border=True):
                st.markdown(f"**[{keyword}] ({len(articles)}ê±´)**")

                for idx, article in enumerate(articles):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    cache_key = f"summary_{key}"

                    # ì²´í¬ë°•ìŠ¤ì™€ ì œëª© ë Œë”ë§
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                    with cols[1]:
                        sentiment = ""
                        if show_sentiment_badge and cache_key in st.session_state:
                            _, _, sentiment, _ = st.session_state[cache_key]
                        badge_html = f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>({sentiment})</span>" if sentiment else ""
                        st.markdown(f"[{article['title']}]({article['link']}) {badge_html} {article['date']} | {article['source']}", unsafe_allow_html=True)

                    st.session_state.article_checked_left[key] = checked
                    if checked:
                        st.session_state.article_checked[key] = True

    # ì„ íƒ ê¸°ì‚¬ ìš”ì•½ ë° ë‹¤ìš´ë¡œë“œ
    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):
            selected_articles = []

            for keyword, articles in results.items():
                for idx, article in enumerate(articles):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    cache_key = f"summary_{key}"

                    if st.session_state.article_checked.get(key, False):
                        if cache_key in st.session_state:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        else:
                            one_line, summary, sentiment, full_text = summarize_article_from_url(
                                article['link'], article['title'], do_summary=enable_summary
                            )
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)

                        selected_articles.append({
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": safe_title(article['title']),
                            "ìš”ì•½": one_line,
                            "ìš”ì•½ë³¸": summary,
                            "ê°ì„±": sentiment,
                            "ë§í¬": article['link'],
                            "ë‚ ì§œ": article['date'],
                            "ì¶œì²˜": article['source']
                        })

                        st.markdown(
                            f"#### [{article['title']}]({article['link']}) "
                            f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>({sentiment})</span>",
                            unsafe_allow_html=True
                        )
                        st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {article['date']} | {article['source']}")
                        if enable_summary:
                            st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {one_line}")
                        st.markdown(f"- **ê°ì„±ë¶„ì„:** `{sentiment}`")
                        st.markdown("---")

            st.session_state.selected_articles = selected_articles
            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(selected_articles)}")

            col_dl1, col_dl2 = st.columns([0.5, 0.5])
            with col_dl1:
                st.download_button(
                    label="ğŸ“¥ ë§ì¶¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=get_excel_download_with_favorite_and_excel_company_col(
                        st.session_state.selected_articles,
                        favorite_categories,
                        excel_company_categories
                    ).getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë§ì¶¤í˜•.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

        # ì¤‘ìš” ê¸°ì‚¬ ë¦¬ë·° UI
        render_important_article_review_and_download()

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        
        # --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì²˜ë¦¬ ---
        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)
        
        if filtered_articles:
            filtered_results[keyword] = filtered_articles

    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )
