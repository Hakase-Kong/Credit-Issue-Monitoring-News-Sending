import os
import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
from datetime import datetime, timedelta
import telepot
from openai import OpenAI
import newspaper
import difflib
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_keywords_parallel(keyword_list, start_date, end_date, require_keyword_in_title=False):
    def fetch_and_store(k):
        return k, fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
    with ThreadPoolExecutor(max_workers=min(5, len(keyword_list))) as executor:
        futures = [executor.submit(fetch_and_store, k) for k in keyword_list]
        for future in as_completed(futures):
            k, articles = future.result()
            st.session_state.search_results[k] = articles
            if k not in st.session_state.show_limit:
                st.session_state.show_limit[k] = 5

# --- CSS Ïä§ÌÉÄÏùº ---
st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] { background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold; }
.sentiment-badge { display: inline-block; padding: 0.08em 0.6em; margin-left: 0.2em; border-radius: 0.8em; font-size: 0.85em; font-weight: bold; vertical-align: middle; }
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
</style>
""", unsafe_allow_html=True)

# --- Ï†úÏô∏ ÌÇ§ÏõåÎìú ---
EXCLUDE_TITLE_KEYWORDS = [
    "ÏïºÍµ¨", "Ï∂ïÍµ¨", "Î∞∞Íµ¨", "ÎÜçÍµ¨", "Í≥®ÌîÑ", "eÏä§Ìè¨Ï∏†", "Ïò¨Î¶ºÌîΩ", "ÏõîÎìúÏªµ", "KÎ¶¨Í∑∏", "ÌîÑÎ°úÏïºÍµ¨", "ÌîÑÎ°úÏ∂ïÍµ¨", "ÌîÑÎ°úÎ∞∞Íµ¨", "ÌîÑÎ°úÎÜçÍµ¨", "Ïú†ÎèÑ",
    "Î∂ÄÍ≥†", "Î∂ÄÏùå", "Ïù∏ÏÇ¨", "ÏäπÏßÑ", "ÏûÑÎ™Ö", "Î∞úÎ†π", "Ïù∏ÏÇ¨Î∞úÎ†π", "Ïù∏ÏÇ¨Ïù¥Îèô",
    "Î∏åÎûúÎìúÌèâÌåê", "Î∏åÎûúÎìú ÌèâÌåê", "Î∏åÎûúÎìú ÏàúÏúÑ", "Î∏åÎûúÎìúÏßÄÏàò",
    "ÏΩîÏä§Ìîº", "ÏΩîÏä§Îã•", "Ï£ºÍ∞Ä", "Ï£ºÏãù", "Ï¶ùÏãú", "ÏãúÏÑ∏", "ÎßàÍ∞ê", "Ïû•Ï§ë", "Ïû•ÎßàÍ∞ê", "Í±∞ÎûòÎüâ", "Í±∞ÎûòÎåÄÍ∏à", "ÏÉÅÌïúÍ∞Ä", "ÌïòÌïúÍ∞Ä",
    "Î¥âÏÇ¨", "ÌõÑÏõê", "Í∏∞Î∂Ä", "Ïö∞Ïäπ", "Î¨¥ÏäπÎ∂Ä", "Ìå®Î∞∞", "Ïä§Ìè¨Ï∏†", "Ïä§Ìè∞ÏÑú", "ÏßÄÏÜçÍ∞ÄÎä•", "ESG", "ÏúÑÏ¥â", "Ïù¥Î≤§Ìä∏", "ÏÇ¨Ï†ÑÏòàÏïΩ", "Ï±îÌîÑÏ†Ñ",
    "ÌîÑÎ°úÎ™®ÏÖò", "Ïó∞Í∑π", "Í≥µÏó∞", "Ïñ¥Î•¥Ïã†", "ÎßÅÏª®", "ÏóêÎπÑÏóêÏù¥ÌÑ∞", "NHÌçºÌîåÌÜµÏû•", "Í≥®ÎùºÎã¥Í∏∞",
    "ÏùåÏïÖÌöå", "ÍµêÌñ•ÏïÖÎã®", "ÏÇ¨Ïù¥Î≤ÑÎåÄ", "Ïã†ÏßÑÏÑú", "ÏïàÏÑ±Ï§Ä", "GSÏπºÌÖçÏä§Î∞∞", "ÌîÑÎ°úÍ∏∞Ï†Ñ", "Îã§Î¨∏Ìôî", "ÏôÄÏù∏25ÌîåÎü¨Ïä§", "Í≥ºÏ±ÑÏ£ºÏä§", "Ï±ÖDream", "Ï±ÖÎìúÎ¶º", "Ìä∏Î°úÌéòÏò§",
    "Î∏åÎûúÎìúÎç∞Ïù¥", "ÏáºÌïëÎùºÏù¥Î∏å", "ÏÇ∞ÌïôÌòëÎ†• Ïª®ÌçºÎü∞Ïä§", "ÎÖπÏÉâÏÉÅÌíà", "ÏÜåÎπÑÏûêÍ∞Ä ÎΩëÏùÄ", "Ï∫†ÌéòÏù∏", "ÎÇòÎàî", "Ï±îÌîºÏñ∏Ïã≠", "ÏÇ¨ÌöåÍ≥µÌóå", "ÏÑ±Í∏à"
]

# ÌïÑÌÑ∞ÎßÅÌï† Ïñ∏Î°†ÏÇ¨ ÎèÑÎ©îÏù∏ Î¶¨Ïä§Ìä∏ (www. Ï†úÍ±∞Îêú ÎèÑÎ©îÏù∏ Í∏∞Ï§Ä)
ALLOWED_SOURCES = {
    "news.einfomax.co.kr", "yna.co.kr", "newsis.com", "mk.co.kr", "news1.kr", "mt.co.kr", "hankyung.com",
    "joongang.co.kr", "chosun.com", "edaily.co.kr", "biz.heraldcorp.com", "kmib.co.kr", "seoul.co.kr",
    "thebell.co.kr", "fnnews.com", "ichannela.com", "donga.com", "hani.co.kr", "news.jtbc.co.kr",
    "khan.co.kr", "imnews.imbc.com", "mbn.co.kr", "news.sbs.co.kr", "tvchosun.com", "ytn.co.kr",
    "biz.chosun.com", "media.naver.com", "ohmynews.com", "ajunews.com", "sedaily.com", "asiae.co.kr",
    "hankookilbo.com", "nocutnews.co.kr", "bloter.net", "segye.com", "bizwatch.co.kr", "newsprime.co.kr",
    "meconomynews.com", "newsway.co.kr"
}

def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

# --- ÏÑ∏ÏÖò ÏÉÅÌÉú Î≥ÄÏàò Ï¥àÍ∏∞Ìôî ---
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False
if "selected_articles" not in st.session_state:
    st.session_state.selected_articles = []
if "cat_major_autoset" not in st.session_state:
    st.session_state.cat_major_autoset = []
if "important_articles_preview" not in st.session_state:
    st.session_state.important_articles_preview = []
if "important_selected_index" not in st.session_state:
    st.session_state["important_selected_index"] = []
if "article_checked_left" not in st.session_state:
    st.session_state.article_checked_left = {}

# --- Ï¶êÍ≤®Ï∞æÍ∏∞ Ïπ¥ÌÖåÍ≥†Î¶¨(Î≥ÄÍ≤Ω Í∏àÏßÄ) ---
favorite_categories = {
    "Íµ≠/Í≥µÏ±Ñ": [],
    "Í≥µÍ≥µÍ∏∞Í¥Ä": [],
    "Î≥¥ÌóòÏÇ¨": ["ÌòÑÎåÄÌï¥ÏÉÅ", "ÎÜçÌòëÏÉùÎ™Ö", "Î©îÎ¶¨Ï∏†ÌôîÏû¨", "ÍµêÎ≥¥ÏÉùÎ™Ö", "ÏÇºÏÑ±ÌôîÏû¨", "ÏÇºÏÑ±ÏÉùÎ™Ö", "Ïã†ÌïúÎùºÏù¥ÌîÑ", "Ìù•Íµ≠ÏÉùÎ™Ö", "ÎèôÏñëÏÉùÎ™Ö", "ÎØ∏ÎûòÏóêÏÖãÏÉùÎ™Ö"],
    "5ÎåÄÍ∏àÏúµÏßÄÏ£º": ["Ïã†ÌïúÍ∏àÏúµ", "ÌïòÎÇòÍ∏àÏúµ", "KBÍ∏àÏúµ", "ÎÜçÌòëÍ∏àÏúµ", "Ïö∞Î¶¨Í∏àÏúµ"],
    "5ÎåÄÏãúÏ§ëÏùÄÌñâ": ["ÎÜçÌòëÏùÄÌñâ", "Íµ≠ÎØºÏùÄÌñâ", "Ïã†ÌïúÏùÄÌñâ", "Ïö∞Î¶¨ÏùÄÌñâ", "ÌïòÎÇòÏùÄÌñâ"],
    "Ïπ¥ÎìúÏÇ¨": ["KBÍµ≠ÎØºÏπ¥Îìú", "ÌòÑÎåÄÏπ¥Îìú", "Ïã†ÌïúÏπ¥Îìú", "ÎπÑÏî®Ïπ¥Îìú", "ÏÇºÏÑ±Ïπ¥Îìú"],
    "Ï∫êÌîºÌÉà": ["ÌïúÍµ≠Ï∫êÌîºÌÉà", "ÌòÑÎåÄÏ∫êÌîºÌÉà"],
    "ÏßÄÏ£ºÏÇ¨": ["SKÏù¥ÎÖ∏Î≤†Ïù¥ÏÖò", "GSÏóêÎÑàÏßÄ", "SK", "GS"],
    "ÏóêÎÑàÏßÄ": ["SKÍ∞ÄÏä§", "GSÏπºÌÖçÏä§", "S-Oil", "SKÏóêÎÑàÏßÄ", "SKÏï§Î¨¥Î∏å", "ÏΩîÎ¶¨ÏïÑÏóêÎÑàÏßÄÌÑ∞ÎØ∏ÎÑê"],
    "Î∞úÏ†Ñ": ["GSÌååÏõå", "GSEPS", "ÏÇºÏ≤úÎ¶¨"],
    "ÏûêÎèôÏ∞®": ["LGÏóêÎÑàÏßÄÏÜîÎ£®ÏÖò", "ÌïúÏò®ÏãúÏä§ÌÖú", "Ìè¨Ïä§ÏΩîÌì®Ï≤òÏó†", "ÌïúÍµ≠ÌÉÄÏù¥Ïñ¥"],
    "Ï†ÑÍ∏∞/Ï†ÑÏûê": ["SKÌïòÏù¥ÎãâÏä§", "LGÏù¥ÎÖ∏ÌÖç", "LGÏ†ÑÏûê", "LSÏùºÎ†âÌä∏Î¶≠"],
    "ÏÜåÎπÑÏû¨": ["Ïù¥ÎßàÌä∏", "LF", "CJÏ†úÏùºÏ†úÎãπ", "SKÎÑ§Ìä∏ÏõçÏä§", "CJÎåÄÌïúÌÜµÏö¥"],
    "ÎπÑÏ≤†/Ï≤†Í∞ï": ["Ìè¨Ïä§ÏΩî", "ÌòÑÎåÄÏ†úÏ≤†", "Í≥†Î†§ÏïÑÏó∞"],
    "ÏÑùÏú†ÌôîÌïô": ["LGÌôîÌïô", "SKÏßÄÏò§ÏÑºÌä∏Î¶≠"],
    "Í±¥ÏÑ§": ["Ìè¨Ïä§ÏΩîÏù¥Ïï§Ïî®"],
    "ÌäπÏàòÏ±Ñ": ["Ï£ºÌÉùÎèÑÏãúÎ≥¥Ï¶ùÍ≥µÏÇ¨", "Í∏∞ÏóÖÏùÄÌñâ"]
}

excel_company_categories = {
    "Íµ≠/Í≥µÏ±Ñ": [],
    "Í≥µÍ≥µÍ∏∞Í¥Ä": [],
    "Î≥¥ÌóòÏÇ¨": [
        "ÌòÑÎåÄÌï¥ÏÉÅÌôîÏû¨Î≥¥Ìóò(ÌõÑ)", "ÎÜçÌòëÏÉùÎ™ÖÎ≥¥Ìóò(ÌõÑ)", "Î©îÎ¶¨Ï∏†ÌôîÏû¨Ìï¥ÏÉÅÎ≥¥Ìóò(ÌõÑ)", "ÍµêÎ≥¥ÏÉùÎ™Ö(ÌõÑ)",
        "ÏÇºÏÑ±ÌôîÏû¨", "ÏÇºÏÑ±ÏÉùÎ™Ö", "Ïã†ÌïúÎùºÏù¥ÌîÑ(ÌõÑ)", "Ìù•Íµ≠ÏÉùÎ™ÖÎ≥¥Ìóò(ÌõÑ)", "ÎèôÏñëÏÉùÎ™ÖÎ≥¥Ìóò(ÌõÑ)", "ÎØ∏ÎûòÏóêÏÖãÏÉùÎ™Ö(ÌõÑ)"
    ],
    "5ÎåÄÍ∏àÏúµÏßÄÏ£º": [
        "Ïã†ÌïúÏßÄÏ£º", "ÌïòÎÇòÍ∏àÏúµÏßÄÏ£º", "KBÍ∏àÏúµ", "ÎÜçÌòëÍ∏àÏúµÏßÄÏ£º", "Ïö∞Î¶¨Í∏àÏúµÏßÄÏ£º"
    ],
    "5ÎåÄÏãúÏ§ëÏùÄÌñâ": [
        "ÎÜçÌòëÏùÄÌñâ", "Íµ≠ÎØºÏùÄÌñâ", "Ïã†ÌïúÏùÄÌñâ", "Ïö∞Î¶¨ÏùÄÌñâ", "ÌïòÎÇòÏùÄÌñâ"
    ],
    "Ïπ¥ÎìúÏÇ¨": [
        "ÏºÄÏù¥ÎπÑÏπ¥Îìú", "ÌòÑÎåÄÏπ¥Îìú", "Ïã†ÌïúÏπ¥Îìú", "ÎπÑÏî®Ïπ¥Îìú", "ÏÇºÏÑ±Ïπ¥Îìú"
    ],
    "Ï∫êÌîºÌÉà": [
        "ÌïúÍµ≠Ï∫êÌîºÌÉà", "ÌòÑÎåÄÏ∫êÌîºÌÉà"
    ],
    "ÏßÄÏ£ºÏÇ¨": [
        "SKÏù¥ÎÖ∏Î≤†Ïù¥ÏÖò", "ÏßÄÏóêÏä§ÏóêÎÑàÏßÄ", "SK", "GS"
    ],
    "ÏóêÎÑàÏßÄ": [
        "SKÍ∞ÄÏä§", "GSÏπºÌÖçÏä§", "S-Oil", "SKÏóêÎÑàÏßÄ", "ÏóêÏä§ÏºÄÏù¥ÏóîÎ¨¥Î∏å", "ÏΩîÎ¶¨ÏïÑÏóêÎÑàÏßÄÌÑ∞ÎØ∏ÎÑê"
    ],
    "Î∞úÏ†Ñ": [
        "GSÌååÏõå", "ÏßÄÏóêÏä§Ïù¥ÌîºÏóêÏä§", "ÏÇºÏ≤úÎ¶¨"
    ],
    "ÏûêÎèôÏ∞®": [
        "LGÏóêÎÑàÏßÄÏÜîÎ£®ÏÖò", "ÌïúÏò®ÏãúÏä§ÌÖú", "Ìè¨Ïä§ÏΩîÌì®Ï≤òÏó†", "ÌïúÍµ≠ÌÉÄÏù¥Ïñ¥Ïï§ÌÖåÌÅ¨ÎÜÄÎ°úÏßÄ"
    ],
    "Ï†ÑÍ∏∞/Ï†ÑÏûê": [
        "SKÌïòÏù¥ÎãâÏä§", "LGÏù¥ÎÖ∏ÌÖç", "LGÏ†ÑÏûê", "ÏóòÏóêÏä§ÏùºÎ†âÌä∏Î¶≠"
    ],
    "ÏÜåÎπÑÏû¨": [
        "Ïù¥ÎßàÌä∏", "LF", "CJÏ†úÏùºÏ†úÎãπ", "SKÎÑ§Ìä∏ÏõçÏä§", "CJÎåÄÌïúÌÜµÏö¥"
    ],
    "ÎπÑÏ≤†/Ï≤†Í∞ï": [
        "Ìè¨Ïä§ÏΩî", "ÌòÑÎåÄÏ†úÏ≤†", "Í≥†Î†§ÏïÑÏó∞"
    ],
    "ÏÑùÏú†ÌôîÌïô": [
        "LGÌôîÌïô", "SKÏßÄÏò§ÏÑºÌä∏Î¶≠"
    ],
    "Í±¥ÏÑ§": [
        "Ìè¨Ïä§ÏΩîÏù¥Ïï§Ïî®"
    ],
    "ÌäπÏàòÏ±Ñ": [
        "Ï£ºÌÉùÎèÑÏãúÎ≥¥Ï¶ùÍ≥µÏÇ¨", "Í∏∞ÏóÖÏùÄÌñâ"
    ]
}

# --- Í≥µÌÜµ ÌïÑÌÑ∞ ÏòµÏÖò(ÎåÄÎ∂ÑÎ•ò/ÏÜåÎ∂ÑÎ•ò ÏóÜÏù¥ Î™®Îëê Ï†ÅÏö©) ---
common_filter_categories = {
    "Ïã†Ïö©/Îì±Í∏â": [
        "Ïã†Ïö©Îì±Í∏â", "Îì±Í∏âÏ†ÑÎßù", "ÌïòÎùΩ", "Í∞ïÎì±", "ÌïòÌñ•", "ÏÉÅÌñ•", "ÎîîÌè¥Ìä∏", "Î∂ÄÏã§", "Î∂ÄÎèÑ", "ÎØ∏ÏßÄÍ∏â", "ÏàòÏöî ÎØ∏Îã¨", "ÎØ∏Îß§Í∞Å", "Ï†úÎèÑ Í∞úÌé∏", "EOD"
    ],
    "ÏàòÏöî/Í≥µÍ∏â": [
        "ÏàòÏöî", "Í≥µÍ∏â", "ÏàòÍ∏â", "ÎëîÌôî", "ÏúÑÏ∂ï", "ÏÑ±Ïû•", "Í∏âÎì±", "Í∏âÎùΩ", "ÏÉÅÏäπ", "ÌïòÎùΩ", "Î∂ÄÏßÑ", "Ïã¨Ìôî"
    ],
    "Ïã§Ï†Å/Ïû¨Î¨¥": [
        "Ïã§Ï†Å", "Îß§Ï∂ú", "ÏòÅÏóÖÏù¥Ïùµ", "Ï†ÅÏûê", "ÏÜêÏã§", "ÎπÑÏö©", "Î∂ÄÏ±ÑÎπÑÏú®", "Ïù¥ÏûêÎ≥¥ÏÉÅÎ∞∞Ïú®"
    ],
    "ÏûêÍ∏à/Ï°∞Îã¨": [
        "Ï∞®ÏûÖ", "Ï°∞Îã¨", "ÏÑ§ÎπÑÌà¨Ïûê", "ÌöåÏÇ¨Ï±Ñ", "Î∞úÌñâ", "Ïù∏Ïàò", "Îß§Í∞Å"
    ],
    "Íµ¨Ï°∞/Ï°∞Ï†ï": [
        "M&A", "Ìï©Î≥ë", "Í≥ÑÏó¥ Î∂ÑÎ¶¨", "Íµ¨Ï°∞Ï°∞Ï†ï", "Îã§Í∞ÅÌôî", "Íµ¨Ï°∞ Ïû¨Ìé∏"
    ],
    "Í±∞Ïãú/Ï†ïÏ±Ö": [
        "Í∏àÎ¶¨", "ÌôòÏú®", "Í¥ÄÏÑ∏", "Î¨¥Ïó≠Ï†úÏû¨", "Î≥¥Ï°∞Í∏à", "ÏÑ∏Ïï° Í≥µÏ†ú", "Í≤ΩÏüÅ"
    ],
    "ÏßÄÎ∞∞Íµ¨Ï°∞/Î≤ï": [
        "Ìö°Î†π", "Î∞∞ÏûÑ", "Í≥µÏ†ïÍ±∞Îûò", "Ïò§ÎÑàÎ¶¨Ïä§ÌÅ¨", "ÎåÄÏ£ºÏ£º", "ÏßÄÎ∞∞Íµ¨Ï°∞"
    ]
}
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)

# --- ÏÇ∞ÏóÖÎ≥Ñ ÌïÑÌÑ∞ ÏòµÏÖò ---
industry_filter_categories = {
    "ÏùÄÌñâ Î∞è Í∏àÏúµÏßÄÏ£º": [
        "Í≤ΩÏòÅÏã§ÌÉúÌèâÍ∞Ä", "BIS", "CET1", "ÏûêÎ≥∏ÎπÑÏú®", "ÏÉÅÍ∞ÅÌòï Ï°∞Í±¥Î∂ÄÏûêÎ≥∏Ï¶ùÍ∂å", "ÏûêÎ≥∏ÌôïÏ∂©", "ÏûêÎ≥∏Ïó¨Î†•", "ÏûêÎ≥∏Ï†ÅÏ†ïÏÑ±", "LCR",
        "Ï°∞Îã¨Í∏àÎ¶¨", "NIM", "ÏàúÏù¥ÏûêÎßàÏßÑ", "Í≥†Ï†ïÏù¥ÌïòÏó¨Ïã†ÎπÑÏú®", "ÎåÄÏÜêÏ∂©ÎãπÍ∏à", "Ï∂©ÎãπÍ∏à", "Î∂ÄÏã§Ï±ÑÍ∂å", "Ïó∞Ï≤¥Ïú®", "Í∞ÄÍ≥ÑÎåÄÏ∂ú", "Ï∑®ÏïΩÏ∞®Ï£º"
    ],
    "Î≥¥ÌóòÏÇ¨": [
        "Î≥¥Ïû•ÏÑ±Î≥¥Ìóò", "Ï†ÄÏ∂ïÏÑ±Î≥¥Ìóò", "Î≥ÄÏï°Î≥¥Ìóò", "Ìá¥ÏßÅÏó∞Í∏à", "ÏùºÎ∞òÎ≥¥Ìóò", "ÏûêÎèôÏ∞®Î≥¥Ìóò", "ALM", "ÏßÄÍ∏âÏó¨Î†•ÎπÑÏú®", "K-ICS",
        "Î≥¥ÌóòÏàòÏùµÏÑ±", "Î≥¥ÌóòÏÜêÏùµ", "ÏàòÏûÖÎ≥¥ÌóòÎ£å", "CSM", "ÏÉÅÍ∞Å", "Ìà¨ÏûêÏÜêÏùµ", "Ïö¥Ïö©ÏÑ±Í≥º", "IFRS4", "IFRS17", "Î≥¥ÌóòÎ∂ÄÏ±Ñ",
        "Ïû•Í∏∞ÏÑ†ÎèÑÍ∏àÎ¶¨", "ÏµúÏ¢ÖÍ¥ÄÏ∞∞ÎßåÍ∏∞", "Ïú†ÎèôÏÑ± ÌîÑÎ¶¨ÎØ∏ÏóÑ", "Ïã†Ï¢ÖÏûêÎ≥∏Ï¶ùÍ∂å", "ÌõÑÏàúÏúÑÏ±Ñ", "ÏúÑÌóòÏûêÏÇ∞ÎπÑÏ§ë", "Í∞ÄÏ§ëÎ∂ÄÏã§ÏûêÏÇ∞ÎπÑÏú®"
    ],
    "Ïπ¥ÎìúÏÇ¨": [
        "ÎØºÍ∞ÑÏÜåÎπÑÏßÄÌëú", "ÎåÄÏÜêÏ§ÄÎπÑÍ∏à", "Í∞ÄÍ≥ÑÎ∂ÄÏ±Ñ", "Ïó∞Ï≤¥Ïú®", "Í∞ÄÎßπÏ†êÏπ¥ÎìúÏàòÏàòÎ£å", "ÎåÄÏ∂úÏÑ±ÏûêÏÇ∞", "Ïã†Ïö©ÌåêÎß§ÏûêÏÇ∞", "Í≥†Ï†ïÏù¥ÌïòÏó¨Ïã†", "Î†àÎ≤ÑÎ¶¨ÏßÄÎ∞∞Ïú®",
        "Í±¥Ï†ÑÏÑ±", "ÏºÄÏù¥Î±ÖÌÅ¨", "Ïù¥ÌÉà"
    ],
    "Ï∫êÌîºÌÉà": [
        "Ï∂©ÎãπÍ∏àÏª§Î≤ÑÎ¶¨ÏßÄÎπÑÏú®", "Í≥†Ï†ïÏù¥ÌïòÏó¨Ïã†", "PFÍµ¨Ï°∞Ï°∞Ï†ï", "Î¶¨Ïä§ÏûêÏÇ∞", "ÏÜêÏã§Ìù°ÏàòÎä•Î†•", "Î∂ÄÎèôÏÇ∞PFÏó∞Ï≤¥Ï±ÑÍ∂å", "ÏûêÏÇ∞Ìè¨Ìä∏Ìè¥Î¶¨Ïò§", "Í±¥Ï†ÑÏÑ±",
        "Ï°∞Ï†ïÏ¥ùÏûêÏÇ∞ÏàòÏùµÎ•†", "Íµ∞Ïù∏Í≥µÏ†úÌöå"
    ],
    "ÏßÄÏ£ºÏÇ¨": [
        "SKÏßÄÏò§ÏÑºÌä∏Î¶≠", "SKÏóêÎÑàÏßÄ", "SKÏóîÎ¨¥Î∏å", "SKÏù∏Ï≤úÏÑùÏú†ÌôîÌïô", "GSÏπºÌÖçÏä§", "GSÌååÏõå", "SKÏù¥ÎÖ∏Î≤†Ïù¥ÏÖò", "SKÌÖîÎ†àÏΩ§", "SKÏò®",
        "GSÏóêÎÑàÏßÄ", "GSÎ¶¨ÌÖåÏùº", "GS E&C", "2Ï∞®Ï†ÑÏßÄ", "ÏÑùÏú†ÌôîÌïô", "Ïú§ÌôúÏú†", "Ï†ÑÍ∏∞Ï∞®", "Î∞∞ÌÑ∞Î¶¨", "Ï†ïÏú†", "Ïù¥ÎèôÌÜµÏã†"
    ],
    "ÏóêÎÑàÏßÄ": [
        "Ï†ïÏú†", "Ïú†Í∞Ä", "Ï†ïÏ†úÎßàÏßÑ", "Ïä§ÌîÑÎ†àÎìú", "Í∞ÄÎèôÎ•†", "Ïû¨Í≥† ÏÜêÏã§", "Ï§ëÍµ≠ ÏàòÏöî", "IMO Í∑úÏ†ú", "Ï†ÄÏú†Ìô© Ïó∞Î£å", "LNG",
        "ÌÑ∞ÎØ∏ÎÑê", "Ïú§ÌôúÏú†"
    ],
    "Î∞úÏ†Ñ": [
        "LNG", "Ï≤úÏó∞Í∞ÄÏä§", "Ïú†Í∞Ä", "SMP", "REC", "Í≥ÑÌÜµÏãúÏû•", "ÌÉÑÏÜåÏÑ∏", "ÌÉÑÏÜåÎ∞∞Ï∂úÍ∂å", "Ï†ÑÎ†•ÏãúÏû• Í∞úÌé∏", "Ï†ÑÎ†• ÏûêÏú®Ìôî",
        "Í∞ÄÎèôÎ•†", "ÎèÑÏãúÍ∞ÄÏä§"
    ],
    "ÏûêÎèôÏ∞®": [
        "AMPC Î≥¥Ï°∞Í∏à", "IRA Ïù∏ÏÑºÌã∞Î∏å", "Ï§ëÍµ≠ Î∞∞ÌÑ∞Î¶¨", "EV ÏàòÏöî", "Ï†ÑÍ∏∞Ï∞®", "ESSÏàòÏöî", "Î¶¨Ìä¨", "ÌÉÄÏù¥Ïñ¥"
    ],
    "Ï†ÑÍ∏∞Ï†ÑÏûê": [
        "CHIPS Î≥¥Ï°∞Í∏à", "Ï§ëÍµ≠", "DRAM", "HBM", "Í¥ëÌï†ÏÜîÎ£®ÏÖò", "ÏïÑÏù¥Ìè∞", "HVAC", "HVTR"
    ],
    "Ï≤†Í∞ï": [
        "Ï≤†Í¥ëÏÑù", "ÌõÑÌåê", "Í∞ïÌåê", "Ï≤†Í∑º", "Ïä§ÌîÑÎ†àÎìú", "Ï≤†Í∞ï", "Í∞ÄÎèôÎ•†", "Ï†úÏ≤†ÏÜå", "ÏÖßÎã§Ïö¥", "Ï§ëÍµ≠ÏÇ∞ Ï†ÄÍ∞Ä",
        "Ï§ëÍµ≠ ÏàòÏ∂ú Í∞êÏÜå", "Í±¥ÏÑ§Í≤ΩÍ∏∞", "Ï°∞ÏÑ† ÏàòÏöî", "ÌååÏóÖ"
    ],
    "ÎπÑÏ≤†": [
        "Ïó∞", "ÏïÑÏó∞", "ÎãàÏºà", "ÏïàÌã∞Î™®Îãà", "Í≤ΩÏòÅÍ∂å Î∂ÑÏüÅ", "MBK", "ÏòÅÌíç"
    ],
    "ÏÜåÎß§": [
        "ÎÇ¥ÏàòÎ∂ÄÏßÑ", "ÏãúÏû•ÏßÄÎ∞∞Î†•", "SKÌÖîÎ†àÏΩ§", "SKÎß§ÏßÅ", "CLS", "HMR", "ÎùºÏù¥Ïã†", "ÏïÑÎØ∏ÎÖ∏ÏÇ∞", "ÏäàÏôÑÏä§Ïª¥ÌçºÎãà",
        "ÏùòÎ•ò", "Ïã†ÏÑ∏Í≥Ñ", "ÎåÄÌòïÎßàÌä∏ ÏùòÎ¨¥Ìú¥ÏóÖ", "GÎßàÏºì", "WÏª®ÏÖâ", "Ïä§ÌÉÄÌïÑÎìú"
    ],
    "ÏÑùÏú†ÌôîÌïô": [
        "ÏÑùÏú†ÌôîÌïô", "ÏÑùÌôî", "Ïú†Í∞Ä", "Ï¶ùÏÑ§", "Ïä§ÌîÑÎ†àÎìú", "Í∞ÄÎèôÎ•†", "PX", "Î≤§Ï††", "Ï§ëÍµ≠ Ï¶ùÏÑ§", "Ï§ëÎèô COTC",
        "LGÏóêÎÑàÏßÄÏÜîÎ£®ÏÖò", "Ï†ÑÍ∏∞Ï∞®", "Î∞∞ÌÑ∞Î¶¨", "Î¶¨Ìä¨", "IRA", "AMPC"
    ],
    "Í±¥ÏÑ§": [
        "Ï≤†Í∑º Í∞ÄÍ≤©", "ÏãúÎ©òÌä∏ Í∞ÄÍ≤©", "Í≥µÏÇ¨ÎπÑ", "SOC ÏòàÏÇ∞", "ÎèÑÏãúÏ†ïÎπÑ ÏßÄÏõê", "Ïö∞Î∞úÏ±ÑÎ¨¥", "ÏàòÏ£º", "Ï£ºÍ∞ÑÏÇ¨", "ÏÇ¨Í≥†",
        "ÏãúÍ≥µÎä•Î†•ÏàúÏúÑ", "ÎØ∏Î∂ÑÏñë", "ÎåÄÏÜêÏ∂©ÎãπÍ∏à"
    ],
    "ÌäπÏàòÏ±Ñ": [
        "ÏûêÎ≥∏ÌôïÏ∂©", "HUG", "Ï†ÑÏÑ∏ÏÇ¨Í∏∞", "Î≥¥Ï¶ùÏÇ¨Í≥†", "Î≥¥Ï¶ùÎ£åÏú®", "ÌöåÏàòÏú®", "Î≥¥Ï¶ùÏûîÏï°", "ÎåÄÏúÑÎ≥ÄÏ†úÏï°",
        "Ï§ëÏÜåÍ∏∞ÏóÖÎåÄÏ∂ú", "ÎåÄÏÜêÏ∂©ÎãπÍ∏à", "Î∂ÄÏã§Ï±ÑÍ∂å", "Î∂àÎ≤ï", "Íµ¨ÏÜç"
    ]
}

# --- Ïπ¥ÌÖåÍ≥†Î¶¨-ÏÇ∞ÏóÖ ÎåÄÎ∂ÑÎ•ò Îß§Ìïë Ìï®Ïàò ---
def get_industry_majors_from_favorites(selected_categories):
    favorite_to_industry_major = {
        "5ÎåÄÍ∏àÏúµÏßÄÏ£º": ["ÏùÄÌñâ Î∞è Í∏àÏúµÏßÄÏ£º"],
        "5ÎåÄÏãúÏ§ëÏùÄÌñâ": ["ÏùÄÌñâ Î∞è Í∏àÏúµÏßÄÏ£º"],
        "Î≥¥ÌóòÏÇ¨": ["Î≥¥ÌóòÏÇ¨"],
        "Ïπ¥ÎìúÏÇ¨": ["Ïπ¥ÎìúÏÇ¨"],
        "Ï∫êÌîºÌÉà": ["Ï∫êÌîºÌÉà"],
        "ÏßÄÏ£ºÏÇ¨": ["ÏßÄÏ£ºÏÇ¨"],
        "ÏóêÎÑàÏßÄ": ["ÏóêÎÑàÏßÄ"],
        "Î∞úÏ†Ñ": ["Î∞úÏ†Ñ"],
        "ÏûêÎèôÏ∞®": ["ÏûêÎèôÏ∞®"],
        "ÏÑùÏú†ÌôîÌïô": ["ÏÑùÏú†ÌôîÌïô"],
        "Ï†ÑÍ∏∞/Ï†ÑÏûê": ["Ï†ÑÍ∏∞Ï†ÑÏûê"],
        "ÎπÑÏ≤†/Ï≤†Í∞ï": ["Ï≤†Í∞ï", "ÎπÑÏ≤†"],
        "ÏÜåÎπÑÏû¨": ["ÏÜåÎß§"],
        "Í±¥ÏÑ§": ["Í±¥ÏÑ§"],
        "ÌäπÏàòÏ±Ñ": ["ÌäπÏàòÏ±Ñ"],
    }
    majors = set()
    for cat in selected_categories:
        for major in favorite_to_industry_major.get(cat, []):
            majors.add(major)
    return list(majors)

# --- UI ÏãúÏûë ---
st.set_page_config(layout="wide")
col_title, col_option1, col_option2 = st.columns([0.6, 0.2, 0.2])
with col_title:
    st.markdown(
        "&lt;h1 style='color:#1a1a1a; margin-bottom:0.5rem;'&gt;"
        "&lt;a href='https://credit-issue-monitoring-news-sending.onrender.com/' target='_blank' style='text-decoration:none; color:#1a1a1a;'&gt;"
        "üìä Credit Issue Monitoring&lt;/a&gt;&lt;/h1&gt;",
        unsafe_allow_html=True
    )
with col_option1:
    show_sentiment_badge = st.checkbox("Í∏∞ÏÇ¨Î™©Î°ùÏóê Í∞êÏÑ±Î∂ÑÏÑù Î∞∞ÏßÄ ÌëúÏãú", value=False, key="show_sentiment_badge")
with col_option2:
    enable_summary = st.checkbox("ÏöîÏïΩ Í∏∞Îä• Ï†ÅÏö©", value=False, key="enable_summary")

col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input(label="", value="", key="keyword_input", label_visibility="collapsed")
with col_kw_btn:
    search_clicked = st.button("Í≤ÄÏÉâ", key="search_btn", help="ÌÇ§ÏõåÎìúÎ°ú Í≤ÄÏÉâ", use_container_width=True)

st.markdown("**‚≠ê ÏÇ∞ÏóÖÍµ∞ ÏÑ†ÌÉù**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        "",
        list(favorite_categories.keys()), key="cat_multi", label_visibility="collapsed"
        )
if selected_categories:
    auto_selected_majors = get_industry_majors_from_favorites(selected_categories)
    st.session_state.cat_major_autoset = auto_selected_majors.copy()
else:
    st.session_state.cat_major_autoset = []
with col_cat_btn:
    category_search_clicked = st.button("üîç Í≤ÄÏÉâ", key="cat_search_btn", help="Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú Í≤ÄÏÉâ", use_container_width=True)
for cat in selected_categories:
    st.session_state.favorite_keywords.update(favorite_categories[cat])

# ÎÇ†Ïßú ÏûÖÎ†• (Í∏∞Î≥∏ ÏÑ∏ÌåÖ: Ï¢ÖÎ£åÏùº=Ïò§Îäò, ÏãúÏûëÏùº=Ïò§Îäò-7Ïùº)
today = datetime.today().date()
if "end_date" not in st.session_state:
    st.session_state["end_date"] = today
if "start_date" not in st.session_state:
    st.session_state["start_date"] = today - timedelta(days=7)
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ÏãúÏûëÏùº", value=st.session_state["start_date"], key="start_date_input")
    st.session_state["start_date"] = start_date
with date_col2:
    end_date = st.date_input("Ï¢ÖÎ£åÏùº", value=st.session_state["end_date"], key="end_date_input")
    st.session_state["end_date"] = end_date

with st.expander("üß© Í≥µÌÜµ ÌïÑÌÑ∞ ÏòµÏÖò (Ìï≠ÏÉÅ Ï†ÅÏö©Îê®)"):
    for major, subs in common_filter_categories.items():
        st.markdown(f"**{major}**: {', '.join(subs)}")

with st.expander("üè≠ ÏÇ∞ÏóÖÎ≥Ñ ÌïÑÌÑ∞ ÏòµÏÖò (ÎåÄÎ∂ÑÎ•òÎ≥Ñ ÏÜåÎ∂ÑÎ•ò ÌïÑÌÑ∞ÎßÅ)"):
    use_industry_filter = st.checkbox("Ïù¥ ÌïÑÌÑ∞ Ï†ÅÏö©", value=True, key="use_industry_filter")
    
    # ÏÑ∏ÏÖò Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
    if "industry_major_sub_map" not in st.session_state:
        st.session_state.industry_major_sub_map = {}

    # UI: ÏÑ†ÌÉùÎêú ÏÇ∞ÏóÖÍµ∞ÏóêÏÑú ÏûêÎèô Îß§ÌïëÎêú ÎåÄÎ∂ÑÎ•ò Ï∂îÏ∂ú
    selected_major_map = get_industry_majors_from_favorites(selected_categories)

    updated_map = {}
    for major in selected_major_map:
        options = industry_filter_categories.get(major, [])
        default_selected = options if major not in st.session_state.industry_major_sub_map else st.session_state.industry_major_sub_map[major]
        selected_sub = st.multiselect(
            f"{major} ÏÜåÎ∂ÑÎ•ò ÌÇ§ÏõåÎìú",
            options,
            default=default_selected,
            key=f"subfilter_{major}"
        )
        updated_map[major] = selected_sub

    st.session_state.industry_major_sub_map = updated_map
    
# --- Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ï†úÍ±∞ Í∏∞Îä• Ï≤¥ÌÅ¨Î∞ïÏä§ Ìè¨Ìï®Îêú ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ ÏòµÏÖò ---
with st.expander("üîç ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ ÏòµÏÖò"):
    require_exact_keyword_in_title_or_content = st.checkbox("ÌÇ§ÏõåÎìúÍ∞Ä Ï†úÎ™© ÎòêÎäî Î≥∏Î¨∏Ïóê Ìè¨Ìï®Îêú Í∏∞ÏÇ¨Îßå Î≥¥Í∏∞", value=True, key="require_exact_keyword_in_title_or_content")
    # Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ï†úÍ±∞ Ï≤¥ÌÅ¨Î∞ïÏä§ Ï∂îÍ∞Ä (Í∏∞Î≥∏ Ìï¥Ï†ú)
    remove_duplicate_articles = st.checkbox("Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ï†úÍ±∞", value=False, key="remove_duplicate_articles", help="ÌÇ§ÏõåÎìú Í≤ÄÏÉâ ÌõÑ Ï§ëÎ≥µ Í∏∞ÏÇ¨Î•º Ï†úÍ±∞Ìï©ÎãàÎã§.")
    filter_allowed_sources_only = st.checkbox(
    "ÌäπÏ†ï Ïñ∏Î°†ÏÇ¨Îßå Í≤ÄÏÉâ", 
    value=True, 
    key="filter_allowed_sources_only", 
    help="ÏÑ†ÌÉùÎêú Î©îÏù¥Ï†Ä Ïñ∏Î°†ÏÇ¨Îßå ÌïÑÌÑ∞ÎßÅÌïòÍ≥†, Í∑∏ Ïô∏ Ïñ∏Î°†ÏùÄ Ï†úÏô∏Ìï©ÎãàÎã§."
)

def extract_article_text(url):
    try:
        article = newspaper.Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"Î≥∏Î¨∏ Ï∂îÏ∂ú Ïò§Î•ò: {e}"

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[Í∞Ä-Ìû£]", text) else "en"

def summarize_and_sentiment_with_openai(text, do_summary=True):
    if not OPENAI_API_KEY:
        return "OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            ("ÏïÑÎûò Í∏∞ÏÇ¨ Î≥∏Î¨∏ÏùÑ Í∞êÏÑ±Î∂ÑÏÑù(Í∏çÏ†ï/Î∂ÄÏ†ïÎßå)ÌïòÍ≥†" +
             ("\n- [Ìïú Ï§Ñ ÏöîÏïΩ]: Í∏∞ÏÇ¨ Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏùÑ Ìïú Î¨∏Ïû•ÏúºÎ°ú ÏöîÏïΩ" if do_summary else "") +
             "\n- [Í∞êÏÑ±]: Í∏∞ÏÇ¨ Ï†ÑÏ≤¥Ïùò Í∞êÏ†ïÏùÑ Í∏çÏ†ï/Î∂ÄÏ†ï Ï§ë ÌïòÎÇòÎ°úÎßå ÎãµÌï¥Ï§ò. Ï§ëÎ¶ΩÏùÄ Ï†àÎåÄ ÎãµÌïòÏßÄ Îßà. ÌååÏÇ∞, ÏûêÍ∏àÎÇú Îì± Î∂ÄÏ†ïÏ†Å ÏÇ¨Í±¥Ïù¥ Ï§ëÏã¨Ïù¥Î©¥ Î∞òÎìúÏãú 'Î∂ÄÏ†ï'ÏúºÎ°ú ÎãµÌï¥Ï§ò.\n\n"
             "ÏïÑÎûò Ìè¨Îß∑ÏúºÎ°ú ÎãµÎ≥ÄÌï¥Ï§ò:\n" +
             ("[Ìïú Ï§Ñ ÏöîÏïΩ]: (Ïó¨Í∏∞Ïóê Ìïú Ï§Ñ ÏöîÏïΩ)\n" if do_summary else "") +
             "[Í∞êÏÑ±]: (Í∏çÏ†ï/Î∂ÄÏ†ï Ï§ë ÌïòÎÇòÎßå)\n\n"
             "[Í∏∞ÏÇ¨ Î≥∏Î¨∏]\n" + text)
        )
    else:
        prompt = (
            ("Analyze the following news article for sentiment (positive/negative only)." +
             ("\n- [One-line Summary]: Summarize the entire article in one sentence." if do_summary else "") +
             "\n- [Sentiment]: Classify the overall sentiment as either positive or negative ONLY. Never answer 'neutral'. If the article is about bankruptcy, crisis, etc., answer 'negative'.\n\n"
             "Respond in this format:\n" +
             ("[One-line Summary]: (your one-line summary)\n" if do_summary else "") +
             "[Sentiment]: (positive/negative only)\n\n"
             "[ARTICLE]\n" + text)
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[Ìïú Ï§Ñ ÏöîÏïΩ\]:\s*(.+)", answer)
        m3 = re.search(r"\[Í∞êÏÑ±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if (do_summary and m1) else ""
    summary = ""
    sentiment = m3.group(1).strip() if m3 else ""
    if sentiment.lower() in ['neutral', 'Ï§ëÎ¶Ω', '']:
        sentiment = 'Î∂ÄÏ†ï' if lang == "ko" else 'negative'
    if lang == "en":
        sentiment = 'Í∏çÏ†ï' if sentiment.lower() == 'positive' else 'Î∂ÄÏ†ï'
    return one_line, summary, sentiment, text

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID
    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()

            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(re.sub("<.*?>", "", title), EXCLUDE_TITLE_KEYWORDS):
                continue

            source = item.get("source")
            if not source or source.strip() == "":
                source = infer_source_from_url(item.get("originallink", ""))
                if not source:
                    source = "Naver"
            source_domain = source.lower()
            if source_domain.startswith("www."):
                source_domain = source_domain[4:]

            # Ïñ∏Î°†ÏÇ¨ ÌïÑÌÑ∞ÎßÅ: ALLOWED_SOURCESÏóê ÏóÜÏúºÎ©¥ Ïä§ÌÇµ
            if source_domain not in ALLOWED_SOURCES:
                continue

            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": source_domain
            })

        if len(items) < 100:
            break
    return articles[:limit]

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def summarize_article_from_url(article_url, title, do_summary=True):
    cache_key_base = re.sub(r"\W+", "", article_url)[-16:]
    summary_key = f"summary_{cache_key_base}"

    # Ïù¥ÎØ∏ ÏöîÏïΩ Í≤∞Í≥ºÍ∞Ä ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú Î∞òÌôò
    if summary_key in st.session_state:
        return st.session_state[summary_key]

    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("Î≥∏Î¨∏ Ï∂îÏ∂ú Ïò§Î•ò"):
            result = (full_text, None, None, None)
        else:
            one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text, do_summary=do_summary)
            result = (one_line, summary, sentiment, full_text)
    except Exception as e:
        result = (f"ÏöîÏïΩ Ïò§Î•ò: {e}", None, None, None)

    # Ï∫êÏãúÏóê Ï†ÄÏû•
    st.session_state[summary_key] = result
    return result

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    title = article.get("title", "")
    content = ""
    cache_key = article.get("link", "")
    summary_cache_key = None
    for key in st.session_state.keys():
        if key.startswith("summary_") and cache_key in key:
            summary_cache_key = key
            break
    if summary_cache_key and isinstance(st.session_state[summary_cache_key], tuple):
        _, _, _, content = st.session_state[summary_cache_key]
    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

def article_passes_all_filters(article):
    # Ï†úÎ™©Ïóê Ï†úÏô∏ ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®ÎêòÎ©¥ Ï†úÏô∏
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False

    # ÎÇ†Ïßú Î≤îÏúÑ ÌïÑÌÑ∞ÎßÅ
    try:
        pub_date = datetime.strptime(article['date'], '%Y-%m-%d').date()
        if pub_date < st.session_state.get("start_date") or pub_date > st.session_state.get("end_date"):
            return False
    except:
        return False

    # ÌÇ§ÏõåÎìú ÌïÑÌÑ∞: ÏûÖÎ†• ÌÇ§ÏõåÎìú OR Ïπ¥ÌÖåÍ≥†Î¶¨ ÌÇ§ÏõåÎìúÏóê Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï®
    all_keywords = []
    if "keyword_input" in st.session_state:
        all_keywords.extend([k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()])
    if "cat_multi" in st.session_state:
        for cat in st.session_state["cat_multi"]:
            all_keywords.extend(favorite_categories.get(cat, []))

    if not article_contains_exact_keyword(article, all_keywords):
        return False

    # ‚úÖ Ïñ∏Î°†ÏÇ¨ ÎèÑÎ©îÏù∏ ÌïÑÌÑ∞ÎßÅ (Ï≤¥ÌÅ¨Î∞ïÏä§Î°ú ON/OFF Í∞ÄÎä•)
    if st.session_state.get("filter_allowed_sources_only", True):
        source = article.get('source', '').lower()
        if source.startswith("www."):
            source = source[4:]
        if source not in ALLOWED_SOURCES:
            return False

    # Í≥µÌÜµ ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ Ï°∞Í±¥ (OR Ï°∞Í±¥)
    common_passed = or_keyword_filter(article, ALL_COMMON_FILTER_KEYWORDS)

    # ÏÇ∞ÏóÖ ÌïÑÌÑ∞ Ï°∞Í±¥ (ÏÇ¨Ïö© Ïãú)
    industry_passed = True
    if st.session_state.get("use_industry_filter", False):
        keyword = article.get("ÌÇ§ÏõåÎìú")
        matched_major = None
        for cat, companies in favorite_categories.items():
            if keyword in companies:
                majors = get_industry_majors_from_favorites([cat])
                if majors:
                    matched_major = majors[0]
                    break
        if matched_major:
            sub_keyword_filter = st.session_state.industry_major_sub_map.get(matched_major, [])
            if sub_keyword_filter:
                industry_passed = or_keyword_filter(article, sub_keyword_filter)

    if not (common_passed or industry_passed):
        return False

    return True

# --- Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ï†úÍ±∞ Ìï®Ïàò ---
def is_similar(title1, title2, threshold=0.5):
    ratio = difflib.SequenceMatcher(None, title1, title2).ratio()
    return ratio >= threshold

def remove_duplicates(articles):
    unique_articles = []
    titles = []
    for article in articles:
        title = article.get("title", "")
        if all(not is_similar(title, existing_title) for existing_title in titles):
            unique_articles.append(article)
            titles.append(title)
    return unique_articles

# Ìï≠ÏÉÅ Î®ºÏ†Ä ÏÑ†Ïñ∏Ìï¥ ÏóêÎü¨ Î∞©ÏßÄ
keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()] if keywords_input else []
search_clicked = False

if keyword_list:
        search_clicked = True

if keyword_list and (search_clicked or st.session_state.get("search_triggered")):
    with st.spinner("Îâ¥Ïä§ Í≤ÄÏÉâ Ï§ë..."):
        process_keywords_parallel(
            sorted(keyword_list),  # ‚úÖ Ïò§Î•ò Î∞úÏÉù Ïïà Ìï®
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )
    st.session_state.search_triggered = False

if category_search_clicked and selected_categories:
    with st.spinner("Îâ¥Ïä§ Í≤ÄÏÉâ Ï§ë..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords_parallel(
            sorted(keywords),  # ‚úÖ ÏàòÏ†ï: keyword_list ‚Üí sorted(keywords)
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()

            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(re.sub("<.*?>", "", title), EXCLUDE_TITLE_KEYWORDS):
                continue

            source = item.get("source")
            if not source or source.strip() == "":
                source = infer_source_from_url(item.get("originallink", ""))
                if not source:
                    source = "Naver"
            source_domain = source.lower()
            if source_domain.startswith("www."):
                source_domain = source_domain[4:]

            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": source_domain
            })

        if len(items) < 100:
            break
    return articles[:limit]

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "Ï†úÎ™©ÏóÜÏùå"
    return str(val)

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories):
    company_order = []
    for cat in [
        "Íµ≠/Í≥µÏ±Ñ", "Í≥µÍ≥µÍ∏∞Í¥Ä", "Î≥¥ÌóòÏÇ¨", "5ÎåÄÍ∏àÏúµÏßÄÏ£º", "5ÎåÄÏãúÏ§ëÏùÄÌñâ", "Ïπ¥ÎìúÏÇ¨", "Ï∫êÌîºÌÉà",
        "ÏßÄÏ£ºÏÇ¨", "ÏóêÎÑàÏßÄ", "Î∞úÏ†Ñ", "ÏûêÎèôÏ∞®", "Ï†ÑÍ∏∞/Ï†ÑÏûê", "ÏÜåÎπÑÏû¨", "ÎπÑÏ≤†/Ï≤†Í∞ï", "ÏÑùÏú†ÌôîÌïô", "Í±¥ÏÑ§", "ÌäπÏàòÏ±Ñ"
    ]:
        company_order.extend(favorite_categories.get(cat, []))
    excel_company_order = []
    for cat in [
        "Íµ≠/Í≥µÏ±Ñ", "Í≥µÍ≥µÍ∏∞Í¥Ä", "Î≥¥ÌóòÏÇ¨", "5ÎåÄÍ∏àÏúµÏßÄÏ£º", "5ÎåÄÏãúÏ§ëÏùÄÌñâ", "Ïπ¥ÎìúÏÇ¨", "Ï∫êÌîºÌÉà",
        "ÏßÄÏ£ºÏÇ¨", "ÏóêÎÑàÏßÄ", "Î∞úÏ†Ñ", "ÏûêÎèôÏ∞®", "Ï†ÑÍ∏∞/Ï†ÑÏûê", "ÏÜåÎπÑÏû¨", "ÎπÑÏ≤†/Ï≤†Í∞ï", "ÏÑùÏú†ÌôîÌïô", "Í±¥ÏÑ§", "ÌäπÏàòÏ±Ñ"
    ]:
        excel_company_order.extend(excel_company_categories.get(cat, []))

    df_articles = pd.DataFrame(summary_data)

    # ‚úÖ Î≥¥Ìò∏ Î°úÏßÅ: 'ÌÇ§ÏõåÎìú' Ïª¨ÎüºÏù¥ ÏóÜÏúºÎ©¥ Îπà ÏóëÏÖÄ Î∞òÌôò
    if "ÌÇ§ÏõåÎìú" not in df_articles.columns:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            pd.DataFrame(columns=["Í∏∞ÏóÖÎ™Ö", "ÌëúÍ∏∞Î™Ö", "Í∏çÏ†ï Îâ¥Ïä§", "Î∂ÄÏ†ï Îâ¥Ïä§"]).to_excel(writer, index=False)
        output.seek(0)
        return output

    result_rows = []
    for idx, company in enumerate(company_order):
        excel_company_name = excel_company_order[idx] if idx < len(excel_company_order) else ""
        comp_articles = df_articles[df_articles["ÌÇ§ÏõåÎìú"] == company]
        pos_news = comp_articles[comp_articles["Í∞êÏÑ±"] == "Í∏çÏ†ï"].sort_values(by="ÎÇ†Ïßú", ascending=False)
        neg_news = comp_articles[comp_articles["Í∞êÏÑ±"] == "Î∂ÄÏ†ï"].sort_values(by="ÎÇ†Ïßú", ascending=False)

        if not pos_news.empty:
            pos_date = pos_news.iloc[0]["ÎÇ†Ïßú"]
            pos_title = pos_news.iloc[0]["Í∏∞ÏÇ¨Ï†úÎ™©"]
            pos_link = pos_news.iloc[0]["ÎßÅÌÅ¨"]
            pos_display = f'({pos_date}) {pos_title}'
            pos_hyperlink = f'=HYPERLINK("{pos_link}", "{pos_display}")'
        else:
            pos_hyperlink = ""

        if not neg_news.empty:
            neg_date = neg_news.iloc[0]["ÎÇ†Ïßú"]
            neg_title = neg_news.iloc[0]["Í∏∞ÏÇ¨Ï†úÎ™©"]
            neg_link = neg_news.iloc[0]["ÎßÅÌÅ¨"]
            neg_display = f'({neg_date}) {neg_title}'
            neg_hyperlink = f'=HYPERLINK("{neg_link}", "{neg_display}")'
        else:
            neg_hyperlink = ""

        result_rows.append({
            "Í∏∞ÏóÖÎ™Ö": company,
            "ÌëúÍ∏∞Î™Ö": excel_company_name,
            "Í∏çÏ†ï Îâ¥Ïä§": pos_hyperlink,
            "Î∂ÄÏ†ï Îâ¥Ïä§": neg_hyperlink
        })

    df_result = pd.DataFrame(result_rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_result.to_excel(writer, index=False, sheet_name='Îâ¥Ïä§ÏöîÏïΩ')
    output.seek(0)
    return output

def build_important_excel_same_format(important_articles, favorite_categories, excel_company_categories):
    """
    Ï§ëÏöîÍ∏∞ÏÇ¨ ÏûêÎèô Ï∂îÏ∂ú Í≤∞Í≥ºÎ•º Í∏∞Ï°¥ 'ÎßûÏ∂§ ÏóëÏÖÄ ÏñëÏãù'Í≥º ÎèôÏùºÌïú Ìè¨Îß∑ÏúºÎ°ú Ï†ÄÏû•
    """
    company_order = []
    excel_company_order = []

    for cat in [
        "Íµ≠/Í≥µÏ±Ñ", "Í≥µÍ≥µÍ∏∞Í¥Ä", "Î≥¥ÌóòÏÇ¨", "5ÎåÄÍ∏àÏúµÏßÄÏ£º", "5ÎåÄÏãúÏ§ëÏùÄÌñâ", "Ïπ¥ÎìúÏÇ¨", "Ï∫êÌîºÌÉà",
        "ÏßÄÏ£ºÏÇ¨", "ÏóêÎÑàÏßÄ", "Î∞úÏ†Ñ", "ÏûêÎèôÏ∞®", "Ï†ÑÍ∏∞/Ï†ÑÏûê", "ÏÜåÎπÑÏû¨", "ÎπÑÏ≤†/Ï≤†Í∞ï", "ÏÑùÏú†ÌôîÌïô", "Í±¥ÏÑ§", "ÌäπÏàòÏ±Ñ"
    ]:
        company_order.extend(favorite_categories.get(cat, []))
        excel_company_order.extend(excel_company_categories.get(cat, []))

    # Í∏∞ÏóÖÎ≥Ñ Í∏∞ÏÇ¨ Ï†ïÎ¶¨
    rows = []
    for i, comp in enumerate(company_order):
        display_name = excel_company_order[i] if i < len(excel_company_order) else ""
        pos_article = ""
        neg_article = ""

        # Ïù¥ Í∏∞ÏóÖÏóê Ìï¥ÎãπÌïòÎäî Í∏∞ÏÇ¨Îì§ ÌïÑÌÑ∞ÎßÅ
        articles = [a for a in important_articles if a["ÌöåÏÇ¨Î™Ö"] == comp]

        for article in articles:
            link = article["ÎßÅÌÅ¨"]
            title = article["Ï†úÎ™©"]
            date = article["ÎÇ†Ïßú"]
            display_text = f"({date}) {title}"
            hyperlink = f'=HYPERLINK("{link}", "{display_text}")'

            if article["Í∞êÏÑ±"] == "Í∏çÏ†ï":
                pos_article = hyperlink
            elif article["Í∞êÏÑ±"] == "Î∂ÄÏ†ï":
                neg_article = hyperlink

        rows.append({
            "Í∏∞ÏóÖÎ™Ö": comp,
            "ÌëúÍ∏∞Î™Ö": display_name,
            "Í∏çÏ†ï Îâ¥Ïä§": pos_article,
            "Î∂ÄÏ†ï Îâ¥Ïä§": neg_article
        })

    df = pd.DataFrame(rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="Ï§ëÏöîÎâ¥Ïä§_ÏñëÏãù")
    output.seek(0)
    return output

def generate_important_article_list(search_results, common_keywords, industry_keywords, favorites):
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    client = OpenAI(api_key=OPENAI_API_KEY)
    result = []

    for category, companies in favorites.items():
        for comp in companies:
            articles = search_results.get(comp, [])
            filtered_keywords = list(set(common_keywords + industry_keywords))
            target_articles = [a for a in articles if any(kw in a["title"] for kw in filtered_keywords)]

            if not target_articles:
                continue

            prompt_list = "\n".join([f"{i+1}. {a['title']} - {a['link']}" for i, a in enumerate(target_articles)])
            prompt = (
                f"[ÌïÑÌÑ∞ ÌÇ§ÏõåÎìú]\n{', '.join(filtered_keywords)}\n\n"
                f"[Í∏∞ÏÇ¨ Î™©Î°ù]\n{prompt_list}\n\n"
                "Í∞Å Í∏∞ÏÇ¨Ïóê ÎåÄÌï¥ Í∞êÏÑ±(Í∏çÏ†ï/Î∂ÄÏ†ï)ÏùÑ ÌåêÎã®ÌïòÍ≥†,\n"
                "Ï†úÎ™©Ïóê ÌïÑÌÑ∞ ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú Îâ¥Ïä§Îßå Í∏∞Ï§ÄÏúºÎ°ú,\n"
                "- Í∏çÏ†ïÏóêÏÑú Í∞ÄÏû• Ï§ëÏöîÌïú Îâ¥Ïä§ 1Í±¥\n"
                "- Î∂ÄÏ†ïÏóêÏÑú Í∞ÄÏû• Ï§ëÏöîÌïú Îâ¥Ïä§ 1Í±¥\n"
                "ÏùÑ Í≥®ÎùºÏ£ºÏÑ∏Ïöî. ÏóÜÏúºÎ©¥ ÎπàÏπ∏ÏúºÎ°ú:\n\n"
                "[Í∏çÏ†ï]: (Îâ¥Ïä§ Ï†úÎ™©)\n[Î∂ÄÏ†ï]: (Îâ¥Ïä§ Ï†úÎ™©)"
            )

            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=800,
                    temperature=0.3
                )
                answer = response.choices[0].message.content.strip()
                import re
                pos_title = re.search(r"\[Í∏çÏ†ï\]:\s*(.+)", answer)
                neg_title = re.search(r"\[Î∂ÄÏ†ï\]:\s*(.+)", answer)
                pos_title = pos_title.group(1).strip() if pos_title else ""
                neg_title = neg_title.group(1).strip() if neg_title else ""

                for a in target_articles:
                    if pos_title and pos_title in a["title"]:
                        result.append({
                            "ÌöåÏÇ¨Î™Ö": comp,
                            "Í∞êÏÑ±": "Í∏çÏ†ï",
                            "Ï†úÎ™©": a["title"],
                            "ÎßÅÌÅ¨": a["link"],
                            "ÎÇ†Ïßú": a["date"],
                            "Ï∂úÏ≤ò": a["source"]
                        })
                    if neg_title and neg_title in a["title"]:
                        result.append({
                            "ÌöåÏÇ¨Î™Ö": comp,
                            "Í∞êÏÑ±": "Î∂ÄÏ†ï",
                            "Ï†úÎ™©": a["title"],
                            "ÎßÅÌÅ¨": a["link"],
                            "ÎÇ†Ïßú": a["date"],
                            "Ï∂úÏ≤ò": a["source"]
                        })
            except Exception:
                continue
    return result

def extract_keyword_from_link(search_results, article_link):
    for keyword, articles in search_results.items():
        for article in articles:
            if article["link"] == article_link:
                return keyword
    return "ÏïåÏàòÏóÜÏùå"

def render_important_article_review_and_download():
    with st.container(border=True):
        st.markdown("### ‚≠ê Ï§ëÏöî Í∏∞ÏÇ¨ Î¶¨Î∑∞ Î∞è Ìé∏Ïßë")
        
        # Ï§ëÏöîÍ∏∞ÏÇ¨ ÏûêÎèô ÏÑ†Ï†ï Î≤ÑÌäº
        if st.button("üöÄ OpenAI Í∏∞Î∞ò Ï§ëÏöî Í∏∞ÏÇ¨ ÏûêÎèô ÏÑ†Ï†ï"):
            with st.spinner("OpenAIÎ°ú Ï§ëÏöî Îâ¥Ïä§ ÏÑ†Ï†ï Ï§ë..."):
                important_articles = generate_important_article_list(
                    search_results=st.session_state.search_results,
                    common_keywords=ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords=st.session_state.get("industry_sub", []),
                    favorites=favorite_categories
                )
                st.session_state.important_articles_preview = important_articles
                st.session_state.important_selected_index = []
        
        # Ï§ëÏöîÍ∏∞ÏÇ¨ Î¶¨Ïä§Ìä∏ ÏóÜÏúºÎ©¥ ÏïàÎÇ¥ Î©îÏãúÏßÄ
        if not st.session_state.get("important_articles_preview"):
            st.info("ÏïÑÏßÅ Ï§ëÏöî Í∏∞ÏÇ¨ ÌõÑÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§. ÏúÑ Î≤ÑÌäºÏùÑ ÎàåÎü¨ ÏûêÎèô ÏÉùÏÑ±ÌïòÏã≠ÏãúÏò§.")
            return
        
        st.markdown("üéØ **Ï§ëÏöî Í∏∞ÏÇ¨ Î™©Î°ù** (ÍµêÏ≤¥ ÎòêÎäî ÏÇ≠Ï†úÌï† Ìï≠Î™©ÏùÑ Ï≤¥ÌÅ¨ÌïòÏÑ∏Ïöî)")
        
        # Ï§ëÏöîÍ∏∞ÏÇ¨ Ï≤¥ÌÅ¨Î∞ïÏä§ Î¶¨Ïä§Ìä∏
        new_selection = []
        for idx, article in enumerate(st.session_state["important_articles_preview"]):
            checked = st.checkbox(
                f"{article['ÌöåÏÇ¨Î™Ö']} | {article['Í∞êÏÑ±']} | {article['Ï†úÎ™©']}",
                key=f"important_chk_{idx}",
                value=(idx in st.session_state.important_selected_index), on_change=None
            )
            if checked:
                new_selection.append(idx)
        st.session_state.important_selected_index = new_selection

        # --- Ï∂îÍ∞Ä Î≤ÑÌäº, ÏÇ≠Ï†ú Î≤ÑÌäº, ÍµêÏ≤¥ Î≤ÑÌäº Ìïú Ï§ÑÏóê Î∞∞Ïπò ---
        col_add, col_del, col_rep = st.columns([0.3, 0.35, 0.35])

        with col_add:
            if st.button("‚ûï ÏÑ†ÌÉù Í∏∞ÏÇ¨ Ï∂îÍ∞Ä"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                if len(left_selected_keys) != 1:
                    st.warning("ÏôºÏ™Ω Îâ¥Ïä§Í≤ÄÏÉâ Í≤∞Í≥ºÏóêÏÑú Í∏∞ÏÇ¨ 1Í∞úÎßå ÏÑ†ÌÉùÌï¥ Ï£ºÏÑ∏Ïöî.")
                else:
                    from_key = left_selected_keys[0]
                    # --- Ïú†ÎãàÌÅ¨IDÎ°ú Í∏∞ÏÇ¨ ÌÉêÏÉâ ---
                    m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                    if not m:
                        st.warning("Í∏∞ÏÇ¨ ÏãùÎ≥ÑÏûê ÌååÏã± Ïã§Ìå®")
                        return
                    key_tail = m.group(1)
                    selected_article = None
                    article_link = None
                    for kw, arts in st.session_state.search_results.items():
                        for art in arts:
                            uid = re.sub(r'\W+', '', art['link'])[-16:]
                            if uid == key_tail:
                                selected_article = art
                                article_link = art["link"]
                                break
                        if selected_article: break

                    if not selected_article or not article_link:
                        st.warning("ÏÑ†ÌÉùÌïú Í∏∞ÏÇ¨ Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                        return

                    # üî∑ ÌöåÏÇ¨Î™ÖÏùÑ Ìï≠ÏÉÅ extract_keyword_from_linkÎ°ú Ï∞æÏùå!
                    keyword = extract_keyword_from_link(st.session_state.search_results, article_link)

                    # Í∞êÏÑ± Ï†ïÎ≥¥ ÌôïÏù∏ ÎòêÎäî ÏöîÏïΩ/Í∞êÏÑ± Îã§Ïãú ÏÉùÏÑ±
                    cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                    sentiment = None
                    for k in st.session_state.keys():
                        if k.startswith("summary_") and cleaned_id in k:
                            sentiment = st.session_state[k][2]
                            break
                    if not sentiment:
                        _, _, sentiment, _ = summarize_article_from_url(selected_article["link"], selected_article["title"])

                    new_article = {
                        "ÌöåÏÇ¨Î™Ö": keyword,
                        "Í∞êÏÑ±": sentiment or "",
                        "Ï†úÎ™©": selected_article["title"],
                        "ÎßÅÌÅ¨": selected_article["link"],
                        "ÎÇ†Ïßú": selected_article["date"],
                        "Ï∂úÏ≤ò": selected_article["source"]
                    }

                    important = st.session_state.get("important_articles_preview", [])
                    if any(a["ÎßÅÌÅ¨"] == new_article["ÎßÅÌÅ¨"] for a in important):
                        st.info("Ïù¥ÎØ∏ Ï§ëÏöî Í∏∞ÏÇ¨ Î™©Î°ùÏóê Ï°¥Ïû¨ÌïòÎäî Í∏∞ÏÇ¨ÏûÖÎãàÎã§.")
                    else:
                        important.append(new_article)
                        st.session_state["important_articles_preview"] = important
                        st.success("Ï§ëÏöî Í∏∞ÏÇ¨ Î™©Î°ùÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§: " + new_article["Ï†úÎ™©"])
                        st.rerun()

        with col_del:
            if st.button("üóë ÏÑ†ÌÉù Í∏∞ÏÇ¨ ÏÇ≠Ï†ú"):
                for idx in sorted(st.session_state.important_selected_index, reverse=True):
                    if 0 <= idx < len(st.session_state["important_articles_preview"]):
                        st.session_state["important_articles_preview"].pop(idx)
                st.session_state.important_selected_index = []
                st.rerun()

        with col_rep:
            if st.button("üîÅ ÏÑ†ÌÉù Í∏∞ÏÇ¨ ÍµêÏ≤¥"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                right_selected_indexes = st.session_state.important_selected_index
                if len(left_selected_keys) != 1 or len(right_selected_indexes) != 1:
                    st.warning("ÏôºÏ™ΩÏóêÏÑú Í∏∞ÏÇ¨ 1Í∞ú, Ïò§Î•∏Ï™ΩÏóêÏÑú Í∏∞ÏÇ¨ 1Í∞úÎßå ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî.")
                    return

                from_key = left_selected_keys[0]
                target_idx = right_selected_indexes[0]
                m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                if not m:
                    st.warning("Í∏∞ÏÇ¨ ÏãùÎ≥ÑÏûê ÌååÏã± Ïã§Ìå®")
                    return
                key_tail = m.group(1)
                selected_article = None
                article_link = None
                for kw, art_list in st.session_state.search_results.items():
                    for art in art_list:
                        uid = re.sub(r'\W+', '', art['link'])[-16:]
                        if uid == key_tail:
                            selected_article = art
                            article_link = art["link"]
                            break
                    if selected_article:
                        break

                if not selected_article or not article_link:
                    st.warning("ÏôºÏ™ΩÏóêÏÑú ÏÑ†ÌÉùÌïú Í∏∞ÏÇ¨ Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                    return

                # üî∑ ÌöåÏÇ¨Î™Ö(ÌÇ§ÏõåÎìú)ÏùÄ extract_keyword_from_linkÎ°ú Ï†ïÌôïÌûà Í≤∞Ï†ï
                keyword = extract_keyword_from_link(st.session_state.search_results, article_link)
                cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                sentiment = None
                for k in st.session_state.keys():
                    if k.startswith("summary_") and cleaned_id in k:
                        sentiment = st.session_state[k][2]
                        break
                if not sentiment:
                    _, _, sentiment, _ = summarize_article_from_url(selected_article["link"], selected_article["title"])

                new_article = {
                    "ÌöåÏÇ¨Î™Ö": keyword,
                    "Í∞êÏÑ±": sentiment or "",
                    "Ï†úÎ™©": selected_article["title"],
                    "ÎßÅÌÅ¨": selected_article["link"],
                    "ÎÇ†Ïßú": selected_article["date"],
                    "Ï∂úÏ≤ò": selected_article["source"]
                }
                st.session_state["important_articles_preview"][target_idx] = new_article
                st.session_state.article_checked_left[from_key] = False
                st.session_state.article_checked[from_key] = False
                st.session_state.important_selected_index = []
                st.success("Ï§ëÏöî Í∏∞ÏÇ¨ ÍµêÏ≤¥ ÏôÑÎ£å: " + new_article["Ï†úÎ™©"])
                st.rerun()

        st.markdown("---")
        st.markdown("üì• **Î¶¨Î∑∞Ìïú Ï§ëÏöî Í∏∞ÏÇ¨Îì§ÏùÑ ÏóëÏÖÄÎ°ú Îã§Ïö¥Î°úÎìúÌïòÏÑ∏Ïöî.**")
        output_excel = build_important_excel_same_format(
            st.session_state["important_articles_preview"],
            favorite_categories,
            excel_company_categories
        )
        st.download_button(
            label="üì• Ï§ëÏöî Í∏∞ÏÇ¨ ÏµúÏ¢Ö ÏóëÏÖÄ Îã§Ïö¥Î°úÎìú (ÎßûÏ∂§ ÏñëÏãù)",
            data=output_excel.getvalue(),
            file_name="Ï§ëÏöîÎâ¥Ïä§_ÏµúÏ¢ÖÏÑ†Ï†ï_ÏñëÏãù.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True, enable_summary=True):
    SENTIMENT_CLASS = {
        "Í∏çÏ†ï": "sentiment-positive",
        "Î∂ÄÏ†ï": "sentiment-negative"
    }

    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### üîç Îâ¥Ïä§ Í≤ÄÏÉâ Í≤∞Í≥º")
        for keyword, articles in results.items():
            with st.container(border=True):
                st.markdown(f"**[{keyword}] ({len(articles)}Í±¥)**")

                for idx, article in enumerate(articles):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    cache_key = f"summary_{key}"

                    # Ï≤¥ÌÅ¨Î∞ïÏä§ÏôÄ Ï†úÎ™© Î†åÎçîÎßÅ
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                    with cols[1]:
                        sentiment = ""
                        if show_sentiment_badge and cache_key in st.session_state:
                            _, _, sentiment, _ = st.session_state[cache_key]
                        badge_html = f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>({sentiment})</span>" if sentiment else ""
                        st.markdown(f"[{article['title']}]({article['link']}) {badge_html} {article['date']} | {article['source']}", unsafe_allow_html=True)

                    st.session_state.article_checked_left[key] = checked
                    if checked:
                        st.session_state.article_checked[key] = True

    # ÏÑ†ÌÉù Í∏∞ÏÇ¨ ÏöîÏïΩ Î∞è Îã§Ïö¥Î°úÎìú
    with col_summary:
        st.markdown("### ÏÑ†ÌÉùÎêú Í∏∞ÏÇ¨ ÏöîÏïΩ/Í∞êÏÑ±Î∂ÑÏÑù")
        with st.container(border=True):
            selected_articles = []

            for keyword, articles in results.items():
                for idx, article in enumerate(articles):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    cache_key = f"summary_{key}"

                    if st.session_state.article_checked.get(key, False):
                        if cache_key in st.session_state:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        else:
                            one_line, summary, sentiment, full_text = summarize_article_from_url(
                                article['link'], article['title'], do_summary=enable_summary
                            )
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)

                        selected_articles.append({
                            "ÌÇ§ÏõåÎìú": keyword,
                            "Í∏∞ÏÇ¨Ï†úÎ™©": safe_title(article['title']),
                            "ÏöîÏïΩ": one_line,
                            "ÏöîÏïΩÎ≥∏": summary,
                            "Í∞êÏÑ±": sentiment,
                            "ÎßÅÌÅ¨": article['link'],
                            "ÎÇ†Ïßú": article['date'],
                            "Ï∂úÏ≤ò": article['source']
                        })

                        st.markdown(
                            f"#### [{article['title']}]({article['link']}) "
                            f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>({sentiment})</span>",
                            unsafe_allow_html=True
                        )
                        st.markdown(f"- **ÎÇ†Ïßú/Ï∂úÏ≤ò:** {article['date']} | {article['source']}")
                        if enable_summary:
                            st.markdown(f"- **Ìïú Ï§Ñ ÏöîÏïΩ:** {one_line}")
                        st.markdown(f"- **Í∞êÏÑ±Î∂ÑÏÑù:** `{sentiment}`")
                        st.markdown("---")

            st.session_state.selected_articles = selected_articles
            st.write(f"ÏÑ†ÌÉùÎêú Í∏∞ÏÇ¨ Í∞úÏàò: {len(selected_articles)}")

            col_dl1, col_dl2 = st.columns([0.5, 0.5])
            with col_dl1:
                st.download_button(
                    label="üì• ÎßûÏ∂§ ÏóëÏÖÄ Îã§Ïö¥Î°úÎìú",
                    data=get_excel_download_with_favorite_and_excel_company_col(
                        st.session_state.selected_articles,
                        favorite_categories,
                        excel_company_categories
                    ).getvalue(),
                    file_name="Îâ¥Ïä§ÏöîÏïΩ_ÎßûÏ∂§Ìòï.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

        # Ï§ëÏöî Í∏∞ÏÇ¨ Î¶¨Î∑∞ UI
        render_important_article_review_and_download()

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        
        # --- Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ï†úÍ±∞ Ï≤òÎ¶¨ ---
        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)
        
        if filtered_articles:
            filtered_results[keyword] = filtered_articles

    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )
