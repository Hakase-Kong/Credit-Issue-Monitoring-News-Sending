import nltk

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
import os
from datetime import datetime
import telepot
from openai import OpenAI
import newspaper  # newspaper4k

# --- CSS: ì²´í¬ë°•ìŠ¤ì™€ ê¸°ì‚¬ ì‚¬ì´ gap ìµœì†Œí™” ë° ê°ì„± ë±ƒì§€ ìŠ¤íƒ€ì¼, flex row ë²„íŠ¼ í•˜ë‹¨ì •ë ¬ ---
st.markdown("""
<style>
[data-testid="column"] > div {
    gap: 0rem !important;
}
.stMultiSelect [data-baseweb="tag"] {
    background-color: #ff5c5c !important;
    color: white !important;
    border: none !important;
    font-weight: bold;
}
.sentiment-badge {
    display: inline-block;
    padding: 0.08em 0.6em;
    margin-left: 0.2em;
    border-radius: 0.8em;
    font-size: 0.85em;
    font-weight: bold;
    vertical-align: middle;
}
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-neutral { background: #0074d9; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.stBox {
    background: #fcfcfc;
    border-radius: 0.7em;
    border: 1.5px solid #e0e2e6;
    margin-bottom: 1.2em;
    padding: 1.1em 1.2em 1.2em 1.2em;
    box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03);
}
.flex-row-bottom {
    display: flex;
    align-items: flex-end;
    gap: 0.5rem;
    margin-bottom: 0.5rem;
}
.flex-grow {
    flex: 1 1 0%;
}
.flex-btn {
    min-width: 90px;
}
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False

# --- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬(ë³€ê²½ ê¸ˆì§€) ---
favorite_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": ["í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ"],
    "5ëŒ€ì‹œì¤‘ì€í–‰": ["ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"],
    "ì¹´ë“œì‚¬": ["KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"],
    "ìºí”¼íƒˆ": ["í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": ["SKì´ë…¸ë² ì´ì…˜", "GSì—ë„ˆì§€", "SK", "GS"],
    "ì—ë„ˆì§€": ["SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "SKì•¤ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"],
    "ë°œì „": ["GSíŒŒì›Œ", "GSEPS", "ì‚¼ì²œë¦¬"],
    "ìë™ì°¨": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´"],
    "ì „ê¸°/ì „ì": ["SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "LSì¼ë ‰íŠ¸ë¦­"],
    "ì†Œë¹„ì¬": ["ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"],
    "ë¹„ì² /ì² ê°•": ["í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"],
    "ì„ìœ í™”í•™": ["LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"],
    "ê±´ì„¤": ["í¬ìŠ¤ì½”ì´ì•¤ì”¨"],
    "íŠ¹ìˆ˜ì±„": ["ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"]
}

# --- ê¸°ì—…ë³„ í•„í„° ì˜µì…˜: ê¸°ì—…ëª…(ë¶„ë¥˜) - í‚¤ì›Œë“œ(ì†Œë¶„ë¥˜) ---
company_filter_categories = {
    "í˜„ëŒ€í•´ìƒ": [],
    "ë†í˜‘ìƒëª…": [],
    "ë©”ë¦¬ì¸ í™”ì¬": ["ë¶€ë™ì‚°PF"],
    "êµë³´ìƒëª…": [],
    "ì‚¼ì„±í™”ì¬": [],
    "ì‚¼ì„±ìƒëª…": [],
    "ì‹ í•œë¼ì´í”„ìƒëª…ë³´í—˜": [],
    "í¥êµ­ìƒëª…ë³´í—˜": ["íƒœê´‘ê·¸ë£¹"],
    "ë™ì–‘ìƒëª…": ["ë‹¤ìë³´í—˜", "ì•ˆë°©ê·¸ë£¹", "ìš°ë¦¬ê¸ˆìœµ"],
    "ë¯¸ë˜ì—ì…‹ìƒëª…": [],
    "KBêµ­ë¯¼ì¹´ë“œ": [],
    "í˜„ëŒ€ì¹´ë“œ": ["PLCC", "ì¹´ë“œëŒ€ì¶œìì‚° ì·¨ê¸‰í™•ëŒ€"],
    "ì‹ í•œì¹´ë“œ": [],
    "ë¹„ì”¨ì¹´ë“œ": ["íšŒì›ì‚¬ ì´íƒˆ", "IPO", "ì¼€ì´ë±…í¬"],
    "ì‚¼ì„±ì¹´ë“œ": [],
    "í•œêµ­ìºí”¼íƒˆ": ["êµ°ì¸ê³µì œíšŒ"],
    "í˜„ëŒ€ìºí”¼íƒˆ": ["ìë™ì°¨ê¸ˆìœµ"],
    "SKì´ë…¸ë² ì´ì…˜": ["SKì§€ì˜¤ì„¼íŠ¸ë¦­", "SKì—ë„ˆì§€", "SKì—”ë¬´ë¸Œ", "SKì¸ì²œì„ìœ í™”í•™", "2ì°¨ì „ì§€", "ì„ìœ í™”í•™", "ìœ¤í™œìœ ", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬"],
    "GSì—ë„ˆì§€": ["GSì¹¼í…ìŠ¤", "GSíŒŒì›Œ", "ì •ìœ ", "ì—´ë³‘í•© ìˆ˜ìš”"],
    "SK": ["SKì´ë…¸ë² ì´ì…˜", "SKí…”ë ˆì½¤", "SKì˜¨", "ë°°í„°ë¦¬", "ì„ìœ í™”í•™", "ì´ë™í†µì‹ "],
    "GS": ["GSì—ë„ˆì§€", "GSë¦¬í…Œì¼", "GS E&C", "ì •ìœ ", "ê±´ì„¤", "ìœ í†µ"],
    "SKê°€ìŠ¤": ["í”„ë¡œí•„ë Œ", "LPG íŒŒìƒìƒí’ˆ", "í„°ë¯¸ë„"],
    "GSì¹¼í…ìŠ¤": ["GSì—ë„ˆì§€", "PXìŠ¤í”„ë ˆë“œ", "ìœ¤í™œê¸°ìœ ", "ì €íƒ„ì†Œ ì‚°ì—…"],
    "S-Oil": ["PXìŠ¤í”„ë ˆë“œ", "ìœ¤í™œê¸°ìœ ", "Sheheen", "saudi aramco"],
    "SKì—ë„ˆì§€": [],
    "SKì•¤ë¬´ë¸Œ": ["SKì´ë…¸ë² ì´ì…˜", "ìœ¤í™œìœ ", "ê¸°ìœ  ìŠ¤í”„ë ˆë“œ", "ë¯¸ì „í™˜ìœ ", "ì•¡ì¹¨ëƒ‰ê°"],
    "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„": ["í„°ë¯¸ë„", "ê°€ë™ë¥ ", "LNG í„°ë¯¸ë„ ìˆ˜ìš”", "ì—ë„ˆì§€ ì „í™˜ ì •ì±…"],
    "GSíŒŒì›Œ": ["GS", "ê°€ë™ë¥ ", "ì¦ì„¤", "ì—´ë³‘í•© ìˆ˜ìš”"],
    "GSEPS": ["GS", "ê°€ë™ë¥ ", "ë°”ì´ì˜¤ë§¤ìŠ¤"],
    "ì‚¼ì²œë¦¬": ["ë„ì‹œê°€ìŠ¤", "ê³„ì—´ ë¶„ë¦¬", "KOGAS ì¡°ë‹¬ë‹¨ê°€"],
    "LGì—ë„ˆì§€ì†”ë£¨ì…˜": ["ì¤‘êµ­ì‚° ë°°í„°ë¦¬ ê·œì œ", "ë¦¬íŠ¬"],
    "í•œì˜¨ì‹œìŠ¤í…œ": ["í•œì•¤ì»´í¼ë‹ˆ", "HVAC", "íƒ„ì†Œì¤‘ë¦½ì •ì±…"],
    "í¬ìŠ¤ì½”í“¨ì²˜ì— ": ["ë¦¬íŠ¬", "ì–‘ê·¹ì¬", "ìŒê·¹ì¬"],
    "í•œêµ­íƒ€ì´ì–´": ["EV íƒ€ì´ì–´", "ì „ê¸°ì°¨ íƒ€ì´ì–´", "í•©ì„±ê³ ë¬´ ê°€ê²©"],
    "SKí•˜ì´ë‹‰ìŠ¤": ["DRAM", "HBM"],
    "LGì´ë…¸í…": ["ìŠ¤ë§ˆíŠ¸í° íŒë§¤", "ì•„ì´í° íŒë§¤", "ìŠ¤ë§ˆíŠ¸í°", "ì•„ì´í°", "ê´‘í•™ì†”ë£¨ì…˜", "ì¤‘êµ­ ì¹´ë©”ë¼ ëª¨ë“ˆ", "ToFì¹´ë©”ë¼"],
    "LGì „ì": ["ë³´í¸ê´€ì„¸", "TV ìˆ˜ìš”", "LCD ê°€ê²©", "ì „ì¥ ìˆ˜ì£¼ì”ê³ ", "HVAC", "SCFIì»¨í…Œì´ë„ˆ ì§€ìˆ˜"],
    "LSì¼ë ‰íŠ¸ë¦­": ["HVTRìˆ˜ìš”", "ë¯¸êµ­ ì „ë ¥ ìˆ˜ìš”", "ì¦ì„¤", "PLC ê²½ìŸ"],
    "ì´ë§ˆíŠ¸": ["ì‹ ì„¸ê³„", "ëŒ€í˜•ë§ˆíŠ¸ ì˜ë¬´íœ´ì—…", "ì‹ ì„¸ê³„ê±´ì„¤", "Gë§ˆì¼“", "Wì»¨ì…‰", "ìŠ¤íƒ€í•„ë“œ"],
    "LF": ["ì˜ë¥˜ì‹œì¥", "ì½”ëŒì½”ìì‚°ì‹ íƒ"],
    "CJì œì¼ì œë‹¹": ["HMR", "ë¼ì´ì‹ ", "ì•„ë¯¸ë…¸ì‚°", "ìŠˆì™„ìŠ¤ì»´í¼ë‹ˆ"],
    "SKë„¤íŠ¸ì›ìŠ¤": ["SKí…”ë ˆì½¤", "SKë§¤ì§"],
    "CJëŒ€í•œí†µìš´": ["ì¿ íŒ¡", "CLS", "ì£¼ 7ì¼ ë°°ì†¡"],
    "í¬ìŠ¤ì½”": [],
    "í˜„ëŒ€ì œì² ": ["ë…¸ì‚¬ê°ˆë“±"],
    "ê³ ë ¤ì•„ì—°": ["ì—°", "ì•„ì—°", "ë‹ˆì¼ˆ", "ì•ˆí‹°ëª¨ë‹ˆ", "ì œë ¨", "ê²½ì˜ê¶Œ ë¶„ìŸ", "MBK", "ì˜í’", "ì¤‘êµ­ ì•„ì—° ê°ì‚°", "ì¤‘êµ­ ìˆ˜ì¶œ ê·œì œ", "ì¬ê³ í‰ê°€ì†ìµ"],
    "LGí™”í•™": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ë¶ë¯¸ ì ìœ ìœ¨", "ìœ ëŸ½ ë°°í„°ë¦¬ ì‹œì¥", "ë¦¬íŠ¬", "IRA", "AMPC", "EV ìˆ˜ìš”", "ESS ìˆ˜ìš”"],
    "SKì§€ì˜¤ì„¼íŠ¸ë¦­": ["SKì´ë…¸ë² ì´ì…˜"],
    "í¬ìŠ¤ì½”ì´ì•¤ì”¨": ["ì‹ ì•ˆì‚°ì„ "],
    "ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬(ì‹ ì¢…)": ["HUG", "ì „ì„¸ì‚¬ê¸°", "ë³´ì¦ì‚¬ê³ ", "ë³´ì¦ë£Œìœ¨", "íšŒìˆ˜ìœ¨", "ë³´ì¦ì”ì•¡", "ëŒ€ìœ„ë³€ì œì•¡"],
    "ê¸°ì—…ì€í–‰(í›„)": ["ì¤‘ì†Œê¸°ì—…ëŒ€ì¶œ", "ê³µê³µê¸°ê´€ í•´ì œ", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ë¶ˆë²•", "êµ¬ì†"]
}
company_major_categories = list(company_filter_categories.keys())
company_sub_categories = {cat: company_filter_categories[cat] for cat in company_major_categories}

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜: ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ ìµœì‹ í™” ---
industry_filter_categories = {
    "ì€í–‰ ë° ê¸ˆìœµì§€ì£¼": [
        "ê²½ì˜ì‹¤íƒœí‰ê°€", "BIS", "CET1", "ìë³¸ë¹„ìœ¨", "ìƒê°í˜• ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œ", "ìë³¸í™•ì¶©", "ìë³¸ì—¬ë ¥", "ìë³¸ì ì •ì„±", "LCR",
        "ì¡°ë‹¬ê¸ˆë¦¬", "NIM", "ìˆœì´ìë§ˆì§„", "ê³ ì •ì´í•˜ì—¬ì‹ ë¹„ìœ¨", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ì—°ì²´ìœ¨", "ê°€ê³„ëŒ€ì¶œ", "ì·¨ì•½ì°¨ì£¼"
    ],
    "ë³´í—˜ì‚¬": [
        "ë³´ì¥ì„±ë³´í—˜", "ì €ì¶•ì„±ë³´í—˜", "ë³€ì•¡ë³´í—˜", "í‡´ì§ì—°ê¸ˆ", "ì¼ë°˜ë³´í—˜", "ìë™ì°¨ë³´í—˜", "ALM", "ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨", "K-ICS",
        "ë³´í—˜ìˆ˜ìµì„±", "ë³´í—˜ì†ìµ", "ìˆ˜ì…ë³´í—˜ë£Œ", "CSM", "ìƒê°", "íˆ¬ìì†ìµ", "ìš´ìš©ì„±ê³¼", "IFRS4", "IFRS17", "ë³´í—˜ë¶€ì±„",
        "ì¥ê¸°ì„ ë„ê¸ˆë¦¬", "ìµœì¢…ê´€ì°°ë§Œê¸°", "ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„", "ì‹ ì¢…ìë³¸ì¦ê¶Œ", "í›„ìˆœìœ„ì±„", "ìœ„í—˜ìì‚°ë¹„ì¤‘", "ê°€ì¤‘ë¶€ì‹¤ìì‚°ë¹„ìœ¨"
    ],
    "ì¹´ë“œì‚¬": [
        "ë¯¼ê°„ì†Œë¹„ì§€í‘œ", "ëŒ€ì†ì¤€ë¹„ê¸ˆ", "ê°€ê³„ë¶€ì±„", "ì—°ì²´ìœ¨", "ê°€ë§¹ì ì¹´ë“œìˆ˜ìˆ˜ë£Œ", "ëŒ€ì¶œì„±ìì‚°", "ì‹ ìš©íŒë§¤ìì‚°", "ê³ ì •ì´í•˜ì—¬ì‹ ", "ë ˆë²„ë¦¬ì§€ë°°ìœ¨", "ê±´ì „ì„±"
    ],
    "ìºí”¼íƒˆ": [
        "ì¶©ë‹¹ê¸ˆì»¤ë²„ë¦¬ì§€ë¹„ìœ¨", "ê³ ì •ì´í•˜ì—¬ì‹ ", "PFêµ¬ì¡°ì¡°ì •", "ë¦¬ìŠ¤ìì‚°", "ì†ì‹¤í¡ìˆ˜ëŠ¥ë ¥", "ë¶€ë™ì‚°PFì—°ì²´ì±„ê¶Œ", "ìì‚°í¬íŠ¸í´ë¦¬ì˜¤", "ê±´ì „ì„±", "ì¡°ì •ì´ìì‚°ìˆ˜ìµë¥ "
    ],
    "ì§€ì£¼ì‚¬": [],
    "ì—ë„ˆì§€": [
        "ì •ìœ ", "ìœ ê°€", "ì •ì œë§ˆì§„", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "ì¬ê³  ì†ì‹¤", "ì¤‘êµ­ ìˆ˜ìš”", "IMO ê·œì œ", "ì €ìœ í™© ì—°ë£Œ", "LNG"
    ],
    "ë°œì „": [
        "LNG", "ì²œì—°ê°€ìŠ¤", "ìœ ê°€", "SMP", "REC", "ê³„í†µì‹œì¥", "íƒ„ì†Œì„¸", "íƒ„ì†Œë°°ì¶œê¶Œ", "ì „ë ¥ì‹œì¥ ê°œí¸", "ì „ë ¥ ììœ¨í™”", "í•œíŒŒ", "ê¸°ì˜¨ ìƒìŠ¹"
    ],
    "ìë™ì°¨": [
        "AMPC ë³´ì¡°ê¸ˆ", "AMPC", "IRA", "IRA ì¸ì„¼í‹°ë¸Œ", "ì¤‘êµ­ ë°°í„°ë¦¬", "EV ìˆ˜ìš”", "EV", "ì „ê¸°ì°¨", "ESSìˆ˜ìš”"
    ],
    "ì „ê¸°ì „ì": [
        "CHIPS ë³´ì¡°ê¸ˆ", "CHIPS", "ì¤‘êµ­", "ê´€ì„¸"
    ],
    "ì² ê°•": [
        "ì² ê´‘ì„", "í›„íŒ", "ê°•íŒ", "ì² ê·¼", "ìŠ¤í”„ë ˆë“œ", "ì² ê°•", "ê°€ë™ë¥ ", "ì œì² ì†Œ", "ì…§ë‹¤ìš´", "ì¤‘êµ­ì‚° ì €ê°€", "ì¤‘êµ­ ìˆ˜ì¶œ ê°ì†Œ", "ê±´ì„¤ê²½ê¸°", "ì¡°ì„  ìˆ˜ìš”", "íŒŒì—…"
    ],
    "ë¹„ì² ": [],
    "ì†Œë§¤": [
        "ë‚´ìˆ˜ë¶€ì§„", "ì‹œì¥ì§€ë°°ë ¥"
    ],
    "ì„ìœ í™”í•™": [
        "ì„ìœ í™”í•™", "ì„í™”", "ìœ ê°€", "ì¦ì„¤", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "PX", "ë²¤ì  ", "ì¤‘êµ­ ì¦ì„¤", "ì¤‘ë™ COTC"
    ],
    "ê±´ì„¤": [
        "ì² ê·¼ ê°€ê²©", "ì‹œë©˜íŠ¸ ê°€ê²©", "ê³µì‚¬ë¹„", "SOC ì˜ˆì‚°", "ë„ì‹œì •ë¹„ ì§€ì›", "ìš°ë°œì±„ë¬´", "ìˆ˜ì£¼", "ì£¼ê°„ì‚¬", "ì‚¬ê³ ", "ì‹œê³µëŠ¥ë ¥ìˆœìœ„", "ë¯¸ë¶„ì–‘", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ"
    ],
    "íŠ¹ìˆ˜ì±„": ["ìë³¸í™•ì¶©"]
}
major_categories = list(industry_filter_categories.keys())
sub_categories = {cat: industry_filter_categories[cat] for cat in major_categories]
all_fav_keywords = sorted(set(
    kw for cat in favorite_categories.values() for kw in cat if kw not in ["í…ŒìŠ¤íŠ¸1", "í…ŒìŠ¤íŠ¸2", "í…ŒìŠ¤íŠ¸3"]
))

# --- [ê³µí†µ í•„í„° ì˜µì…˜] ---
common_filter_categories = {
    "ì‹ ìš©/ë“±ê¸‰": [
        "ì‹ ìš©ë“±ê¸‰", "ë“±ê¸‰ì „ë§", "í•˜ë½", "ê°•ë“±", "í•˜í–¥", "ìƒí–¥", "ë””í´íŠ¸", "ë¶€ì‹¤", "ë¶€ë„", "ë¯¸ì§€ê¸‰", "ìˆ˜ìš” ë¯¸ë‹¬", "ë¯¸ë§¤ê°", "ì œë„ ê°œí¸", "EOD"
    ],
    "ìˆ˜ìš”/ê³µê¸‰": [
        "ìˆ˜ìš”", "ê³µê¸‰", "ìˆ˜ê¸‰", "ë‘”í™”", "ìœ„ì¶•", "ì„±ì¥", "ê¸‰ë“±", "ê¸‰ë½", "ìƒìŠ¹", "í•˜ë½", "ë¶€ì§„", "ì‹¬í™”"
    ],
    "ì‹¤ì /ì¬ë¬´": [
        "ì‹¤ì ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ì ì", "ì†ì‹¤", "ë¹„ìš©", "ë¶€ì±„ë¹„ìœ¨", "ì´ìë³´ìƒë°°ìœ¨"
    ],
    "ìê¸ˆ/ì¡°ë‹¬": [
        "ì°¨ì…", "ì¡°ë‹¬", "ì„¤ë¹„íˆ¬ì", "íšŒì‚¬ì±„", "ë°œí–‰", "ì¸ìˆ˜", "ë§¤ê°"
    ],
    "êµ¬ì¡°/ì¡°ì •": [
        "M&A", "í•©ë³‘", "ê³„ì—´ ë¶„ë¦¬", "êµ¬ì¡°ì¡°ì •", "ë‹¤ê°í™”", "êµ¬ì¡° ì¬í¸"
    ],
    "ê±°ì‹œ/ì •ì±…": [
        "ê¸ˆë¦¬", "í™˜ìœ¨", "ê´€ì„¸", "ë¬´ì—­ì œì¬", "ë³´ì¡°ê¸ˆ", "ì„¸ì•¡ ê³µì œ", "ê²½ìŸ"
    ],
    "ì§€ë°°êµ¬ì¡°/ë²•": [
        "íš¡ë ¹", "ë°°ì„", "ê³µì •ê±°ë˜", "ì˜¤ë„ˆë¦¬ìŠ¤í¬", "ëŒ€ì£¼ì£¼", "ì§€ë°°êµ¬ì¡°"
    ]
}
common_major_categories = list(common_filter_categories.keys())
common_sub_categories = {cat: common_filter_categories[cat] for cat in common_major_categories}

st.set_page_config(layout="wide")
col_title, col_option = st.columns([0.8, 0.2])
with col_title:
    st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)
with col_option:
    show_sentiment_badge = st.checkbox("ê¸°ì‚¬ëª©ë¡ì— ê°ì„±ë¶„ì„ ë°°ì§€ í‘œì‹œ", value=False)

# 1. í‚¤ì›Œë“œ ì…ë ¥/ê²€ìƒ‰ ë²„íŠ¼ (í•œ ì¤„, ë²„íŠ¼ ì˜¤ë¥¸ìª½)
col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="", key="keyword_input", label_visibility="visible")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

# 2. ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ/ê²€ìƒ‰ ë²„íŠ¼ (í•œ ì¤„, ë²„íŠ¼ ì˜¤ë¥¸ìª½)
st.markdown("**â­ ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ì¦ê²¨ì°¾ê¸° í‚¤ì›Œë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.", list(favorite_categories.keys()), key="cat_multi")
with col_cat_btn:
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", key="cat_search_btn", help="ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰", use_container_width=True)
for cat in selected_categories:
    st.session_state.favorite_keywords.update(favorite_categories[cat])

# ë‚ ì§œ ì…ë ¥
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼")
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼")

# --- ê³µí†µ í•„í„° ì˜µì…˜ (ì´ë¦„ ì˜† ì²´í¬ë°•ìŠ¤, ì›ë˜ ìœ„ì¹˜) ---
with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜"):
    use_common_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_common_filter")
    col_common_major, col_common_sub = st.columns([1, 1])
    with col_common_major:
        selected_common_major = st.selectbox("ê³µí†µ ëŒ€ë¶„ë¥˜(ë¶„ë¥˜)", common_major_categories, key="common_major")
    with col_common_sub:
        selected_common_sub = st.multiselect(
            "ê³µí†µ ì†Œë¶„ë¥˜(í•„í„° í‚¤ì›Œë“œ)",
            common_sub_categories[selected_common_major],
            default=common_sub_categories[selected_common_major],
            key="common_sub"
        )

# --- ê¸°ì—…ë³„ í•„í„° ì˜µì…˜ (ì´ë¦„ ì˜† ì²´í¬ë°•ìŠ¤, ì¢Œìš° ë¶„í• ) ---
with st.expander("ğŸ¢ ê¸°ì—…ë³„ í•„í„° ì˜µì…˜"):
    use_company_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_company_filter")
    col_company_major, col_company_sub = st.columns([1, 1])
    with col_company_major:
        selected_company = st.multiselect("ê¸°ì—…ëª…(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)", company_major_categories, key="company_major")
    with col_company_sub:
        selected_company_sub = []
        for comp in selected_company:
            selected_company_sub.extend(company_sub_categories.get(comp, []))
        selected_company_sub = sorted(set(selected_company_sub))
        st.write("í•„í„° í‚¤ì›Œë“œ")
        st.markdown(", ".join(selected_company_sub) if selected_company_sub else "(ì—†ìŒ)")

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ì´ë¦„ ì˜† ì²´í¬ë°•ìŠ¤, ì›ë˜ ìœ„ì¹˜) ---
with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜"):
    use_industry_filter = st.checkbox("ì´ í•„í„° ì ìš©", value=False, key="use_industry_filter")
    col_major, col_sub = st.columns([1, 1])
    with col_major:
        selected_major = st.selectbox("ëŒ€ë¶„ë¥˜(ì‚°ì—…)", major_categories, key="industry_major")
    with col_sub:
        selected_sub = st.multiselect(
            "ì†Œë¶„ë¥˜(í•„í„° í‚¤ì›Œë“œ)",
            sub_categories[selected_major],
            default=sub_categories[selected_major],
            key="industry_sub"
        )

# --- í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ (í•˜ë‹¨ìœ¼ë¡œ ì´ë™) ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    require_keyword_in_title = st.checkbox("ê¸°ì‚¬ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë³´ê¸°", value=False)
# --- ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜(ìš”ì²­ëŒ€ë¡œ ë‹¨ìˆœí™”) ---
def extract_article_text(url):
    try:
        article = newspaper.article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

# --- OpenAI ìš”ì•½/ê°ì„±ë¶„ì„ í•¨ìˆ˜ ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def summarize_and_sentiment_with_openai(text):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            "ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ìš”ì•½í•˜ê³  ê°ì„±ë¶„ì„ì„ í•´ì¤˜.\n\n"
            "- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n"
            "- [ìš”ì•½ë³¸]: ê¸°ì‚¬ ë‚´ìš©ì„ 2~3 ë¬¸ë‹¨(ê° ë¬¸ë‹¨ 2~4ë¬¸ì¥)ìœ¼ë¡œ, í•µì‹¬ ë‚´ìš©ì„ ì¶©ë¶„íˆ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ìš”ì•½\n"
            "- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. "
            "ë§Œì•½ íŒŒì‚°, ìê¸ˆë‚œ, íšŒìƒ, ì ì, êµ¬ì¡°ì¡°ì •, ì˜ì—…ì†ì‹¤, ë¶€ë„, ì±„ë¬´ë¶ˆì´í–‰, ê²½ì˜ ìœ„ê¸° ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n"
            "ê´‘ê³ , ë°°ë„ˆ, ì¶”ì²œê¸°ì‚¬, ì„œë¹„ìŠ¤ ì•ˆë‚´ ë“± ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì€ ëª¨ë‘ ìš”ì•½ê³¼ ê°ì„±ë¶„ì„ì—ì„œ ì œì™¸.\n\n"
            "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n"
            "[ìš”ì•½ë³¸]: (ì—¬ê¸°ì— ì—¬ëŸ¬ ë¬¸ë‹¨ ìš”ì•½)\n"
            "[ê°ì„±]: (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
            "[ê¸°ì‚¬ ë³¸ë¬¸]\n" + text
        )
    else:
        prompt = (
            "Summarize the following news article and analyze its sentiment.\n\n"
            "- [One-line Summary]: Summarize the entire article in one sentence.\n"
            "- [Summary]: Summarize the article in 2â€“3 paragraphs (each 2â€“4 sentences), so that the main content is well understood.\n"
            "- [Sentiment]: Classify the overall sentiment as one of: positive, negative, or neutral. "
            "If the article centers on bankruptcy, financial distress, restructuring, insolvency, operating loss, default, or management crisis, you must answer 'negative'.\n"
            "Exclude any advertisements, banners, recommended articles, or unrelated content.\n\n"
            "Respond in this format:\n"
            "[One-line Summary]: (your one-line summary)\n"
            "[Summary]: (your multi-paragraph summary)\n"
            "[Sentiment]: (positive/negative/neutral only)\n\n"
            "[ARTICLE]\n" + text
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m2 = re.search(r"\[ìš”ì•½ë³¸\]:\s*([\s\S]+?)(?:\[ê°ì„±\]:|$)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m2 = re.search(r"\[Summary\]:\s*([\s\S]+?)(?:\[Sentiment\]:|$)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if m1 else ""
    summary = m2.group(1).strip() if m2 else answer
    sentiment = m3.group(1).strip() if m3 else ""
    return one_line, summary, sentiment, text

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 2):  # 1íšŒë§Œ ë£¨í”„ (100ê°œë§Œ ìš”ì²­)
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": (page - 1) * 100 + 1,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
    return articles[:limit]

def fetch_gnews_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def summarize_article_from_url(article_url, title):
    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
    st.session_state.search_triggered = False

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords(
            sorted(keywords),
            start_date,
            end_date,
            require_keyword_in_title=require_keyword_in_title
        )

def article_passes_all_filters(article):
    filters = []
    if use_common_filter:
        filters.append(selected_common_sub)
    if use_company_filter:
        filters.append(selected_company_sub)
    if use_industry_filter:
        filters.append(selected_sub)
    if filters:
        return or_keyword_filter(article, *filters)
    else:
        return True

# --- ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì„ íƒ ê¸°ì‚¬ ìš”ì•½ì„ DataFrameìœ¼ë¡œ ë°”ë¡œ ë³€í™˜) ---
def get_excel_download(summary_data):
    df = pd.DataFrame(summary_data)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
    output.seek(0)
    return output

# --- ìš”ì•½/ê°ì„±ë¶„ì„/ê¸°ì‚¬ì„ íƒ/ì—‘ì…€ ì €ì¥ UI ---
def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True):
    SENTIMENT_CLASS = {
        "ê¸ì •": "sentiment-positive",
        "ë¶€ì •": "sentiment-negative",
        "ì¤‘ë¦½": "sentiment-neutral"
    }
    summary_data = []

    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### ê¸°ì‚¬ ìš”ì•½ ê²°ê³¼ (ì—‘ì…€ ì €ì¥í•  ê¸°ì‚¬ ì„ íƒ)")
        for keyword, articles in results.items():
            with st.container(border=True):
                st.markdown(f"**[{keyword}]**")
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    key = f"{keyword}_{idx}"
                    cache_key = f"summary_{key}"
                    if show_sentiment_badge:
                        if cache_key not in st.session_state:
                            one_line, summary, sentiment, full_text = summarize_article_from_url(article['link'], article['title'])
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                        else:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        sentiment_label = sentiment if sentiment else "ë¶„ì„ì¤‘"
                        sentiment_class = SENTIMENT_CLASS.get(sentiment_label, "sentiment-neutral")
                        md_line = (
                            f"[{article['title']}]({article['link']}) "
                            f"<span class='sentiment-badge {sentiment_class}'>({sentiment_label})</span> "
                            f"({article['date']} | {article['source']})"
                        )
                    else:
                        md_line = (
                            f"[{article['title']}]({article['link']}) "
                            f"({article['date']} | {article['source']})"
                        )
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                    with cols[1]:
                        st.markdown(md_line, unsafe_allow_html=True)
                    st.session_state.article_checked[key] = checked

                if limit < len(articles):
                    if st.button("ë”ë³´ê¸°", key=f"more_{keyword}"):
                        st.session_state.show_limit[keyword] += 10

    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):
            selected_articles = []
            for keyword, articles in results.items():
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    key = f"{keyword}_{idx}"
                    cache_key = f"summary_{key}"
                    if st.session_state.article_checked.get(key, False):
                        if (not show_sentiment_badge) or (cache_key not in st.session_state):
                            one_line, summary, sentiment, full_text = summarize_article_from_url(article['link'], article['title'])
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                        else:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        selected_articles.append({
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": article['title'],
                            "ìš”ì•½": one_line,
                            "ìš”ì•½ë³¸": summary,
                            "ê°ì„±": sentiment,
                            "ë§í¬": article['link'],
                            "ë‚ ì§œ": article['date'],
                            "ì¶œì²˜": article['source']
                        })
                        if show_sentiment_badge:
                            st.markdown(
                                f"#### [{article['title']}]({article['link']}) "
                                f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-neutral')}'>({sentiment})</span>",
                                unsafe_allow_html=True
                            )
                        else:
                            st.markdown(f"#### [{article['title']}]({article['link']})", unsafe_allow_html=True)
                        st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {article['date']} | {article['source']}")
                        st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {one_line}")
                        st.markdown(f"- **ìš”ì•½ë³¸:** {summary}")
                        if not show_sentiment_badge:
                            st.markdown(f"- **ê°ì„±ë¶„ì„:** `{sentiment}`")
                        st.markdown("---")
            summary_data = selected_articles

            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(summary_data)}")

            # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            if summary_data:
                excel_bytes = get_excel_download(summary_data)
                st.download_button(
                    label="ğŸ“¥ ì„ íƒ ê¸°ì‚¬ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=excel_bytes.getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë‹¤ìš´ë¡œë“œ.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(filtered_results, st.session_state.show_limit, show_sentiment_badge)
    
# --- ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜(ìš”ì²­ëŒ€ë¡œ ë‹¨ìˆœí™”) ---
def extract_article_text(url):
    try:
        article = newspaper.article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

# --- OpenAI ìš”ì•½/ê°ì„±ë¶„ì„ í•¨ìˆ˜ ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def summarize_and_sentiment_with_openai(text):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            "ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ìš”ì•½í•˜ê³  ê°ì„±ë¶„ì„ì„ í•´ì¤˜.\n\n"
            "- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n"
            "- [ìš”ì•½ë³¸]: ê¸°ì‚¬ ë‚´ìš©ì„ 2~3 ë¬¸ë‹¨(ê° ë¬¸ë‹¨ 2~4ë¬¸ì¥)ìœ¼ë¡œ, í•µì‹¬ ë‚´ìš©ì„ ì¶©ë¶„íˆ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ìš”ì•½\n"
            "- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. "
            "ë§Œì•½ íŒŒì‚°, ìê¸ˆë‚œ, íšŒìƒ, ì ì, êµ¬ì¡°ì¡°ì •, ì˜ì—…ì†ì‹¤, ë¶€ë„, ì±„ë¬´ë¶ˆì´í–‰, ê²½ì˜ ìœ„ê¸° ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n"
            "ê´‘ê³ , ë°°ë„ˆ, ì¶”ì²œê¸°ì‚¬, ì„œë¹„ìŠ¤ ì•ˆë‚´ ë“± ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì€ ëª¨ë‘ ìš”ì•½ê³¼ ê°ì„±ë¶„ì„ì—ì„œ ì œì™¸.\n\n"
            "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n"
            "[ìš”ì•½ë³¸]: (ì—¬ê¸°ì— ì—¬ëŸ¬ ë¬¸ë‹¨ ìš”ì•½)\n"
            "[ê°ì„±]: (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
            "[ê¸°ì‚¬ ë³¸ë¬¸]\n" + text
        )
    else:
        prompt = (
            "Summarize the following news article and analyze its sentiment.\n\n"
            "- [One-line Summary]: Summarize the entire article in one sentence.\n"
            "- [Summary]: Summarize the article in 2â€“3 paragraphs (each 2â€“4 sentences), so that the main content is well understood.\n"
            "- [Sentiment]: Classify the overall sentiment as one of: positive, negative, or neutral. "
            "If the article centers on bankruptcy, financial distress, restructuring, insolvency, operating loss, default, or management crisis, you must answer 'negative'.\n"
            "Exclude any advertisements, banners, recommended articles, or unrelated content.\n\n"
            "Respond in this format:\n"
            "[One-line Summary]: (your one-line summary)\n"
            "[Summary]: (your multi-paragraph summary)\n"
            "[Sentiment]: (positive/negative/neutral only)\n\n"
            "[ARTICLE]\n" + text
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m2 = re.search(r"\[ìš”ì•½ë³¸\]:\s*([\s\S]+?)(?:\[ê°ì„±\]:|$)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m2 = re.search(r"\[Summary\]:\s*([\s\S]+?)(?:\[Sentiment\]:|$)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if m1 else ""
    summary = m2.group(1).strip() if m2 else answer
    sentiment = m3.group(1).strip() if m3 else ""
    return one_line, summary, sentiment, text

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 2):  # 1íšŒë§Œ ë£¨í”„ (100ê°œë§Œ ìš”ì²­)
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": (page - 1) * 100 + 1,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
    return articles[:limit]

def fetch_gnews_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def summarize_article_from_url(article_url, title):
    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
    st.session_state.search_triggered = False

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords(
            sorted(keywords),
            start_date,
            end_date,
            require_keyword_in_title=require_keyword_in_title
        )

def article_passes_all_filters(article):
    filters = []
    if use_common_filter:
        filters.append(selected_common_sub)
    if use_company_filter:
        filters.append(selected_company_sub)
    if use_industry_filter:
        filters.append(selected_sub)
    if filters:
        return or_keyword_filter(article, *filters)
    else:
        return True

# --- ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì„ íƒ ê¸°ì‚¬ ìš”ì•½ì„ DataFrameìœ¼ë¡œ ë°”ë¡œ ë³€í™˜) ---
def get_excel_download(summary_data):
    df = pd.DataFrame(summary_data)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
    output.seek(0)
    return output

# --- ìš”ì•½/ê°ì„±ë¶„ì„/ê¸°ì‚¬ì„ íƒ/ì—‘ì…€ ì €ì¥ UI ---
def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True):
    SENTIMENT_CLASS = {
        "ê¸ì •": "sentiment-positive",
        "ë¶€ì •": "sentiment-negative",
        "ì¤‘ë¦½": "sentiment-neutral"
    }
    summary_data = []

    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### ê¸°ì‚¬ ìš”ì•½ ê²°ê³¼ (ì—‘ì…€ ì €ì¥í•  ê¸°ì‚¬ ì„ íƒ)")
        for keyword, articles in results.items():
            with st.container(border=True):
                st.markdown(f"**[{keyword}]**")
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    key = f"{keyword}_{idx}"
                    cache_key = f"summary_{key}"
                    if show_sentiment_badge:
                        if cache_key not in st.session_state:
                            one_line, summary, sentiment, full_text = summarize_article_from_url(article['link'], article['title'])
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                        else:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        sentiment_label = sentiment if sentiment else "ë¶„ì„ì¤‘"
                        sentiment_class = SENTIMENT_CLASS.get(sentiment_label, "sentiment-neutral")
                        md_line = (
                            f"[{article['title']}]({article['link']}) "
                            f"<span class='sentiment-badge {sentiment_class}'>({sentiment_label})</span> "
                            f"({article['date']} | {article['source']})"
                        )
                    else:
                        md_line = (
                            f"[{article['title']}]({article['link']}) "
                            f"({article['date']} | {article['source']})"
                        )
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                    with cols[1]:
                        st.markdown(md_line, unsafe_allow_html=True)
                    st.session_state.article_checked[key] = checked

                if limit < len(articles):
                    if st.button("ë”ë³´ê¸°", key=f"more_{keyword}"):
                        st.session_state.show_limit[keyword] += 10

    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):
            selected_articles = []
            for keyword, articles in results.items():
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    key = f"{keyword}_{idx}"
                    cache_key = f"summary_{key}"
                    if st.session_state.article_checked.get(key, False):
                        if (not show_sentiment_badge) or (cache_key not in st.session_state):
                            one_line, summary, sentiment, full_text = summarize_article_from_url(article['link'], article['title'])
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                        else:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        selected_articles.append({
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": article['title'],
                            "ìš”ì•½": one_line,
                            "ìš”ì•½ë³¸": summary,
                            "ê°ì„±": sentiment,
                            "ë§í¬": article['link'],
                            "ë‚ ì§œ": article['date'],
                            "ì¶œì²˜": article['source']
                        })
                        if show_sentiment_badge:
                            st.markdown(
                                f"#### [{article['title']}]({article['link']}) "
                                f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-neutral')}'>({sentiment})</span>",
                                unsafe_allow_html=True
                            )
                        else:
                            st.markdown(f"#### [{article['title']}]({article['link']})", unsafe_allow_html=True)
                        st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {article['date']} | {article['source']}")
                        st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {one_line}")
                        st.markdown(f"- **ìš”ì•½ë³¸:** {summary}")
                        if not show_sentiment_badge:
                            st.markdown(f"- **ê°ì„±ë¶„ì„:** `{sentiment}`")
                        st.markdown("---")
            summary_data = selected_articles

            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(summary_data)}")

            # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            if summary_data:
                excel_bytes = get_excel_download(summary_data)
                st.download_button(
                    label="ğŸ“¥ ì„ íƒ ê¸°ì‚¬ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=excel_bytes.getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë‹¤ìš´ë¡œë“œ.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(filtered_results, st.session_state.show_limit, show_sentiment_badge)
