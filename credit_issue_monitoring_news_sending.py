import streamlit as st
import requests
import re
from datetime import datetime
import telepot

# --- ìŠ¤íƒ€ì¼ ê°œì„  ---
st.markdown("""
    <style>
        .block-container {padding-top: 2rem; padding-bottom: 2rem;}
        .stButton button {margin-top: 6px; margin-bottom: 6px; border-radius: 8px;}
        .stTextInput > div > div > input {font-size: 16px;}
        .stMultiSelect [data-baseweb="tag"] {
            background-color: #fff0f0 !important;
            color: #d60000 !important;
            border: 1px solid #d60000 !important;
        }
        .stMultiSelect label { color: #d60000 !important; font-weight: bold;}
        .stSelectbox, .stDateInput, .stMultiSelect {margin-bottom: 0.5rem;}
    </style>
""", unsafe_allow_html=True)

# --- API í‚¤ ì„¤ì • ---
NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"

# --- í…”ë ˆê·¸ë¨ ì„¤ì • ---
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

# --- í‚¤ì›Œë“œ ---
credit_keywords = ["ì‹ ìš©ë“±ê¸‰", "ì‹ ìš©í•˜í–¥", "ì‹ ìš©ìƒí–¥", "ë“±ê¸‰ì¡°ì •", "ë¶€ì •ì ", "ê¸ì •ì ", "í‰ê°€"]
finance_keywords = ["ì ì", "í‘ì", "ë¶€ì±„", "ì°¨ì…ê¸ˆ", "í˜„ê¸ˆíë¦„", "ì˜ì—…ì†ì‹¤", "ìˆœì´ìµ", "ë¶€ë„", "íŒŒì‚°"]
all_filter_keywords = sorted(set(credit_keywords + finance_keywords))
default_credit_issue_patterns = [
    "ì‹ ìš©ë“±ê¸‰", "ì‹ ìš©í‰ê°€", "í•˜í–¥", "ìƒí–¥", "ê°•ë“±", "ì¡°ì •", "ë¶€ë„",
    "íŒŒì‚°", "ë””í´íŠ¸", "ì±„ë¬´ë¶ˆì´í–‰", "ì ì", "ì˜ì—…ì†ì‹¤", "í˜„ê¸ˆíë¦„", "ìê¸ˆë‚œ",
    "ì¬ë¬´ìœ„í—˜", "ë¶€ì •ì  ì „ë§", "ê¸ì •ì  ì „ë§", "ê¸°ì—…íšŒìƒ", "ì›Œí¬ì•„ì›ƒ", "êµ¬ì¡°ì¡°ì •", "ìë³¸ì ì‹"
]

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "expanded_keywords" not in st.session_state:
    st.session_state.expanded_keywords = set()
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(token=TELEGRAM_TOKEN)
    def send_message(self, message):
        self.bot.sendMessage(TELEGRAM_CHAT_ID, message, parse_mode="Markdown")

def is_credit_risk_news(text, keywords):
    for word in keywords:
        if re.search(word, text, re.IGNORECASE):
            return True
    return False

def filter_by_issues(title, desc, selected_keywords, enable_credit_filter, credit_filter_keywords):
    content = title + " " + desc
    if enable_credit_filter and not is_credit_risk_news(content, credit_filter_keywords):
        return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, enable_credit_filter=True, credit_filter_keywords=None, limit=100):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 6):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 10,
            "start": (page - 1) * 10 + 1,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [], enable_credit_filter, credit_filter_keywords):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
    return articles[:limit]

def fetch_gnews_news(query, enable_credit_filter=True, credit_filter_keywords=None, limit=100):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [], enable_credit_filter, credit_filter_keywords):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def render_articles_columnwise(results, show_limit):
    col_count = min(len(results), 4)
    cols = st.columns(col_count)
    for idx, (keyword, articles) in enumerate(results.items()):
        with cols[idx % col_count]:
            st.markdown(
                f"<span style='font-size:22px;font-weight:700;'>ğŸ“ {keyword}</span>",
                unsafe_allow_html=True
            )
            articles_to_show = articles[:show_limit.get(keyword, 5)]
            for article in articles_to_show:
                st.markdown(
                    f"""
                    <div style='margin-bottom: 12px; padding: 10px; border: 1px solid #eee; border-radius: 10px; background-color: #fafafa;'>
                        <div style='font-weight: bold; font-size: 15px; margin-bottom: 4px;'>
                            <a href="{article['link']}" target="_blank" style='text-decoration: none; color: #1155cc;'>
                                {article['title']}
                            </a>
                        </div>
                        <div style='font-size: 12px; color: gray;'>
                            {article['date']} | {article['source']}
                        </div>
                    </div>
                    """, unsafe_allow_html=True
                )
            if len(articles) > show_limit.get(keyword, 5):
                if st.button("ë”ë³´ê¸°", key=f"more_{keyword}", use_container_width=True):
                    st.session_state.show_limit[keyword] += 5
                    st.rerun()

def send_to_telegram(keyword, articles):
    if articles:
        msg = f"*[{keyword}] ê´€ë ¨ ìƒìœ„ ë‰´ìŠ¤ 5ê±´:*\n"
        for a in articles:
            title = re.sub(r"[\U00010000-\U0010ffff]", "", a['title'])
            msg += f"- [{title}]({a['link']})\n"
        try:
            Telegram().send_message(msg)
        except Exception as e:
            st.warning(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜¤ë¥˜: {e}")

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, enable_credit_filter, credit_filter_keywords):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, enable_credit_filter, credit_filter_keywords)
        else:
            articles = fetch_naver_news(k, start_date, end_date, enable_credit_filter, credit_filter_keywords)
        st.session_state.search_results[k] = articles
        st.session_state.show_limit[k] = 5
        send_to_telegram(k, articles[:5])

# --- ìš”ì•½ API í˜¸ì¶œ í•¨ìˆ˜ (ìë™ ì–¸ì–´ ê°ì§€ í¬í•¨) ---
def summarize_article_from_url(article_url):
    try:
        # ì–¸ì–´ ìë™ ê°ì§€ìš© API URL
        api_url = "https://article-extractor-and-summarizer.p.rapidapi.com/summarize"
        headers = {
            "x-rapidapi-key": "3558ef6abfmshba1bd48265c6fc4p101a63jsnb2c1ee3d33c4",
            "x-rapidapi-host": "article-extractor-and-summarizer.p.rapidapi.com"
        }

        # ìë™ ì–¸ì–´ ê°ì§€ ë° ìš”ì•½ ìˆ˜í–‰
        lang = "ko" if any(ord(c) > 127 for c in article_url) else "en"
        params = {
            "url": article_url,
            "lang": lang,
            "engine": "2"
        }

        response = requests.get(api_url, headers=headers, params=params)
        response.raise_for_status()
        result = response.json()
        return result.get("summary", "ìš”ì•½ ê²°ê³¼ ì—†ìŒ"), result.get("text", "ë³¸ë¬¸ ì—†ìŒ")
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None

# --- ê¸°ì‚¬ ì¹´ë“œ UI ìˆ˜ì •: ìš”ì•½ ë²„íŠ¼ ì¶”ê°€ ---
def render_articles_columnwise_with_summary(results, show_limit):
    col_count = min(len(results), 4)
    cols = st.columns(col_count)
    for idx, (keyword, articles) in enumerate(results.items()):
        with cols[idx % col_count]:
            st.markdown(
                f"<span style='font-size:22px;font-weight:700;'>ğŸ“ {keyword}</span>",
                unsafe_allow_html=True
            )
            articles_to_show = articles[:show_limit.get(keyword, 5)]
            for i, article in enumerate(articles_to_show):
                with st.container():
                    st.markdown(
                        f"""
                        <div style='margin-bottom: 10px; padding: 10px; border: 1px solid #eee; border-radius: 10px; background-color: #fafafa;'>
                            <div style='font-weight: bold; font-size: 15px; margin-bottom: 4px;'>
                                <a href="{article['link']}" target="_blank" style='text-decoration: none; color: #1155cc;'>
                                    {article['title']}
                                </a>
                            </div>
                            <div style='font-size: 12px; color: gray;'>
                                {article['date']} | {article['source']}
                            </div>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
                    # ìš”ì•½ ë²„íŠ¼
                    if st.button("ìš”ì•½", key=f"summary_{keyword}_{i}", use_container_width=True):
                        with st.spinner("ê¸°ì‚¬ ìš”ì•½ ì¤‘..."):
                            summary, full_text = summarize_article_from_url(article['link'])
                            if full_text:
                                st.markdown("<div style='font-size:14px; font-weight:bold;'>ğŸ” ë³¸ë¬¸ ìš”ì•½:</div>", unsafe_allow_html=True)
                                st.write(summary)
                            else:
                                st.warning(summary)

            # ë”ë³´ê¸° ë²„íŠ¼
            if len(articles) > show_limit.get(keyword, 5):
                if st.button("ë”ë³´ê¸°", key=f"more_{keyword}", use_container_width=True):
                    st.session_state.show_limit[keyword] += 5
                    st.rerun()


# --- Streamlit ì„¤ì • ---
st.set_page_config(layout="wide")
st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)

# 1. í‚¤ì›Œë“œ ì…ë ¥ ë° ë²„íŠ¼ í•œ ì¤„ì— ì •ë ¬ (yì¶• ë§ì¶¤ - ë²„íŠ¼ í•œ ë²ˆë§Œ ë‚´ë¦¼)
col1, col2, col3 = st.columns([6, 1, 1])
with col1:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="")
with col2:
    st.write("")  # yì¶• ë§ì¶¤ìš© placeholder (1ë²ˆ)
    search_clicked = st.button("ê²€ìƒ‰", use_container_width=True)
with col3:
    st.write("")  # yì¶• ë§ì¶¤ìš© placeholder (1ë²ˆ)
    fav_add_clicked = st.button("â­ ì¦ê²¨ì°¾ê¸° ì¶”ê°€", use_container_width=True)
    if fav_add_clicked:
        new_keywords = {kw.strip() for kw in keywords_input.split(",") if kw.strip()}
        st.session_state.favorite_keywords.update(new_keywords)
        st.success("ì¦ê²¨ì°¾ê¸°ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 2. ë‚ ì§œ ì…ë ¥ ë‘ ì¹¸ì— ì •ë ¬
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼")
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼")

# 3. í•„í„° ì˜µì…˜
with st.expander("ğŸ›¡ï¸ ì‹ ìš©ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    enable_credit_filter = st.checkbox("ì‹ ìš©ìœ„í—˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§", value=False)
    credit_filter_keywords = st.multiselect(
        "ì‹ ìš©ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ (í•˜ë‚˜ ì´ìƒ ì„ íƒ)",
        options=default_credit_issue_patterns,
        default=default_credit_issue_patterns,
        key="credit_filter"
    )

# 4. ì¦ê²¨ì°¾ê¸° ê²€ìƒ‰ ì˜ì—­ (yì¶• ë§ì¶¤)
fav_col1, fav_col2 = st.columns([5, 1])
with fav_col1:
    fav_selected = st.multiselect("â­ ì¦ê²¨ì°¾ê¸°ì—ì„œ ê²€ìƒ‰", sorted(st.session_state.favorite_keywords))
with fav_col2:
    st.write("")  # yì¶• ë§ì¶¤ìš© placeholder (1ë²ˆ)
    fav_search_clicked = st.button("ì¦ê²¨ì°¾ê¸°ë¡œ ê²€ìƒ‰", use_container_width=True)

# 5. ê²€ìƒ‰ ë° ì¦ê²¨ì°¾ê¸° ê²€ìƒ‰ ì²˜ë¦¬
if search_clicked and keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, enable_credit_filter, credit_filter_keywords)

if fav_search_clicked and fav_selected:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        process_keywords(fav_selected, start_date, end_date, enable_credit_filter, credit_filter_keywords)

# 6. ë‰´ìŠ¤ ê²°ê³¼ ì¹´ë“œ ì»¬ëŸ¼ ì •ë ¬
if st.session_state.search_results:
    render_articles_columnwise_with_summary(st.session_state.search_results, st.session_state.show_limit)
