import os
import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
from datetime import datetime, timedelta
import telepot
from openai import OpenAI
import newspaper
import difflib
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import html
import json
from datetime import datetime, timedelta

# --- config.json ë¡œë“œ ---
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

EXCLUDE_TITLE_KEYWORDS = config["EXCLUDE_TITLE_KEYWORDS"] # --- ì œì™¸ í‚¤ì›Œë“œ ---
ALLOWED_SOURCES = set(config["ALLOWED_SOURCES"]) # í•„í„°ë§í•  ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ (www. ì œê±°ëœ ë„ë©”ì¸ ê¸°ì¤€)
favorite_categories = config["favorite_categories"] # --- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬(ë³€ê²½ ê¸ˆì§€) ---
excel_company_categories = config["excel_company_categories"]
common_filter_categories = config["common_filter_categories"] # --- ê³µí†µ í•„í„° ì˜µì…˜(ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì—†ì´ ëª¨ë‘ ì ìš©) ---
industry_filter_categories = config["industry_filter_categories"] # --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ ---
SYNONYM_MAP = config["synonym_map"]

# ê³µí†µ í•„í„° í‚¤ì›Œë“œ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)

def expand_keywords_with_synonyms(original_keywords):
    expanded_map = {}
    for kw in original_keywords:
        synonyms = SYNONYM_MAP.get(kw, [])
        expanded_map[kw] = [kw] + synonyms
    return expanded_map

def process_keywords_with_synonyms(favorite_to_expand_map, start_date, end_date, require_keyword_in_title=False):
    for main_kw, kw_list in favorite_to_expand_map.items():
        all_articles = []
        for search_kw in kw_list:
            fetched = fetch_naver_news(search_kw, start_date, end_date,
                                       require_keyword_in_title=require_keyword_in_title)
            all_articles.extend(fetched)

        # ì¤‘ë³µ ì œê±°
        if st.session_state.get("remove_duplicate_articles", False):
            all_articles = remove_duplicates(all_articles)

        st.session_state.search_results[main_kw] = all_articles
        if main_kw not in st.session_state.show_limit:
            st.session_state.show_limit[main_kw] = 5

# --- CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] { background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold; }
.sentiment-badge { display: inline-block; padding: 0.08em 0.6em; margin-left: 0.2em; border-radius: 0.8em; font-size: 0.85em; font-weight: bold; vertical-align: middle; }
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
</style>

""", unsafe_allow_html=True)
st.markdown("""
<style>
.news-title { 
    word-break: break-all !important; 
    white-space: normal !important; 
    display: block !important;
    overflow: visible !important;
}
</style>
""", unsafe_allow_html=True)

def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

def init_session_state():
    """Streamlit ì„¸ì…˜ ë³€ìˆ˜ë“¤ì„ ì¼ê´„ ì´ˆê¸°í™”"""
    defaults = {
        "favorite_keywords": set(),
        "search_results": {},
        "show_limit": {},
        "search_triggered": False,
        "selected_articles": [],
        "cat_multi": [],
        "cat_major_autoset": [],
        "important_articles_preview": [],
        "important_selected_index": [],
        "article_checked_left": {},
        "article_checked": {},
        "industry_major_sub_map": {},
        "end_date": datetime.today().date(),
        "start_date": datetime.today().date() - timedelta(days=7),
        "remove_duplicate_articles": True,
        "require_exact_keyword_in_title_or_content": True,
        "filter_allowed_sources_only": False,
        "use_industry_filter": True,
        "show_sentiment_badge": False,
        "enable_summary": True,
        "keyword_input": ""
    }
    for key, default_val in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_val

# --- UI ì‹œì‘ ---
st.set_page_config(layout="wide")

# âœ… ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™” í˜¸ì¶œ
init_session_state()

col_title, col_option1, col_option2 = st.columns([0.5, 0.2, 0.3])

# --- ì¹´í…Œê³ ë¦¬-ì‚°ì—… ëŒ€ë¶„ë¥˜ ë§¤í•‘ í•¨ìˆ˜ ---
def get_industry_majors_from_favorites(selected_categories):
    favorite_to_industry_major = config["favorite_to_industry_major"]
    majors = set()
    for cat in selected_categories:
        for major in favorite_to_industry_major.get(cat, []):
            majors.add(major)
    return list(majors)

# --- UI ì‹œì‘ ---
st.set_page_config(layout="wide")
col_title, col_option1, col_option2 = st.columns([0.5, 0.2, 0.3])
with col_title:
    st.markdown(
        "<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>"
        "<a href='https://credit-issue-monitoring-news-sending.onrender.com/' target='_blank' style='text-decoration:none; color:#1a1a1a;'>"
        "ğŸ“Š Credit Issue Monitoring</a></h1>",
        unsafe_allow_html=True
    )
with col_option1:
    show_sentiment_badge = st.checkbox("ê°ì„±ë¶„ì„ ë°°ì§€í‘œì‹œ", key="show_sentiment_badge")
with col_option2:
    enable_summary = st.checkbox("ìš”ì•½ ê¸°ëŠ¥", key="enable_summary")
    
col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input(label="", value="", key="keyword_input", label_visibility="collapsed")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

st.markdown("**â­ ì‚°ì—…êµ° ì„ íƒ**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        "",
        list(favorite_categories.keys()), key="cat_multi", label_visibility="collapsed"
        )
if selected_categories:
    auto_selected_majors = get_industry_majors_from_favorites(selected_categories)
    st.session_state.cat_major_autoset = auto_selected_majors.copy()
else:
    st.session_state.cat_major_autoset = []
with col_cat_btn:
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", key="cat_search_btn", help="ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰", use_container_width=True)
for cat in selected_categories:
    st.session_state.favorite_keywords.update(favorite_categories[cat])

# ë‚ ì§œ ì…ë ¥ (ê¸°ë³¸ ì„¸íŒ…: ì¢…ë£Œì¼=ì˜¤ëŠ˜, ì‹œì‘ì¼=ì˜¤ëŠ˜-7ì¼)
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼", value=st.session_state["start_date"], key="start_date_input")
    st.session_state["start_date"] = start_date
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼", value=st.session_state["end_date"], key="end_date_input")
    st.session_state["end_date"] = end_date

with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©ë¨)"):
    for major, subs in common_filter_categories.items():
        st.markdown(f"**{major}**: {', '.join(subs)}")

with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ëŒ€ë¶„ë¥˜ë³„ ì†Œë¶„ë¥˜ í•„í„°ë§)"):
    use_industry_filter = st.checkbox("ì´ í•„í„° ì ìš©", key="use_industry_filter")

    # UI: ì„ íƒëœ ì‚°ì—…êµ°ì—ì„œ ìë™ ë§¤í•‘ëœ ëŒ€ë¶„ë¥˜ ì¶”ì¶œ
    selected_major_map = get_industry_majors_from_favorites(selected_categories)

    updated_map = {}
    for major in selected_major_map:
        options = industry_filter_categories.get(major, [])
        default_selected = options if major not in st.session_state.industry_major_sub_map else st.session_state.industry_major_sub_map[major]
        selected_sub = st.multiselect(
            f"{major} ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ",
            options,
            default=default_selected,
            key=f"subfilter_{major}"
        )
        updated_map[major] = selected_sub

    st.session_state.industry_major_sub_map = updated_map
    
# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ê¸°ëŠ¥ ì²´í¬ë°•ìŠ¤ í¬í•¨ëœ í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    require_exact_keyword_in_title_or_content = st.checkbox("í‚¤ì›Œë“œê°€ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", key="require_exact_keyword_in_title_or_content")
    remove_duplicate_articles = st.checkbox("ì¤‘ë³µ ê¸°ì‚¬ ì œê±°", key="remove_duplicate_articles", help="í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.")
    filter_allowed_sources_only = st.checkbox(
        "íŠ¹ì • ì–¸ë¡ ì‚¬ë§Œ ê²€ìƒ‰", 
        key="filter_allowed_sources_only", 
        help="ì„ íƒëœ ë©”ì´ì € ì–¸ë¡ ì‚¬ë§Œ í•„í„°ë§í•˜ê³ , ê·¸ ì™¸ ì–¸ë¡ ì€ ì œì™¸í•©ë‹ˆë‹¤."
    )

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def summarize_and_sentiment_with_openai(text, do_summary=True):
    """
    ë³¸ë¬¸ ìš”ì•½/ê°ì„±ë¶„ì„ê³¼ ëª¨ë“  ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨.
    - OpenAI ì‘ë‹µ í¬ë§· ë¶ˆì¼ì¹˜/ë¹„ì •ê·œ ì¶œë ¥/ë³¸ë¬¸ ì˜¤ë¥˜/í‚¤ ëˆ„ë½ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    - ë°˜í™˜ê°’: (í•œì¤„ìš”ì•½, ì£¼ìš”í‚¤ì›Œë“œ, ê°ì„±, ì „ì²´ë³¸ë¬¸)
    """
    # 1. í‚¤ ì—†ì„ ë•Œ ë°˜í™˜
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", text

    # 2. ë³¸ë¬¸ ì²´í¬
    if not text or "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜" in text:
        return "ê¸°ì‚¬ ë³¸ë¬¸ì´ ì¶”ì¶œ ì‹¤íŒ¨", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", text

    # 3. í”„ë¡¬í”„íŠ¸(ìµœì í™” version, ì´ì „ ë‹µë³€ ì°¸ê³ )
    lang = detect_lang(text)
    if lang == "ko":
        role_prompt = (
            "ë„ˆëŠ” ê²½ì œ ë‰´ìŠ¤ ìš”ì•½/ë¶„ì„ ì „ë¬¸ê°€ì•¼. í•œ ë¬¸ì¥ ìš”ì•½ì—ëŠ” ë°˜ë“œì‹œ ì£¼ì²´, í•µì‹¬ ì‚¬ê±´, ê²°ê³¼ë¥¼, "
            "ê°ì„± ë¶„ë¥˜ëŠ” íŒŒì‚°Â·ê°ì› ë“± ë¶€ì • ì´ìŠˆë©´ 'ë¶€ì •', ì‹ ê·œ íˆ¬ìÂ·í˜¸ì¬ ë• 'ê¸ì •', ì¤‘ë¦½ì€ ê¸ˆì§€. í¬ë§·ì€ ì§€ì •ëœ í‚¤ ê·¸ëŒ€ë¡œ."
        )
        main_prompt = """
ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ë¶„ì„í•´ ë‹¤ìŒ ì„¸ê°€ì§€ë¥¼ ì •í™•íˆ ì‘ë‹µí•˜ë¼.

[í•œ ì¤„ ìš”ì•½]: ì£¼ìš” ì¸ë¬¼/ê¸°ì—…, ì‚¬ê±´, ê²°ê³¼ í¬í•¨
[ê²€ìƒ‰ í‚¤ì›Œë“œ]: ì´ ê¸°ì‚¬ê°€ ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ ëª¨ë‘ ëª…ì‹œ 
[ê°ì„±]: ê¸ì • ë˜ëŠ” ë¶€ì • (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ)
[ì£¼ìš” í‚¤ì›Œë“œ]: ì¸ë¬¼, ê¸°ì—…, ì¡°ì§ëª…ë§Œ ì½¤ë§ˆ(,)ë¡œ, ì—†ìœ¼ë©´ ì—†ìŒ

[ê¸°ì‚¬ ë³¸ë¬¸]
""" + text
    else:
        role_prompt = (
            "You are a financial news summarization expert. The summary must contain entity, main event, outcome. "
            "Sentiment is only positive/negative strictly (never neutral). Use labels exactly as requested."
        )
        main_prompt = """
Analyze the article and extract these three exactly:

[One-line Summary]: One sentence, include entity, event, outcome
[Search Keywords]: Comma-separated list of keywords used to retrieve this article  
[Sentiment]: positive or negative (only one)
[Key Entities]: All mentioned companies/people/org, comma separated, or None

[ARTICLE]
""" + text

    # 4. OpenAI í˜¸ì¶œ & ì˜¤ë¥˜ ì²˜ë¦¬
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": role_prompt},
                {"role": "user", "content": main_prompt}
            ],
            max_tokens=900,
            temperature=0.3
        )
        answer = response.choices[0].message.content.strip()
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", text

    # 5. ì •ê·œì‹ íŒŒì‹± (ì‹¤íŒ¨ì‹œì—ë„ ê¸°ë³¸ê°’ ë°˜í™˜, None ë°©ì§€)
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*([^\n]+)", answer)
        m2 = re.search(r"\[ì£¼ìš” í‚¤ì›Œë“œ\]:\s*([^\n]+)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(ê¸ì •|ë¶€ì •)", answer)
        # ì¶”ê°€: ë§Œì¼ 'ê°ì„±'ì´ ë’¤ì— ë‚˜ì˜¤ë©´
        if not m3:
            m3 = re.search(r"\[ê°ì„±\]:\s*([^\n]+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*([^\n]+)", answer)
        m2 = re.search(r"\[Key Entities\]:\s*([^\n]+)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(positive|negative)", answer, re.I)
        # fallback for Sentiment
        if not m3:
            m3 = re.search(r"\[Sentiment\]:\s*([^\n]+)", answer)

    # 6. ê°’ ì¶”ì¶œ & ìµœì¢… ë³´ì •
    one_line = m1.group(1).strip() if (m1 and do_summary) else "ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨"
    keywords = m2.group(1).strip() if m2 else ""
    sentiment = ''
    if m3:
        sentiment = m3.group(1).strip()
        # ì˜ë¬¸ ì‘ë‹µì„ í•œê¸€ë¡œ í†µì¼
        if sentiment.lower() == 'positive':
            sentiment = 'ê¸ì •'
        elif sentiment.lower() == 'negative':
            sentiment = 'ë¶€ì •'
        elif sentiment not in ['ê¸ì •', 'ë¶€ì •']:
            sentiment = 'ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨'
    else:
        sentiment = 'ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨'

    # 7. ëˆ„ë½, ë¹ˆê°’ ë³´ì • (ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜ ì ˆëŒ€ ë°©ì§€)
    if not one_line or one_line.lower() in ["none", ""]:
        one_line = "ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨"
    if not sentiment or sentiment.lower() in ["none", "ì¤‘ë¦½", "neutral", ""]:
        sentiment = "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨"
    if not keywords or keywords.lower() in ["none", "ì—†ìŒ"]:
        keywords = ""

    return one_line, keywords, sentiment, text

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)  # ì´ë¯¸ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜
        self.chat_id = TELEGRAM_CHAT_ID
    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)
        
def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        for item in items:
            title = html.unescape(re.sub("<.*?>", "", item["title"]))
            desc  = html.unescape(re.sub("<.*?>", "", item["description"]))
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()

            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
                continue

            source = item.get("source") or infer_source_from_url(item.get("originallink", "")) or "Naver"
            source_domain = source.lower()
            if source_domain.startswith("www."):
                source_domain = source_domain[4:]

            real_link = item.get("originallink") or item["link"]

            articles.append({
                "title": title,
                "description": desc,  # í˜¹ì‹œ ì—‘ì…€ì— ì„¤ëª…ë„ ì“¸ ê²½ìš° ëŒ€ë¹„
                "link": real_link,
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": source_domain
            })

        if len(items) < 100:
            break
    return articles[:limit]

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def summarize_article_from_url(article_url, title, do_summary=True):
    cache_key_base = re.sub(r"\W+", "", article_url)[-16:]
    summary_key = f"summary_{cache_key_base}"

    # ì´ë¯¸ ìš”ì•½ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if summary_key in st.session_state:
        return st.session_state[summary_key]

    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            result = (full_text, None, None, None)
        else:
            one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text, do_summary=do_summary)
            result = (one_line, summary, sentiment, full_text)
    except Exception as e:
        result = (f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None)

    # ìºì‹œì— ì €ì¥
    st.session_state[summary_key] = result
    return result

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    title = article.get("title", "")
    content = ""
    cache_key = article.get("link", "")
    summary_cache_key = None
    for key in st.session_state.keys():
        if key.startswith("summary_") and cache_key in key:
            summary_cache_key = key
            break
    if summary_cache_key and isinstance(st.session_state[summary_cache_key], tuple):
        _, _, _, content = st.session_state[summary_cache_key]
    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

def article_passes_all_filters(article):
    # ì œëª©ì— ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ì œì™¸
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False

    # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
    try:
        pub_date = datetime.strptime(article['date'], '%Y-%m-%d').date()
        if pub_date < st.session_state.get("start_date") or pub_date > st.session_state.get("end_date"):
            return False
    except:
        return False

    # í‚¤ì›Œë“œ í•„í„°: ì…ë ¥ í‚¤ì›Œë“œ ë° ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì§‘í•© ì¤€ë¹„
    all_keywords = []
    if "keyword_input" in st.session_state:
        all_keywords.extend([k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()])
    if "cat_multi" in st.session_state:
        for cat in st.session_state["cat_multi"]:
            all_keywords.extend(favorite_categories.get(cat, []))

    # í‚¤ì›Œë“œ í•„í„°(ì…ë ¥ ë° ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ) í†µê³¼ ì—¬ë¶€
    keyword_passed = article_contains_exact_keyword(article, all_keywords)

    # ì–¸ë¡ ì‚¬ ë„ë©”ì¸ í•„í„°ë§ (íŠ¹ì • ì–¸ë¡ ì‚¬ë§Œ í•„í„°ë§)
    if st.session_state.get("filter_allowed_sources_only", True):
        source = article.get('source', '').lower()
        if source.startswith("www."):
            source = source[4:]
        if source not in ALLOWED_SOURCES:
            return False

    # ê³µí†µ í•„í„° ì¡°ê±´ (AND ì¡°ê±´, ì¦‰ ë°˜ë“œì‹œ í†µê³¼í•´ì•¼ í•¨)
    common_passed = or_keyword_filter(article, ALL_COMMON_FILTER_KEYWORDS)
    if not common_passed:
        return False

    # ì‚°ì—…ë³„ í•„í„° ì¡°ê±´ (OR ì¡°ê±´)
    industry_passed = True
    if st.session_state.get("use_industry_filter", False):
        keyword = article.get("í‚¤ì›Œë“œ")  # íšŒì‚¬ëª… ë˜ëŠ” í‚¤ì›Œë“œ í•­ëª©ëª…
        matched_major = None
        for cat, companies in favorite_categories.items():
            if keyword in companies:
                majors = get_industry_majors_from_favorites([cat])
                if majors:
                    matched_major = majors[0]
                    break
        if matched_major:
            sub_keyword_filter = st.session_state.industry_major_sub_map.get(matched_major, [])
            if sub_keyword_filter:
                industry_passed = or_keyword_filter(article, sub_keyword_filter)

    # ìµœì¢… í•„í„°ë§: ê³µí†µ í•„í„°ëŠ” ë°˜ë“œì‹œ í†µê³¼í•˜ê³ ,
    # ì‚°ì—…ë³„ í•„í„°ë‚˜ í‚¤ì›Œë“œ í•„í„° ì¤‘ í•˜ë‚˜ë¼ë„ í†µê³¼í•˜ë©´ í†µê³¼
    if not (industry_passed or keyword_passed):
        return False

    return True

# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° í•¨ìˆ˜ ---
def is_similar(title1, title2, threshold=0.5):
    ratio = difflib.SequenceMatcher(None, title1, title2).ratio()
    return ratio >= threshold

def remove_duplicates(articles):
    unique_articles = []
    titles = []
    for article in articles:
        title = article.get("title", "")
        if all(not is_similar(title, existing_title) for existing_title in titles):
            unique_articles.append(article)
            titles.append(title)
    return unique_articles

# í•­ìƒ ë¨¼ì € ì„ ì–¸í•´ ì—ëŸ¬ ë°©ì§€
keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()] if keywords_input else []
search_clicked = False

if keyword_list:
        search_clicked = True

if keyword_list and (search_clicked or st.session_state.get("search_triggered")):
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        # ë™ì˜ì–´ í™•ì¥
        expanded = expand_keywords_with_synonyms(sorted(keyword_list))
        process_keywords_with_synonyms(
            expanded,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )
    st.session_state.search_triggered = False


if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])

        expanded = expand_keywords_with_synonyms(sorted(keywords))
        process_keywords_with_synonyms(
            expanded,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def clean_excel_formula_text(text):
    """ì—‘ì…€ ìˆ˜ì‹(HYPERLINK)ì—ì„œ ê¹¨ì§ ë°©ì§€ìš© ì „ì²˜ë¦¬"""
    if not isinstance(text, str):  # Noneì´ë‚˜ ìˆ«ìì´ë©´ ë¬¸ì ë³€í™˜
        text = str(text)
    text = text.replace('"', "'")   # í°ë”°ì˜´í‘œ â†’ í™‘ë”°ì˜´í‘œ
    text = text.replace('\n', ' ')  # ì¤„ë°”ê¿ˆ â†’ ê³µë°±
    text = text.replace('\r', '')
    return text[:250]  # ì•ˆì „í•˜ê²Œ 255ì ë¯¸ë§Œìœ¼ë¡œ ì œí•œ

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories, search_results):
    company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        company_order.extend(favorite_categories.get(cat, []))
    excel_company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        excel_company_order.extend(excel_company_categories.get(cat, []))

    df_articles = pd.DataFrame(summary_data)

    if "í‚¤ì›Œë“œ" not in df_articles.columns:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            pd.DataFrame(columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ê¸ì • ë‰´ìŠ¤", "ë¶€ì • ë‰´ìŠ¤"]).to_excel(writer, index=False)
        output.seek(0)
        return output

    result_rows = []
    for idx, company in enumerate(company_order):
        excel_company_name = excel_company_order[idx] if idx < len(excel_company_order) else ""

        filtered_articles = [
            a for a in search_results.get(company, [])
            if article_passes_all_filters(a)
        ]
        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)
        total_count = len(filtered_articles)

        comp_articles = df_articles[df_articles["í‚¤ì›Œë“œ"] == company]
        pos_news = comp_articles[comp_articles["ê°ì„±"] == "ê¸ì •"].sort_values(by="ë‚ ì§œ", ascending=False)
        neg_news = comp_articles[comp_articles["ê°ì„±"] == "ë¶€ì •"].sort_values(by="ë‚ ì§œ", ascending=False)

        if not pos_news.empty:
            pos_date = clean_excel_formula_text(pos_news.iloc[0]["ë‚ ì§œ"])
            pos_title = clean_excel_formula_text(pos_news.iloc[0]["ê¸°ì‚¬ì œëª©"])
            pos_link = clean_excel_formula_text(pos_news.iloc[0]["ë§í¬"])
            pos_display = f'({pos_date}) {pos_title}'
            pos_hyperlink = f'=HYPERLINK("{pos_link}", "{pos_display}")'
        else:
            pos_hyperlink = ""

        if not neg_news.empty:
            neg_date = clean_excel_formula_text(neg_news.iloc[0]["ë‚ ì§œ"])
            neg_title = clean_excel_formula_text(neg_news.iloc[0]["ê¸°ì‚¬ì œëª©"])
            neg_link = clean_excel_formula_text(neg_news.iloc[0]["ë§í¬"])
            neg_display = f'({neg_date}) {neg_title}'
            neg_hyperlink = f'=HYPERLINK("{neg_link}", "{neg_display}")'
        else:
            neg_hyperlink = ""

        result_rows.append({
            "ê¸°ì—…ëª…": company,
            "í‘œê¸°ëª…": excel_company_name,
            "ê±´ìˆ˜": total_count,
            "ê¸ì • ë‰´ìŠ¤": pos_hyperlink,
            "ë¶€ì • ë‰´ìŠ¤": neg_hyperlink
        })

    df_result = pd.DataFrame(result_rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_result.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
    output.seek(0)
    return output

def generate_important_article_list(search_results, common_keywords, industry_keywords, favorites):
    import os
    from openai import OpenAI
    import re

    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    client = OpenAI(api_key=OPENAI_API_KEY)
    result = []

    for category, companies in favorites.items():
        for comp in companies:
            articles = search_results.get(comp, [])
            filtered_keywords = list(set(common_keywords + industry_keywords))
            target_articles = [a for a in articles if any(kw in a["title"] for kw in filtered_keywords)]

            if not target_articles:
                continue

            prompt_list = "\n".join([f"{i+1}. {a['title']} - {a['link']}" for i, a in enumerate(target_articles)])

            # â—¾ï¸ ì—¬ê¸°ì— ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ì ìš©
            prompt = (
                f"[ê¸°ì‚¬ ëª©ë¡]\n{prompt_list}\n\n"
                "1. ê° ê¸°ì‚¬ ë‚´ìš©ì„ ì½ê³ , ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ ê°ì„± í†¤(ê¸ì •/ë¶€ì •)ì„ íŒë‹¨í•´ ì£¼ì„¸ìš”.\n"
                "   - ë§Œì•½ ê¸ì •ê³¼ ë¶€ì •ì´ í˜¼ì¬ëœ ê²½ìš°, ê¸°ì‚¬ì˜ ì „ì²´ì ì¸ ë¶„ìœ„ê¸°ì—ì„œ ìš°ì„¸í•œ ê°ì„± í†¤ì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.\n\n"
                "2. ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ê¸°ì—…ì˜ ì±„ê¶Œ íˆ¬ìì ì…ì¥ì—ì„œ íŒë‹¨í•´,\n"
                "   2.1 ì¬ë¬´ ì•ˆì •ì„±, í˜„ê¸ˆì°½ì¶œë ¥, ì‹¤ì  ê°œì„ , ë¦¬ìŠ¤í¬ ì™„í™” ì—¬ë¶€, ê¸ì •ì  ì „ë§ì— ê¸ì •ì ìœ¼ë¡œ ê¸°ì—¬í•˜ëŠ” í•µì‹¬ ê¸ì • ê¸°ì‚¬ 1ê±´\n"
                "   2.2 ìˆ˜ìµì„± ì €í•˜, ë¦¬ìŠ¤í¬ í™•ëŒ€, ë¶€ì •ì  ì „ë§ì— í•´ë‹¹í•˜ëŠ” í•µì‹¬ ë¶€ì • ê¸°ì‚¬ 1ê±´ì„ ê°ê° ì„ ì •í•´ì£¼ì„¸ìš”.\n"
                "3. ê°ì„± ë¶„ë¥˜(ê¸ì •/ë¶€ì •)ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ê³µë€ìœ¼ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.\n\n"
                "[ê¸ì •]: (ê¸ì •ì  ì„ ì • ê¸°ì‚¬ ì œëª©)\n[ë¶€ì •]: (ë¶€ì •ì  ì„ ì • ê¸°ì‚¬ ì œëª©)"
            )

            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=800,
                    temperature=0.3
                )
                answer = response.choices[0].message.content.strip()
                pos_title = re.search(r"\[ê¸ì •\]:\s*(.+)", answer)
                neg_title = re.search(r"\[ë¶€ì •\]:\s*(.+)", answer)
                pos_title = pos_title.group(1).strip() if pos_title else ""
                neg_title = neg_title.group(1).strip() if neg_title else ""

                for a in target_articles:
                    if pos_title and pos_title in a["title"]:
                        result.append({
                            "íšŒì‚¬ëª…": comp,
                            "ê°ì„±": "ê¸ì •",
                            "ì œëª©": a["title"],
                            "ë§í¬": a["link"],
                            "ë‚ ì§œ": a["date"],
                            "ì¶œì²˜": a["source"]
                        })
                    if neg_title and neg_title in a["title"]:
                        result.append({
                            "íšŒì‚¬ëª…": comp,
                            "ê°ì„±": "ë¶€ì •",
                            "ì œëª©": a["title"],
                            "ë§í¬": a["link"],
                            "ë‚ ì§œ": a["date"],
                            "ì¶œì²˜": a["source"]
                        })
            except Exception:
                continue
    return result

def extract_article_text(url):
    # ë„¤ì´ë²„, ë‹¤ìŒ ë“± í¬í„¸ ë‰´ìŠ¤ ì¤‘ê³„ URLì€ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - ë°”ë¡œ ì˜¤ë¥˜ ë°˜í™˜
    PORTAL_DOMAINS = ["news.naver.com", "n.news.naver.com", "news.daum.net"]
    if any(domain in url for domain in PORTAL_DOMAINS):
        return "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: í¬í„¸ ë‰´ìŠ¤ ì¤‘ê³„ URLì…ë‹ˆë‹¤. 'originallink'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    try:
        article = newspaper.Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"
    
def extract_keyword_from_link(search_results, article_link):
    """
    ë‰´ìŠ¤ê²€ìƒ‰ê²°ê³¼ dictì™€ ê¸°ì‚¬ ë§í¬ë¡œ í•´ë‹¹ ê¸°ì‚¬ì˜ í‚¤ì›Œë“œ(íšŒì‚¬ëª…/ì¹´í…Œê³ ë¦¬)ë¥¼ ì¶”ì¶œ
    """
    for kw, arts in search_results.items():
        for art in arts:
            if art.get("link") == article_link:
                return kw
    return ""

def build_important_excel_same_format(important_articles, favorite_categories, excel_company_categories, search_results):
    """
    ì¤‘ìš”ê¸°ì‚¬ ëª©ë¡ì„ 'ë§ì¶¤ ì–‘ì‹' ì—‘ì…€ íŒŒì¼ë¡œ ìƒì„±í•˜ì—¬ BytesIO ë°˜í™˜
    """
    # DataFrame ìƒì„±
    df = pd.DataFrame(important_articles)

    # ë§ì¶¤ ì—´ ëª©ë¡ ì§€ì •
    columns = ["ì‚°ì—…ëŒ€ë¶„ë¥˜", "ì‚°ì—…ì†Œë¶„ë¥˜", "íšŒì‚¬ëª…", "ê°ì„±", "ì œëª©", "ë§í¬", "ë‚ ì§œ", "ì¶œì²˜"]
    for col in columns:
        if col not in df.columns:
            df[col] = ""

    # ì‚°ì—…ë¶„ë¥˜/ì†Œë¶„ë¥˜ ìë™ ì±„ìš°ê¸°
    for idx, row in df.iterrows():
        company = row["íšŒì‚¬ëª…"]
        sub_cat, major_cat = "", ""
        # ì†Œë¶„ë¥˜ ì°¾ê¸°
        for sub, comps in favorite_categories.items():
            if company in comps:
                sub_cat = sub
                break
        # ëŒ€ë¶„ë¥˜ ì°¾ê¸°
        for major, subs in excel_company_categories.items():
            if sub_cat in subs:
                major_cat = major
                break
        df.at[idx, "ì‚°ì—…ëŒ€ë¶„ë¥˜"] = major_cat
        df.at[idx, "ì‚°ì—…ì†Œë¶„ë¥˜"] = sub_cat

    # ë‚ ì§œ í¬ë§· ë³€í™˜
    if "ë‚ ì§œ" in df.columns:
        try:
            df["ë‚ ì§œ"] = pd.to_datetime(df["ë‚ ì§œ"]).dt.strftime("%Y-%m-%d")
        except:
            pass

    # ìµœì¢… ì—´ ìˆœì„œ
    df = df[columns]

    # ì—‘ì…€ ìƒì„± ë° ìŠ¤íƒ€ì¼ ì§€ì •
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="ì¤‘ìš”ë‰´ìŠ¤")
        workbook = writer.book
        worksheet = writer.sheets["ì¤‘ìš”ë‰´ìŠ¤"]

        header_format = workbook.add_format({
            "bold": True, "bg_color": "#DCE6F1", "border": 1,
            "align": "center", "valign": "vcenter"
        })
        for col_num, value in enumerate(df.columns.values):
            worksheet.write(0, col_num, value, header_format)
            worksheet.set_column(col_num, col_num, 20)

    output.seek(0)
    return output
   
def matched_filter_keywords(article, common_keywords, industry_keywords):
    """
    ê¸°ì‚¬ ì œëª©/ìš”ì•½/ë³¸ë¬¸ì—ì„œ ì‹¤ì œë¡œ í¬í•¨ëœ í•„í„° í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    text_candidates = [
        article.get("title", ""),
        article.get("description", ""),
        article.get("ìš”ì•½ë³¸", ""),
        article.get("ìš”ì•½", ""),
        article.get("full_text", ""),
        article.get("content", ""),
    ]
    text_long = " ".join([str(t) for t in text_candidates if t])
    matched_common = [kw for kw in common_keywords if kw in text_long]
    matched_industry = [kw for kw in industry_keywords if kw in text_long]
    return list(set(matched_common + matched_industry))


def render_articles_with_single_summary_and_telegram(
    results, show_limit, show_sentiment_badge=True, enable_summary=True
):
    SENTIMENT_CLASS = {"ê¸ì •": "sentiment-positive", "ë¶€ì •": "sentiment-negative"}
    col_list, col_summary = st.columns([1, 1])

    # ---------------------------- ë‰´ìŠ¤ ëª©ë¡ ì—´ ----------------------------
    with col_list:
        st.markdown("### ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼")
        for keyword, articles in results.items():
            # âœ… ì ‘ì—ˆë‹¤/í¼ì¹  ìˆ˜ ìˆëŠ” expanderë¡œ ë³€ê²½
            with st.expander(f"[{keyword}] ({len(articles)}ê±´)", expanded=True):
                # ì „ì²´ ì„ íƒ/í•´ì œ ì²´í¬ë°•ìŠ¤
                all_article_keys = []
                for idx, article in enumerate(articles):
                    uid = re.sub(r"\W+", "", article["link"])[-16:]
                    key = f"{keyword}_{idx}_{uid}"
                    all_article_keys.append(key)

                prev_value = all(st.session_state.article_checked.get(k, False) for k in all_article_keys)
                # í˜„ì¬ ìƒíƒœ(ìœ ì €ê°€ ì‹¤ì œë¡œ í´ë¦­í•œ í›„ì˜ ê°’)
                select_all = st.checkbox(
                    f"ì „ì²´ ê¸°ì‚¬ ì„ íƒ/í•´ì œ ({keyword})",
                    value=prev_value,
                    key=f"{keyword}_select_all"
                )
                # í´ë¦­ ë³€í™” ê°ì§€ â€” í•œ ë²ˆì˜ í´ë¦­ì— ì¦‰ì‹œ ì²˜ë¦¬!
                if select_all != prev_value:
                    for k in all_article_keys:
                        st.session_state.article_checked[k] = select_all
                        st.session_state.article_checked_left[k] = select_all
                    st.rerun()  # ì¦‰ì‹œ ë¦¬ë Œë”ë§

                # ê°œë³„ ê¸°ì‚¬ ì²´í¬ë°•ìŠ¤
                for idx, article in enumerate(articles):
                    uid = re.sub(r"\W+", "", article["link"])[-16:]
                    key = f"{keyword}_{idx}_{uid}"
                    cache_key = f"summary_{key}"
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox(
                            "",
                            value=st.session_state.article_checked.get(key, False),
                            key=f"news_{key}",
                        )
                    with cols[1]:
                        sentiment = ""
                        if show_sentiment_badge and cache_key in st.session_state:
                            _, _, sentiment, _ = st.session_state[cache_key]
                        badge_html = (
                            f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>{sentiment}</span>"
                            if sentiment else ""
                        )
                        st.markdown(
                            f"<span class='news-title'><a href='{article['link']}' target='_blank'>{article['title']}</a></span> "
                            f"{badge_html} {article['date']} | {article['source']}",
                            unsafe_allow_html=True,
                        )
                    st.session_state.article_checked_left[key] = checked
                    st.session_state.article_checked[key] = checked

    # ---------------------------- ì„ íƒ ê¸°ì‚¬ ìš”ì•½ ì—´ ----------------------------
    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):

            # 1) í˜„ì¬ ì„ íƒëœ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
            selected_to_process = []
            industry_keywords_all = []
            if st.session_state.get("use_industry_filter", False):
                for sublist in st.session_state.industry_major_sub_map.values():
                    industry_keywords_all.extend(sublist)

            for keyword, articles in results.items():
                for idx, article in enumerate(articles):
                    uid = re.sub(r"\W+", "", article["link"])[-16:]
                    key = f"{keyword}_{idx}_{uid}"
                    if st.session_state.article_checked.get(key, False):
                        selected_to_process.append((keyword, idx, article))

            # 2) ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìš”ì•½/ê°ì„±ë¶„ì„
            def process_article(item):
                keyword, idx, art = item
                cache_key = f"summary_{keyword}_{idx}_" + re.sub(r"\W+", "", art["link"])[-16:]
                if cache_key in st.session_state:
                    one_line, summary, sentiment, full_text = st.session_state[cache_key]
                else:
                    one_line, summary, sentiment, full_text = summarize_article_from_url(
                        art["link"], art["title"], do_summary=enable_summary
                    )
                    st.session_state[cache_key] = (one_line, summary, sentiment, full_text)

                filter_hits = matched_filter_keywords(
                    {"title": art["title"], "ìš”ì•½ë³¸": summary, "ìš”ì•½": one_line, "full_text": full_text},
                    ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords_all
                )
                return {
                    "í‚¤ì›Œë“œ": keyword,
                    "í•„í„°íˆíŠ¸": ", ".join(filter_hits),
                    "ê¸°ì‚¬ì œëª©": safe_title(art["title"]),
                    "ìš”ì•½": one_line,
                    "ìš”ì•½ë³¸": summary,
                    "ê°ì„±": sentiment,
                    "ë§í¬": art["link"],
                    "ë‚ ì§œ": art["date"],
                    "ì¶œì²˜": art["source"],
                    "full_text": full_text or "",
                }

            from concurrent.futures import ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=10) as executor:
                selected_articles = list(executor.map(process_article, selected_to_process))

            # 3) ì „ì²´ ê²°ê³¼ ë Œë”ë§
            for art in selected_articles:
                st.markdown(
                    f"#### <span class='news-title'><a href='{art['ë§í¬']}' target='_blank'>{art['ê¸°ì‚¬ì œëª©']}</a></span> "
                    f"<span class='sentiment-badge {SENTIMENT_CLASS.get(art['ê°ì„±'], 'sentiment-negative')}'>{art['ê°ì„±']}</span>",
                    unsafe_allow_html=True,
                )
                st.markdown(f"- **ê²€ìƒ‰ í‚¤ì›Œë“œ:** `{art['í‚¤ì›Œë“œ']}`")
                st.markdown(f"- **í•„í„°ë¡œ ì¸ì‹ëœ í‚¤ì›Œë“œ:** `{art['í•„í„°íˆíŠ¸'] or 'ì—†ìŒ'}`")
                st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {art['ë‚ ì§œ']} | {art['ì¶œì²˜']}")
                if enable_summary:
                    st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {art['ìš”ì•½']}")
                st.markdown(f"- **ê°ì„±ë¶„ì„:** `{art['ê°ì„±']}`")
                st.markdown("---")

            st.session_state.selected_articles = selected_articles
            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(selected_articles)}")

            # ë‹¤ìš´ë¡œë“œ / ì „ì²´ í•´ì œ ë²„íŠ¼
            col_dl1, col_dl2 = st.columns([0.55, 0.45])
            with col_dl1:
                st.download_button(
                    label="ğŸ“¥ ë§ì¶¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=get_excel_download_with_favorite_and_excel_company_col(
                        selected_articles, favorite_categories, excel_company_categories,
                        st.session_state.search_results
                    ).getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë§ì¶¤í˜•.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )

            with col_dl2:
                if st.button("ğŸ—‘ ì„ íƒ í•´ì œ (ì „ì²´)"):
                    for key in list(st.session_state.article_checked.keys()):
                        st.session_state.article_checked[key] = False
                    for key in list(st.session_state.article_checked_left.keys()):
                        st.session_state.article_checked_left[key] = False
                    st.rerun()

        render_important_article_review_and_download()

def render_important_article_review_and_download():
    with st.container(border=True):
        st.markdown("### â­ ì¤‘ìš” ê¸°ì‚¬ ë¦¬ë·° ë° í¸ì§‘")

        # ì¤‘ìš”ê¸°ì‚¬ ìë™ì„ ì • ë²„íŠ¼
        auto_btn = st.button("ğŸš€ OpenAI ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ìë™ ì„ ì •")
        if auto_btn:
            with st.spinner("OpenAIë¡œ ì¤‘ìš” ë‰´ìŠ¤ ì„ ì • ì¤‘..."):
                filtered_results_for_important = {}
                for keyword, articles in st.session_state.search_results.items():
                    filtered_articles = [a for a in articles if article_passes_all_filters(a)]
                    if st.session_state.get("remove_duplicate_articles", False):
                        filtered_articles = remove_duplicates(filtered_articles)
                    if filtered_articles:
                        filtered_results_for_important[keyword] = filtered_articles

                important_articles = generate_important_article_list(
                    search_results=filtered_results_for_important,
                    common_keywords=ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords=st.session_state.get("industry_sub", []),
                    favorites=favorite_categories
                )
                # key naming í†µì¼
                for i, art in enumerate(important_articles):
                    important_articles[i] = {
                        "í‚¤ì›Œë“œ": art.get("í‚¤ì›Œë“œ") or art.get("íšŒì‚¬ëª…") or art.get("keyword") or "",
                        "ê¸°ì‚¬ì œëª©": art.get("ê¸°ì‚¬ì œëª©") or art.get("ì œëª©") or art.get("title") or "",
                        "ê°ì„±": art.get("ê°ì„±", ""),
                        "ë§í¬": art.get("ë§í¬") or art.get("link", ""),
                        "ë‚ ì§œ": art.get("ë‚ ì§œ") or art.get("date", ""),
                        "ì¶œì²˜": art.get("ì¶œì²˜") or art.get("source", "")
                    }

                st.session_state["important_articles_preview"] = important_articles
                st.session_state["important_selected_index"] = []

        articles = st.session_state.get("important_articles_preview", [])
        selected_indexes = st.session_state.get("important_selected_index", [])

        st.markdown("ğŸ¯ **ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡** (êµì²´ ë˜ëŠ” ì‚­ì œí•  í•­ëª©ì„ ì²´í¬í•˜ì„¸ìš”)")

        # ====== ë³‘ë ¬ ìš”ì•½ ì¤€ë¹„ ======
        from concurrent.futures import ThreadPoolExecutor
        one_line_map = {}
        to_summarize = []

        for idx, article in enumerate(articles):
            link = article.get("ë§í¬", "")
            cleaned_id = re.sub(r"\W+", "", link)[-16:] if link else ""
            in_cache = False
            for k, v in st.session_state.items():
                if k.startswith("summary_") and cleaned_id in k and isinstance(v, tuple):
                    one_line_map[idx] = v[0]
                    in_cache = True
                    break
            if not in_cache and link:
                to_summarize.append((idx, link, article.get("ê¸°ì‚¬ì œëª©", "")))

        if to_summarize:
            with st.spinner("ì¤‘ìš” ê¸°ì‚¬ ìš”ì•½ ìƒì„± ì¤‘..."):
                def get_one_line(args):
                    idx, link, title = args
                    one_line, _, _, _ = summarize_article_from_url(link, title, do_summary=True)
                    return idx, one_line

                with ThreadPoolExecutor(max_workers=10) as executor:
                    for idx, one_line in executor.map(get_one_line, to_summarize):
                        one_line_map[idx] = one_line
        # ====== ë³‘ë ¬ ìš”ì•½ ì™„ë£Œ ======

        new_selection = []
        for idx, article in enumerate(articles):
            checked = st.checkbox(
                f"{article.get('í‚¤ì›Œë“œ', '')} | {article.get('ê°ì„±', '')} | {article.get('ê¸°ì‚¬ì œëª©', '')}",
                key=f"important_chk_{idx}",
                value=(idx in selected_indexes)
            )
            if idx in one_line_map and one_line_map[idx]:
                st.markdown(
                    f"<span style='color:gray;font-style:italic;'>{one_line_map[idx]}</span>",
                    unsafe_allow_html=True
                )
            if checked:
                new_selection.append(idx)

        st.session_state["important_selected_index"] = new_selection
        st.markdown("---")

        col_add, col_del, col_rep = st.columns([0.3, 0.35, 0.35])
        # â• ì„ íƒ ê¸°ì‚¬ ì¶”ê°€
        with col_add:
            if st.button("â• ì„ íƒ ê¸°ì‚¬ ì¶”ê°€"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                if not left_selected_keys:
                    st.warning("ì™¼ìª½ ë‰´ìŠ¤ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì ì–´ë„ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")
                else:
                    added_count = 0
                    important = st.session_state.get("important_articles_preview", [])
                    for from_key in left_selected_keys:
                        m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                        if not m:
                            continue
                        key_tail = m.group(1)
                        selected_article, article_link = None, None
                        for kw, arts in st.session_state.search_results.items():
                            for art in arts:
                                uid = re.sub(r'\W+', '', art['link'])[-16:]
                                if uid == key_tail:
                                    selected_article = art
                                    article_link = art["link"]
                                    break
                            if selected_article:
                                break
                        if not selected_article:
                            continue

                        keyword = extract_keyword_from_link(st.session_state.search_results, article_link)
                        cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                        sentiment = None
                        for k in st.session_state.keys():
                            if k.startswith("summary_") and cleaned_id in k:
                                sentiment = st.session_state[k][2]
                                break
                        if not sentiment:
                            _, _, sentiment, _ = summarize_article_from_url(
                                selected_article["link"], selected_article["title"]
                            )

                        new_article = {
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": selected_article["title"],
                            "ê°ì„±": sentiment or "",
                            "ë§í¬": selected_article["link"],
                            "ë‚ ì§œ": selected_article["date"],
                            "ì¶œì²˜": selected_article["source"]
                        }
                        if not any(a["ë§í¬"] == new_article["ë§í¬"] for a in important):
                            important.append(new_article)
                            added_count += 1
                        st.session_state.article_checked_left[from_key] = False
                        st.session_state.article_checked[from_key] = False

                    st.session_state["important_articles_preview"] = important
                    if added_count > 0:
                        st.success(f"{added_count}ê±´ì˜ ê¸°ì‚¬ê°€ ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.info("ì¶”ê°€ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.rerun()

        # ğŸ—‘ ì„ íƒ ê¸°ì‚¬ ì‚­ì œ
        with col_del:
            if st.button("ğŸ—‘ ì„ íƒ ê¸°ì‚¬ ì‚­ì œ"):
                important = st.session_state.get("important_articles_preview", [])
                for idx in sorted(st.session_state["important_selected_index"], reverse=True):
                    if 0 <= idx < len(important):
                        important.pop(idx)
                st.session_state["important_articles_preview"] = important
                st.session_state["important_selected_index"] = []
                st.rerun()

        # ğŸ” ì„ íƒ ê¸°ì‚¬ êµì²´
        with col_rep:
            if st.button("ğŸ” ì„ íƒ ê¸°ì‚¬ êµì²´"):
                left_selected_keys = [k for k, v in st.session_state.article_checked_left.items() if v]
                right_selected_indexes = st.session_state["important_selected_index"]
                if len(left_selected_keys) != 1 or len(right_selected_indexes) != 1:
                    st.warning("ì™¼ìª½ 1ê°œ, ì˜¤ë¥¸ìª½ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    return
                from_key = left_selected_keys[0]
                target_idx = right_selected_indexes[0]
                m = re.match(r"^[^_]+_[0-9]+_(.+)$", from_key)
                if not m:
                    st.warning("ê¸°ì‚¬ ì‹ë³„ì íŒŒì‹± ì‹¤íŒ¨")
                    return
                key_tail = m.group(1)
                selected_article, article_link = None, None
                for kw, art_list in st.session_state.search_results.items():
                    for art in art_list:
                        uid = re.sub(r'\W+', '', art['link'])[-16:]
                        if uid == key_tail:
                            selected_article = art
                            article_link = art["link"]
                            break
                    if selected_article:
                        break
                if not selected_article:
                    st.warning("ì™¼ìª½ì—ì„œ ì„ íƒí•œ ê¸°ì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return

                keyword = extract_keyword_from_link(st.session_state.search_results, article_link)
                cleaned_id = re.sub(r'\W+', '', selected_article['link'])[-16:]
                sentiment = None
                for k in st.session_state.keys():
                    if k.startswith("summary_") and cleaned_id in k:
                        sentiment = st.session_state[k][2]
                        break
                if not sentiment:
                    _, _, sentiment, _ = summarize_article_from_url(
                        selected_article["link"], selected_article["title"]
                    )

                new_article = {
                    "í‚¤ì›Œë“œ": keyword,
                    "ê¸°ì‚¬ì œëª©": selected_article["title"],
                    "ê°ì„±": sentiment or "",
                    "ë§í¬": selected_article["link"],
                    "ë‚ ì§œ": selected_article["date"],
                    "ì¶œì²˜": selected_article["source"]
                }
                st.session_state["important_articles_preview"][target_idx] = new_article
                st.session_state.article_checked_left[from_key] = False
                st.session_state.article_checked[from_key] = False
                st.session_state["important_selected_index"] = []
                st.success("ì¤‘ìš” ê¸°ì‚¬ êµì²´ ì™„ë£Œ")
                st.rerun()

        # --- ë§ì¶¤ ì–‘ì‹ ë™ì¼ í¬ë§· ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ---
        st.markdown("---")
        st.markdown("ğŸ“¥ **ë¦¬ë·°í•œ ì¤‘ìš” ê¸°ì‚¬ë“¤ì„ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.**")

        final_selected_indexes = st.session_state.get("important_selected_index", [])
        articles_source = st.session_state.get("important_articles_preview", [])

        # ì‚°ì—… í‚¤ì›Œë“œ ì „ì²´ ìˆ˜ì§‘ (í•„í„°ìš©)
        industry_keywords_all = []
        if st.session_state.get("use_industry_filter", False):
            for sublist in st.session_state.industry_major_sub_map.values():
                industry_keywords_all.extend(sublist)
        
        def enrich_article_for_excel(raw_article):
            link = raw_article.get("ë§í¬", "")
            keyword = raw_article.get("í‚¤ì›Œë“œ", "")
            cleaned_id = re.sub(r"\W+", "", link)[-16:]
            sentiment, one_line, summary, full_text = None, "", "", ""

            # ìºì‹œì—ì„œ ìš”ì•½/ê°ì„± êº¼ë‚´ì˜¤ê¸°
            for k, v in st.session_state.items():
                if k.startswith("summary_") and cleaned_id in k and isinstance(v, tuple):
                    one_line, summary, sentiment, full_text = v
                    break
            # ì—†ìœ¼ë©´ ì§ì ‘ ë¶„ì„
            if not sentiment:
                one_line, summary, sentiment, full_text = summarize_article_from_url(
                    link, raw_article.get("ê¸°ì‚¬ì œëª©", "")
                )

            filter_hits = matched_filter_keywords(
                {"title": raw_article.get("ê¸°ì‚¬ì œëª©", ""), "ìš”ì•½ë³¸": summary,
                 "ìš”ì•½": one_line, "full_text": full_text},
                ALL_COMMON_FILTER_KEYWORDS,
                industry_keywords_all
            )
            return {
                "í‚¤ì›Œë“œ": keyword,
                "í•„í„°íˆíŠ¸": ", ".join(filter_hits),
                "ê¸°ì‚¬ì œëª©": safe_title(raw_article.get("ê¸°ì‚¬ì œëª©", "")),
                "ìš”ì•½": one_line,
                "ìš”ì•½ë³¸": summary,
                "ê°ì„±": sentiment,
                "ë§í¬": link,
                "ë‚ ì§œ": raw_article.get("ë‚ ì§œ", ""),
                "ì¶œì²˜": raw_article.get("ì¶œì²˜", ""),
                "full_text": full_text or "",
            }
        
        # âœ… ëª¨ë“  'ì¤‘ìš” ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸'ë¥¼ ì—‘ì…€ summary_data êµ¬ì¡°ë¡œ ë³€í™˜
        #    â†’ ì„ íƒ/ë¹„ì„ íƒê³¼ ê´€ê³„ì—†ì´ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥
        summary_data = [enrich_article_for_excel(a) for a in articles_source]

        # ğŸ”¹ favorite_categories / excel_company_categories ìˆœì„œì— ë§ì¶° ëª¨ë“  ê¸°ì—… ì¶œë ¥
        excel_data = get_excel_download_with_favorite_and_excel_company_col(
            summary_data,
            favorite_categories,
            excel_company_categories,
            st.session_state.search_results
        )

        st.download_button(
            label="ğŸ“¥ ì¤‘ìš” ê¸°ì‚¬ ìµœì¢… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ë§ì¶¤ ì–‘ì‹)",
            data=excel_data.getvalue(),
            file_name=f"ì¤‘ìš”ë‰´ìŠ¤_ìµœì¢…ì„ ì •_ì–‘ì‹_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        
        # --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì²˜ë¦¬ ---
        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)
        
        if filtered_articles:
            filtered_results[keyword] = filtered_articles

    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )
