import os
import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
from datetime import datetime, timedelta
import telepot
from openai import OpenAI
import newspaper
import difflib
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import html
import json
from bs4 import BeautifulSoup
import hashlib

# =========================================================
# 0. config ë¡œë“œ
# =========================================================
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

EXCLUDE_TITLE_KEYWORDS = config["EXCLUDE_TITLE_KEYWORDS"]
ALLOWED_SOURCES = set(config["ALLOWED_SOURCES"])
favorite_categories = config["favorite_categories"]
excel_company_categories = config["excel_company_categories"]
common_filter_categories = config["common_filter_categories"]
industry_filter_categories = config["industry_filter_categories"]
SYNONYM_MAP = config["synonym_map"]
kiscd_map = config.get("kiscd_map", {})
kr_compcd_map = config.get("kr_COMP_CD_map", {})

# ê³µí†µ í•„í„° í‚¤ì›Œë“œ ì „ì²´ ë¦¬ìŠ¤íŠ¸
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)


# =========================================================
# 1. ìœ í‹¸
# =========================================================
def get_sector_of_company(company: str):
    for sector, comps in favorite_categories.items():
        if company in comps:
            return sector
    return None

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def make_uid(url: str, length: int = 16) -> str:
    if not url:
        return "no_url"
    return hashlib.md5(url.encode("utf-8")).hexdigest()[:length]

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

def is_similar(title1, title2, threshold=0.75):
    ratio = difflib.SequenceMatcher(None, title1, title2).ratio()
    return ratio >= threshold

def remove_duplicates(articles):
    unique_articles = []
    titles = []
    for article in articles:
        title = article.get("title", "")
        if all(not is_similar(title, existing_title) for existing_title in titles):
            unique_articles.append(article)
            titles.append(title)
    return unique_articles

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def clean_excel_formula_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = text.replace('"', "'").replace('\n', ' ').replace('\r', '')
    return text[:250]


# =========================================================
# 2. ì‚°ì—… í‚¤ì›Œë“œ / íŒŒì„œ
# =========================================================
def get_industry_credit_keywords():
    return """
ë³´í—˜ì‚¬: ìˆ˜ìµì„±, ìë³¸ì ì •ì„±, IFRS17, K-ICS, ë¦¬ìŠ¤í¬ê´€ë¦¬, ì†í•´ìœ¨, ì¬ë³´í—˜, ìœ ë™ì„±, íˆ¬ììì‚°, ìŠ¤íŠ¸ë ˆìŠ¤í…ŒìŠ¤íŠ¸, ê²½ì˜íˆ¬ëª…ì„±, ë‚´ë¶€í†µì œ, ì‹œì¥ì§€ìœ„, ìê¸ˆì¡°ë‹¬, ì •ì±…, ê·œì œ, ëŒ€ì²´íˆ¬ì, ì†ìµë³€ë™, ì§€ê¸‰ì—¬ë ¥, ê³„ì•½ìœ ì§€ìœ¨, ìœ„í—˜ì§‘ì¤‘, ì²´ì¦ë¥ , ë³´í—˜ê¸ˆì§€ê¸‰
5ëŒ€ê¸ˆìœµì§€ì£¼ ë° ì€í–‰: ìíšŒì‚¬ ì‹ ìš©ë„, ë°°ë‹¹, ìì‚°ê±´ì „ì„±, ì •ë¶€ì§€ì›, ìë³¸ë¹„ìœ¨, ìœ ë™ì„±ë¹„ìœ¨, ëŒ€ì†ì¶©ë‹¹ê¸ˆ, ë ˆë²„ë¦¬ì§€, ìŠ¤íŠ¸ë ˆìŠ¤, ì‹œì¥ìœ„í—˜, ê¸ˆë¦¬ìœ„í—˜, ë¹„ì´ììˆ˜ìµ, ë‹¤ê°í™”, ê±°ë²„ë„ŒìŠ¤, ê·œì œì¤€ìˆ˜, ìš´ì˜ìœ„í—˜, ë‹¨ê¸°ë¶€ì±„, êµ¬ì¡°ì¡°ì •, ë¶€ì‹¤ì±„ê¶Œ, ì¡°ê¸°ê²½ë³´, ìœ ê°€ì¦ê¶Œ
ì¹´ë“œì‚¬: ì‹œì¥ì ìœ ìœ¨, ìˆ˜ìˆ˜ë£Œìœ¨, ëŒ€ì†ë¹„ìš©, ìì‚°ê±´ì „ì„±, ì‹ ìš©ë¦¬ìŠ¤í¬, ëŒ€ì†ìœ¨, ìƒí™˜ëŠ¥ë ¥, í¬íŠ¸í´ë¦¬ì˜¤, ìˆ˜ìµì„±, ê±°ë˜ëŸ‰, ìš´ì˜ë¦¬ìŠ¤í¬, ë²•ë¥ , íŒŒíŠ¸ë„ˆì‹­, ë¹„ìš©, ê¸ˆìœµì¡°ë‹¬, ì‹ ìš©ì§€ì›, ê²½ìŸë ¥, ê°€ê²©ì±…ì •, ìŠ¹ì¸ê±°ë˜ì•¡, ë¶€ì •ì‚¬ìš©, ê²°ì œì—°ì²´
ìºí”¼íƒˆ: ì‚¬ì—…í†µí•©, ìˆ˜ìµì•ˆì •ì„±, ìì‚°ê±´ì „ì„±, í•´ì™¸ì‹œì¥, ë¶€ì‹¤ë¥ , ìê¸ˆì¡°ë‹¬, ìœ ë™ì„±, ì´ìµì°½ì¶œë ¥, ì„±ì¥ì„±, ì‹ ìš©ë¦¬ìŠ¤í¬, ì‹œì¥ë¦¬ìŠ¤í¬, ë²•ì ì œì•½, ë‚´ë¶€í†µì œ, ì±„ê¶Œí¬íŠ¸í´ë¦¬ì˜¤, íŒŒìƒìƒí’ˆ, ê·¸ë£¹ì§€ì›, ì‚¬ì—…ë‹¤ê°í™”, ë¦¬ìŠ¤í¬ì§‘ì¤‘ë„, ëŒ€ì¶œì±„ê¶Œ, ë¶€ì‹¤ì±„ê¶Œë¹„ìœ¨, íšŒìˆ˜ìœ¨
ì§€ì£¼ì‚¬: ìíšŒì‚¬ ì‹ ìš©ë„, ë°°ë‹¹ì•ˆì •ì„±, ì¬ë¬´ë¶€ë‹´, ê·¸ë£¹ì‹ ìš©, ì§€ë°°êµ¬ì¡°, ì¬ë¬´ë ˆë²„ë¦¬ì§€, ë¶€ì±„ë§Œê¸°, ì‹ ìš©ì§€ì›, ìˆ˜ìµì•ˆì •ì„±, ìë³¸ì¡°ë‹¬, ìì‚°ê±´ì „ì„±, í˜„ê¸ˆíë¦„, ìë³¸ì„±ì¦ê¶Œ, íˆ¬ìë¦¬ìŠ¤í¬, ì „ëµì§€ì›, ì§€ë¶„ìœ¨, ë‚´ë¶€ê±°ë˜, ê²½ì˜ê¶Œìœ„í—˜
ì—ë„ˆì§€: ì‹œì¥ê²½ìŸ, ì‚¬ì—…ë‹¤ê°í™”, í•´ì™¸ì‹¤ì , íˆ¬ìê·œëª¨, ê°€ê²©ë³€ë™ì„±, ì¬ë¬´ì•ˆì •ì„±, ì •ì±…ë³€í™”, í™˜ê²½ê·œì œ, í˜„ê¸ˆíë¦„, í”„ë¡œì íŠ¸ì§‘í–‰, ì¬ë¬´íŒŒìƒìƒí’ˆë¦¬ìŠ¤í¬, ë¶€ì±„êµ¬ì¡°, ìë³¸ì¡°ë‹¬, ê³µê¸‰ë§, ê¸°ìˆ ì „í™˜, ê¸€ë¡œë²Œê²½ì œ, íƒ„ì†Œë°°ì¶œê¶Œ, ì—ë„ˆì§€ìˆ˜ê¸‰, ì •ë¶€ì§€ì›
ë°œì „: ì „ë ¥ê¸°ë°˜, ì„¤ë¹„íˆ¬ì, ì „ë ¥ê°€ê²©, ê°€ë™ë¥ , ê³„ì•½, ì—°ë£Œë¹„, ë¶€ì±„, ìë³¸êµ¬ì¡°, ë°°ë‹¹ì •ì±…, ì¬ë¬´ìœ ì—°ì„±, ì •ë¶€ê·œì œ, í™˜ê²½ë²•ê·œ, í˜„ê¸ˆíë¦„, íˆ¬ìê³„íš, ì°¨ì…ê¸ˆ, ê¸°ìˆ ë¦¬ìŠ¤í¬, ì‚¬ì—…ë‹¤ê°í™”, ì‹œì¥ìˆ˜ìš”, ë°œì „íš¨ìœ¨, ì‹ ì¬ìƒì—ë„ˆì§€, ì •ë¶€ë³´ì¡°ê¸ˆ
ìë™ì°¨: ë°°í„°ë¦¬ì‹œì¥, ì „ê¸°ì°¨ìˆ˜ìš”, ì„¤ë¹„íˆ¬ì, ìˆ˜ìµì„±, ì‹œì¥ì ìœ ìœ¨, ê¸°ìˆ ê²½ìŸë ¥, ë§¤ì¶œë‹¤ê°í™”, ë ˆë²„ë¦¬ì§€, ê³ ì •ë¹„, ìƒì‚°ëŠ¥ë ¥, ì‹ ì œí’ˆê°œë°œ, ì •ë¶€ì •ì±…, ê³µê¸‰ë§, ìë³¸ì§€ì¶œ, ì—°êµ¬ê°œë°œ, í˜„ê¸ˆíë¦„, ì„±ì¥ì „ë§, ê²½ìŸí™˜ê²½, ì¹œí™˜ê²½ì°¨, ê´€ì„¸ì •ì±…
ì „ê¸°ì „ì: ë°˜ë„ì²´ì‹œì¥, AIìˆ˜ìš”, ë¬´ì—­ê·œì œ, ê¸°ìˆ ìš°ìœ„, ì œí’ˆìˆ˜ìš”, ê´€ì„¸, íˆ¬ìê³„íš, ìƒì‚°ì‹œì„¤, ì¬ë¬´ì•ˆì •ì„±, ì—°êµ¬ê°œë°œ, ê³µê¸‰ë§, ì§„ì…ì¥ë²½, ìš´ì˜íš¨ìœ¨, í™˜ìœ¨, ë³´ì•ˆ, ê°€ê²©ê²½ìŸë ¥, ì¸ì¬í™•ë³´, ì¬ë¬´ì •ì±…, ê¸°ìˆ íŠ¹í—ˆ, ë³´ì•ˆìœ„í˜‘
ì†Œë¹„ì¬: ìœ í†µë³€í™”, M&Aì¬ë¬´ë¶€ë‹´, ì˜¨ë¼ì¸ì‚¬ì—…, ìœ í†µì±„ë„, ë¸Œëœë“œ, ì‹œì¥ì ìœ ìœ¨, ì˜ì—…ì´ìµë¥ , í˜„ê¸ˆíë¦„, ì¬ë¬´ê±´ì „ì„±, ì¬ê³ ê´€ë¦¬, ê²½ìŸì••ë ¥, í˜ì‹ , ê³ ê°ì¶©ì„±ë„, ë¹„ìš©, ê³µê¸‰ë§, ì‹ ìš©ì§€ì›, ë§¤ì¶œì„±ì¥, ì‹ ì œí’ˆëŸ°ì¹­, ê³ ê°í™•ë³´
ë¹„ì² ì² ê°•: ìˆ˜ìš”ê³µê¸‰, ê°€ê²©ë³€ë™, í•´ì™¸í”„ë¡œì íŠ¸, ì¹œí™˜ê²½ì„¤ë¹„, ë¹„ìš©, ìë³¸ì§€ì¶œ, ì‹¤í–‰ë ¥, í™˜ê²½ê·œì œ, ë¶€ì±„, í˜„ê¸ˆíë¦„, ì‹œì¥ë‹¤ë³€í™”, ìƒí’ˆí¬íŠ¸í´ë¦¬ì˜¤, ê²½ìŸ, ê³µê¸‰ë§, ê¸°ìˆ ì „í™˜, ì›ìì¬ê°€ê²©, ìˆ˜ì¶œë¹„ì¤‘
ì„ìœ í™”í•™: ê²½ìŸë ¥, í¬íŠ¸í´ë¦¬ì˜¤, íˆ¬ì, ì°¨ì…ê¸ˆ, ì„¸ì œ, ì¬ë¬´ê´€ë¦¬, ì—…í™©ë¯¼ê°ë„, ì°¨ì…ê¸ˆë¹„ìœ¨, ìê¸ˆì¡°ë‹¬, ì¸ìˆ˜í•©ë³‘, ìˆ˜ìµì„±, í˜„ê¸ˆíë¦„, ìì‚°ìœ ë™í™”, ë¦¬ìŠ¤í¬ë¶„ì‚°, ì‹œì¥ì ìœ ìœ¨, ë¹„ìš©, ë¹„í•µì‹¬ìì‚°, í”„ë¡œì íŠ¸ê´€ë¦¬, ì„¸ì œí˜œíƒ
íŠ¹ìˆ˜ì±„: ì¤€ì •ë¶€ê¸°ê´€, ë³´ì¦ì‹œì¥, ë³´ì¦ì‚¬ê³ , ìë³¸í™•ì¶©, ì •ë¶€ì§€ì›, ì‹ ìš©ì—°ê³„, ë³´ì¦ì”ì•¡, ë¦¬ìŠ¤í¬, í˜„ê¸ˆì„±ìì‚°, ë‹¨ê¸°ë¶€ì±„, ë¯¸íšŒìˆ˜ì±„ê¶Œ, ìì‚°ê±´ì „ì„±, ìš´ì˜ì•ˆì •ì„±, ë³´ì¦í•œë„, ì¬ë¬´ì•ˆì •ì„±, ì‹œì¥ì§€ìœ„, ê´€ë¦¬ì²´ê³„, ì •ì±…, ì±„ê¶Œë°œí–‰, ì§€ê¸‰ìœ ì˜ˆ, ë¶ˆí™•ì‹¤ì„±
"""

def parse_industry_credit_keywords():
    raw_text = get_industry_credit_keywords()
    industry_dict = {}
    for line in raw_text.strip().split("\n"):
        line = line.strip()
        if not line or ":" not in line:
            continue
        sector, kws = line.split(":", 1)
        industry_dict[sector.strip()] = [
            kw.strip() for kw in kws.split(",") if kw.strip()
        ]
    return industry_dict

def expand_keywords_with_synonyms(original_keywords):
    expanded_map = {}
    for kw in original_keywords:
        synonyms = SYNONYM_MAP.get(kw, [])
        expanded_map[kw] = [kw] + synonyms
    return expanded_map


# =========================================================
# 3. OpenAI / ìš”ì•½ / ê°ì„±
# =========================================================
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

def summarize_and_sentiment_with_openai(text, do_summary=True, target_keyword=None):
    if not OPENAI_API_KEY or client is None:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", text
    if not text or "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜" in text:
        return "ê¸°ì‚¬ ë³¸ë¬¸ì´ ì¶”ì¶œ ì‹¤íŒ¨", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", text

    industry_keywords = get_industry_credit_keywords()

    prompt = f"""
[ì°¸ê³ : ì‚°ì—…êµ°ë³„ ì‹ ìš©í‰ê°€ í‚¤ì›Œë“œ(ì°¸ê³ ìš©)]
{industry_keywords}

ì•„ë˜ [ê¸°ì‚¬ ë³¸ë¬¸]ì„ ë¶„ì„í•´ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì‹œì˜¤.
ëŒ€ìƒ ê¸°ì—…: "{target_keyword or 'N/A'}"

ìš”êµ¬ í˜•ì‹:
- 1. [í•œ ì¤„ ìš”ì•½]:
  * ì‚¬ì‹¤ ì¤‘ì‹¬(ëˆ„ê°€/ë¬´ì—‡ì„/ì–¸ì œ/ì–´ë–»ê²Œ).
  * 2~3ë¬¸ì¥ ì´ë‚´ë¡œ í•µì‹¬ ì‚¬ì‹¤ ìš”ì•½.
  * ì¶”ì¸¡/í‰ê°€ ê¸ˆì§€.
- 2. [ì‹¬ì¸µ ì‹œì‚¬ì ]: ì‹ ìš©í‰ê°€ì‚¬ì˜ ì½”ë©˜íŠ¸ í˜•ì‹ìœ¼ë¡œ ë“±ê¸‰/ì „ë§/ì¬ë¬´ì•ˆì •ì„±/í˜„ê¸ˆíë¦„/ìœ ë™ì„±/ì‚¬ì—…Â·ê·œì œ í™˜ê²½ ì˜í–¥ ë¶„ì„(3ë¬¸ì¥ ì´ìƒ).
- 3. [í•œ ì¤„ ì‹œì‚¬ì ]: ì˜í–¥ì˜ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ì••ì¶•.
- 4. [ê°ì„±]: ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜.
- 5. [ê²€ìƒ‰ í‚¤ì›Œë“œ]: ëŒ€ìƒ ê¸°ì—…ëª… ë˜ëŠ” ì£¼ìš” ì—”í‹°í‹° ìœ„ì£¼ë¡œ ì½¤ë§ˆ êµ¬ë¶„.
- 6. [ì£¼ìš” í‚¤ì›Œë“œ]: ì¸ë¬¼/ê¸°ì—…/ê¸°ê´€ëª… ì¤‘ì‹¬ìœ¼ë¡œ ì½¤ë§ˆ êµ¬ë¶„. ì—†ìœ¼ë©´ 'ì—†ìŒ'.

[ê¸°ì‚¬ ë³¸ë¬¸]
{text}
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì‹ ìš©í‰ê°€ì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤. ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨í•˜ê³  ê³¼ì¥/ì¶”ì¸¡ì„ í”¼í•œë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=900,
            temperature=0.3
        )
        answer = response.choices[0].message.content.strip()
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", text

    def extract_group(tag):
        pattern = rf"\[{tag}\]:\s*([\s\S]+?)(?=\n\[\w+\]:|\n\d+\. \[|$)"
        m = re.search(pattern, answer)
        return m.group(1).strip() if m else ""

    def clean_llm_text(t: str) -> str:
        if not t:
            return t
        cleaned_lines = []
        for ln in t.splitlines():
            ln = re.sub(r"^\s*\d+\.\s*", "", ln).strip()
            if not ln:
                continue
            if re.fullmatch(r"\d+", ln):
                continue
            cleaned_lines.append(ln)
        return "\n".join(cleaned_lines).strip()

    one_line = clean_llm_text(extract_group("í•œ ì¤„ ìš”ì•½") or "ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨")
    detailed_implication = clean_llm_text(extract_group("ì‹¬ì¸µ ì‹œì‚¬ì ") or "ì‹œì‚¬ì  ì¶”ì¶œ ì‹¤íŒ¨")
    short_implication = clean_llm_text(extract_group("í•œ ì¤„ ì‹œì‚¬ì ") or "í•œ ì¤„ ì‹œì‚¬ì  ìš”ì•½ ì‹¤íŒ¨")
    sentiment = extract_group("ê°ì„±") or "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨"
    keywords = extract_group("ê²€ìƒ‰ í‚¤ì›Œë“œ") or ""
    key_entities = extract_group("ì£¼ìš” í‚¤ì›Œë“œ") or ""

    s = sentiment.strip().lower()
    if "ê¸" in s or "positive" in s:
        sentiment = "ê¸ì •"
    elif "ë¶€" in s or "negative" in s:
        sentiment = "ë¶€ì •"
    elif "ì¤‘ë¦½" in s or "neutral" in s:
        sentiment = "ì¤‘ë¦½"
    else:
        sentiment = "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨"

    return one_line, keywords, sentiment, detailed_implication, short_implication, text


# =========================================================
# 4. ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
# =========================================================
def extract_article_text(url, fallback_desc=None, fallback_title=None):
    try:
        art = newspaper.Article(url, language="ko")
        art.download()
        art.parse()
        txt = (art.text or "").strip()
        if len(txt) >= 300:
            return txt
    except Exception:
        pass

    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        candidates = [
            "article", ".article", ".news_body", "#articleBodyContents",
            ".content", ".article-body", ".art_txt", ".article_view"
        ]
        blocks = []
        for sel in candidates:
            blocks.extend(soup.select(sel))

        paragraphs = []
        if blocks:
            for b in blocks:
                paragraphs.extend(b.select("p"))
        else:
            paragraphs = soup.select("p")

        text = " ".join(
            p.get_text(strip=True)
            for p in paragraphs
            if len(p.get_text(strip=True)) >= 30
        )
        text = " ".join(text.split())
        if len(text) >= 200:
            return text
    except Exception:
        pass

    if fallback_desc or fallback_title:
        return f"{(fallback_title or '').strip()} {(fallback_desc or '').strip()}".strip()

    return "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"


def get_summary_key_from_url(article_url: str, target_keyword: str = None) -> str:
    uid = make_uid(article_url)
    if target_keyword and str(target_keyword).strip():
        return f"summary_{target_keyword}_{uid}"
    return f"summary_{uid}"

def summarize_article_from_url(article_url, title, do_summary=True, target_keyword=None, description=None):
    uid = make_uid(article_url)
    if target_keyword and str(target_keyword).strip():
        summary_key = f"summary_{target_keyword}_{uid}"
    else:
        summary_key = f"summary_{uid}"

    if summary_key in st.session_state:
        return st.session_state[summary_key]

    try:
        full_text = extract_article_text(
            article_url,
            fallback_desc=description,
            fallback_title=title
        )

        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            result = (full_text, "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", full_text)
        else:
            one_line, summary, sentiment, implication, short_implication, text = summarize_and_sentiment_with_openai(
                full_text,
                do_summary=do_summary,
                target_keyword=target_keyword
            )
            result = (one_line, summary, sentiment, implication, short_implication, text)
    except Exception as e:
        result = (f"ìš”ì•½ ì˜¤ë¥˜: {e}", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", "")

    st.session_state[summary_key] = result
    return result


# =========================================================
# 5. Naver ë‰´ìŠ¤ ìˆ˜ì§‘
# =========================================================
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID
    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        text = (title + " " + desc).lower()
        if not any(kw.lower() in text for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        for item in items:
            title = html.unescape(re.sub("<.*?>", "", item["title"]))
            desc  = html.unescape(re.sub("<.*?>", "", item["description"]))
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()

            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
                continue

            source = item.get("source") or infer_source_from_url(item.get("originallink", "")) or "Naver"
            source_domain = source.lower()
            if source_domain.startswith("www."):
                source_domain = source_domain[4:]

            real_link = item.get("originallink") or item["link"]

            articles.append({
                "title": title,
                "description": desc,
                "link": real_link,
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": source_domain
            })

        if len(items) < 100:
            break
    return articles[:limit]


# =========================================================
# 6. LLM ì ìˆ˜ / í•„í„°
# =========================================================
def llm_score_articles_batch(articles, target_keyword=None, mode="company"):
    if not OPENAI_API_KEY or client is None:
        return {i: 3 for i in range(len(articles))}

    prompt_list = "\n".join(
        [f"{i+1}. {a.get('title','')} || {a.get('description','')}" for i, a in enumerate(articles)]
    )

    if mode == "industry":
        guideline = f"""
ë„ˆëŠ” ì‹ ìš©í‰ê°€ì‚¬ ì‚°ì—… ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤. ì•„ë˜ ê¸°ì‚¬ ì œëª©/ìš”ì•½ì„ ë³´ê³ 
ì‚°ì—… ëŒ€ë¶„ë¥˜ "{target_keyword or 'N/A'}" ê´€ì ì—ì„œ ì‚°ì—… ì „ë°˜ ì˜í–¥ë„ë¥¼ 1~5ì ìœ¼ë¡œ í‰ê°€í•˜ë¼.

5ì : ì‚°ì—… êµ¬ì¡°/ê·œì œ/ì •ì±…/ê¸ˆìœµì—¬ê±´/ìˆ˜ìš”Â·ê³µê¸‰/ê°€ê²©ê²°ì • êµ¬ì¡°/ê²½ìŸêµ¬ë„ ë³€í™” ë“± ë‹¤ìˆ˜ ê¸°ì—…ì— ì¥ê¸° êµ¬ì¡°ì  ì˜í–¥.
4ì : ìƒë‹¹ìˆ˜ ê¸°ì—…ì— ì¤‘ê¸°ì  ì˜í–¥ ì˜ˆìƒ.
3ì : ì¼ë¶€ ê¸°ì—…êµ°ì— ì˜í–¥ ìˆìœ¼ë‚˜ íŒŒê¸‰Â·ì§€ì†ì„± ì œí•œ.
2ì : íŠ¹ì • ê¸°ì—… ë‹¨ì¼ ì´ìŠˆ.
1ì : í™ë³´/í–‰ì‚¬ ë“± ì‹ ìš©Â·êµ¬ì¡°ì™€ ë¬´ê´€.

[ê°•ì œ ê·œì¹™]
- íŠ¹ì • ê¸°ì—… 1ê³³ ì´ìŠˆë©´ ìµœëŒ€ 2ì .
- ì‚°ì—… ì „ì²´ êµ¬ì¡°Â·ê·œì œÂ·ìˆ˜ê¸‰Â·ê²½ìŸ/ì‚¬ì´í´ì´ë©´ ê³ ì .

[ê¸°ì‚¬ ëª©ë¡]
{prompt_list}

ì¶œë ¥:
1ë²ˆ: ì ìˆ˜
2ë²ˆ: ì ìˆ˜
...
(ì„¤ëª… ê¸ˆì§€)
"""
    else:
        guideline = f"""
ë„ˆëŠ” ì‹ ìš©í‰ê°€ì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤. ì•„ë˜ ê¸°ì‚¬ ì œëª©/ìš”ì•½ì„ ë³´ê³ 
ëŒ€ìƒ ê¸°ì—… "{target_keyword or 'N/A'}" ê´€ì ì—ì„œ ì‹ ìš©ì˜í–¥ ì¤‘ìš”ë„ë¥¼ 1~5ì ìœ¼ë¡œ íŒë‹¨í•˜ë¼.

5ì : ë“±ê¸‰/ì „ë§ ë³€ê²½ ê°€ëŠ¥ì„±, ëŒ€ê·œëª¨ ì°¨ì…/ì¡°ë‹¬, ìœ ë™ì„±Â·íšŒìƒ/ë¶€ë„ ë“±
4ì : ëŒ€ê·œëª¨ íˆ¬ìÂ·M&AÂ·ìì‚°ë§¤ê°, ë ˆë²„ë¦¬ì§€ ê¸‰ë³€, ìœ ì˜ë¯¸í•œ ì‹¤ì  ë³€í™”
3ì : ì¼ë°˜ì  ì‹¤ì Â·êµ¬ì¡°ì¡°ì •Â·ì¡°ë‹¬ ì´ìŠˆ
2ì : ì˜í–¥ ì œí•œì  ì‚¬ì—…/ë§ˆì¼€íŒ…/ì œíœ´
1ì : í™ë³´/í–‰ì‚¬/ESG ë“± ì‹ ìš© ë¬´ê´€

[ê¸°ì‚¬ ëª©ë¡]
{prompt_list}

ì¶œë ¥:
1ë²ˆ: ì ìˆ˜
2ë²ˆ: ì ìˆ˜
...
(ì„¤ëª… ê¸ˆì§€)
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ì ìˆ˜í™”í•˜ë¼. ê³¼ì¥í•˜ì§€ ë§ ê²ƒ."},
                {"role": "user", "content": guideline},
            ],
            max_tokens=300,
            temperature=0
        )
        ans = resp.choices[0].message.content.strip()
    except Exception:
        return {i: 3 for i in range(len(articles))}

    score_map = {}
    for line in ans.splitlines():
        m = re.match(r"(\d+)ë²ˆ\s*:\s*([1-5])", line.strip())
        if m:
            no = int(m.group(1)) - 1
            score_map[no] = int(m.group(2))

    for i in range(len(articles)):
        score_map.setdefault(i, 3)

    return score_map


def llm_filter_and_rank_articles(main_kw, articles):
    if not articles:
        return articles

    cap = st.session_state.get("llm_candidate_cap", 200)
    top_k = st.session_state.get("llm_top_k", 10)

    main_kw_lower = (main_kw or "").lower()

    common_title_kws = ALL_COMMON_FILTER_KEYWORDS
    selected_industry_sub_kws = []
    if st.session_state.get("use_industry_filter", False):
        for sublist in st.session_state.industry_major_sub_map.values():
            selected_industry_sub_kws.extend(sublist)

    industry_credit_dict = parse_industry_credit_keywords()
    sector = get_sector_of_company(main_kw)
    sector_credit_kws = industry_credit_dict.get(sector, []) if sector else []

    def title_has_any_kw(title, kw_list):
        t = (title or "").lower()
        return any((kw or "").lower() in t for kw in kw_list if kw)

    p1, p2, p3 = [], [], []
    for a in articles:
        title = a.get("title", "") or ""
        t_lower = title.lower()

        if main_kw_lower and main_kw_lower in t_lower:
            p1.append(a); continue

        if (
            title_has_any_kw(title, common_title_kws) or
            title_has_any_kw(title, selected_industry_sub_kws) or
            title_has_any_kw(title, sector_credit_kws)
        ):
            p2.append(a); continue

        p3.append(a)

    def sort_newest(lst):
        return sorted(lst, key=lambda x: x.get("date", ""), reverse=True)

    p1, p2, p3 = sort_newest(p1), sort_newest(p2), sort_newest(p3)
    candidates = (p1 + p2 + p3)[:cap]

    scores = llm_score_articles_batch(candidates, target_keyword=main_kw)
    for i, a in enumerate(candidates):
        a["llm_score"] = scores.get(i, 3)
        if a in p1: a["rule_priority"] = 1
        elif a in p2: a["rule_priority"] = 2
        else: a["rule_priority"] = 3

    ranked = sorted(
        candidates,
        key=lambda x: (
            x.get("rule_priority", 3),
            -x.get("llm_score", 3),
            x.get("date", "")
        )
    )
    return ranked[:top_k]


def build_industry_major_article_pool(results_by_company):
    favorite_to_industry_major = config.get("favorite_to_industry_major", {})
    major_pool = {}
    industry_credit_dict = parse_industry_credit_keywords()

    for company, arts in results_by_company.items():
        majors = []
        for cat, comps in favorite_categories.items():
            if company in comps:
                majors.extend(favorite_to_industry_major.get(cat, []))
        majors = list(dict.fromkeys(majors))
        if not majors:
            continue

        for m in majors:
            sector_kws = industry_credit_dict.get(m, [])
            major_pool.setdefault(m, [])
            for a in arts:
                title = (a.get("title","") or "").lower()
                has_sector_kw = any(kw.lower() in title for kw in sector_kws)
                has_common_kw = any(kw.lower() in title for kw in ALL_COMMON_FILTER_KEYWORDS)
                pr = 1 if (has_sector_kw or has_common_kw) else 2
                major_pool[m].append({**a, "í‚¤ì›Œë“œ": company, "industry_rule_priority": pr})

    for m in major_pool:
        major_pool[m] = sorted(
            major_pool[m],
            key=lambda x: (x.get("industry_rule_priority",3), x.get("date","")),
            reverse=False
        )
        if st.session_state.get("remove_duplicate_articles", False):
            major_pool[m] = remove_duplicates(major_pool[m])

    return major_pool


def llm_filter_and_rank_industry_major(major_name, articles):
    if not articles:
        return articles

    cap = st.session_state.get("industry_issue_cap", 300)
    top_k = st.session_state.get("industry_issue_top_k", 8)

    candidates = articles[:cap]
    scores = llm_score_articles_batch(candidates, target_keyword=major_name, mode="industry")
    for i, a in enumerate(candidates):
        a["llm_score"] = scores.get(i, 3)
        a["rule_priority"] = 2

    ranked = sorted(
        candidates,
        key=lambda x: (-x.get("llm_score", 3), x.get("date", ""))
    )
    return ranked[:top_k]


# =========================================================
# 7. ê°•ë ¥í•„í„° fallback + ì €ì¥
# =========================================================
def process_keywords_with_synonyms(favorite_to_expand_map, start_date, end_date, require_keyword_in_title=False):
    for main_kw, kw_list in favorite_to_expand_map.items():
        all_articles = []
        did_fallback = False

        with ThreadPoolExecutor(max_workers=min(5, len(kw_list))) as executor:
            futures = {
                executor.submit(
                    fetch_naver_news,
                    search_kw,
                    start_date,
                    end_date,
                    require_keyword_in_title=require_keyword_in_title
                ): search_kw
                for search_kw in kw_list
            }
            for future in as_completed(futures):
                search_kw = futures[future]
                try:
                    fetched = future.result()
                    fetched = [{**a, "ê²€ìƒ‰ì–´": search_kw, "í‚¤ì›Œë“œ": main_kw} for a in fetched]
                    all_articles.extend(fetched)
                except Exception as e:
                    st.warning(f"{main_kw} - '{search_kw}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        def passes_strong_filter_for_main(a):
            if st.session_state.get("require_exact_keyword_in_title_or_content", False):
                t = a.get("title", "") or ""
                d = a.get("description", "") or ""
                return (main_kw in t) or (main_kw in d)
            return True

        strong_main_articles = [a for a in all_articles if passes_strong_filter_for_main(a)]

        if (
            len(strong_main_articles) == 0
            and st.session_state.get("require_exact_keyword_in_title_or_content", False)
        ):
            did_fallback = True
            fallback_articles = []
            with ThreadPoolExecutor(max_workers=min(5, len(kw_list))) as executor:
                futures = {
                    executor.submit(
                        fetch_naver_news,
                        search_kw,
                        start_date,
                        end_date,
                        require_keyword_in_title=False
                    ): search_kw
                    for search_kw in kw_list
                }
                for future in as_completed(futures):
                    search_kw = futures[future]
                    try:
                        fetched = future.result()
                        fetched = [{**a, "ê²€ìƒ‰ì–´": search_kw, "í‚¤ì›Œë“œ": main_kw} for a in fetched]
                        fallback_articles.extend(fetched)
                    except Exception as e:
                        st.warning(f"[Fallback] {main_kw} - '{search_kw}' ì‹¤íŒ¨: {e}")
            all_articles = fallback_articles

        if st.session_state.get("remove_duplicate_articles", False):
            all_articles = remove_duplicates(all_articles)

        if st.session_state.get("use_llm_filter", False):
            all_articles = llm_filter_and_rank_articles(main_kw, all_articles)

        st.session_state.search_results[main_kw] = all_articles

        if main_kw not in st.session_state.show_limit:
            st.session_state.show_limit[main_kw] = 5


# =========================================================
# 8. ìµœì¢… í•„í„°(ë Œë” ì§ì „ ê³µí†µ)
# =========================================================
def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords, target_keyword=None):
    title = article.get("title", "") or ""
    content = ""
    link = article.get("link", "") or ""
    summary_key = get_summary_key_from_url(link, target_keyword)
    if summary_key in st.session_state and isinstance(st.session_state[summary_key], tuple):
        _, _, _, _, _, full_text = st.session_state[summary_key]
        content = full_text or ""

    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

def get_industry_majors_from_favorites(selected_categories):
    favorite_to_industry_major = config["favorite_to_industry_major"]
    majors = set()
    for cat in selected_categories:
        for major in favorite_to_industry_major.get(cat, []):
            majors.add(major)
    return list(majors)

def article_passes_all_filters(article):
    main_kw = (article.get("í‚¤ì›Œë“œ") or "").strip()
    if not main_kw:
        return False

    main_kws = [main_kw] + SYNONYM_MAP.get(main_kw, [])
    title = article.get("title", "") or ""
    desc  = article.get("description", "") or ""
    text_short = f"{title} {desc}"
    company_mentioned = any(k in text_short for k in main_kws)

    if st.session_state.get("require_exact_keyword_in_title_or_content", False):
        if not company_mentioned:
            return False

    if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
        return False

    try:
        pub_date = datetime.strptime(article['date'], '%Y-%m-%d').date()
        if pub_date < st.session_state.get("start_date") or pub_date > st.session_state.get("end_date"):
            return False
    except:
        return False

    all_keywords = []
    if "keyword_input" in st.session_state:
        all_keywords.extend([k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()])
    if "cat_multi" in st.session_state:
        for cat in st.session_state["cat_multi"]:
            all_keywords.extend(favorite_categories.get(cat, []))

    keyword_passed = article_contains_exact_keyword(article, all_keywords, target_keyword=main_kw)

    if st.session_state.get("filter_allowed_sources_only", False):
        source = article.get('source', '').lower()
        if source.startswith("www."):
            source = source[4:]
        if source not in ALLOWED_SOURCES:
            return False

    common_passed = or_keyword_filter(article, ALL_COMMON_FILTER_KEYWORDS)
    if not common_passed:
        return False

    industry_passed = True
    if st.session_state.get("use_industry_filter", False):
        keyword = main_kw
        matched_major = None
        for cat, companies in favorite_categories.items():
            if keyword in companies:
                majors = get_industry_majors_from_favorites([cat])
                if majors:
                    matched_major = majors[0]
                    break
        if matched_major:
            sub_keyword_filter = st.session_state.industry_major_sub_map.get(matched_major, [])
            if sub_keyword_filter:
                industry_passed = or_keyword_filter(article, sub_keyword_filter)

    if not (industry_passed or keyword_passed):
        return False

    return True


# =========================================================
# 9. ì¤‘ìš”ê¸°ì‚¬ ìë™ì„ ì • / ì—‘ì…€
# =========================================================
def generate_important_article_list(search_results, common_keywords, industry_keywords, favorites):
    if not OPENAI_API_KEY or client is None:
        return []

    result = []
    ind_kw_by_cat = {cat: industry_keywords for cat in favorites.keys()}

    for category, companies in favorites.items():
        sector_keywords = ind_kw_by_cat.get(category, [])

        for comp in companies:
            articles = search_results.get(comp, [])
            if not articles:
                continue

            target_articles = []
            for a in articles:
                text = (a.get("title", "") + " " + a.get("description", "")).lower()
                has_sector = any(kw.lower() in text for kw in sector_keywords) if sector_keywords else True
                has_common = any(kw.lower() in text for kw in common_keywords) if common_keywords else True
                if has_sector and has_common:
                    target_articles.append(a)

            if not target_articles:
                continue

            prompt_list = "\n".join(
                [
                    f"{i+1}. [ê¸°ì—…:{comp}] {a.get('title','')} || {a.get('description','')}"
                    for i, a in enumerate(target_articles)
                ]
            )

            guideline = f"""
ë‹¹ì‹ ì€ ì‹ ìš©í‰ê°€ì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

[ì‹ ìš©ì˜í–¥ë„ íŒë‹¨ ê¸°ì¤€]
5ì : ì‹ ìš©ë“±ê¸‰/ì „ë§ ë³€í™” ê°€ëŠ¥ì„±, ëŒ€ê·œëª¨ ìë³¸í™•ì¶©Â·ì°¨ì…, ìœ ë™ì„± ìœ„ê¸°, ë¶€ë„Â·íšŒìƒ, ì¤‘ëŒ€í•œ ê·œì œÂ·ì œì¬Â·ì†Œì†¡ ë“±
4ì : ëŒ€ê·œëª¨ íˆ¬ìÂ·M&AÂ·ì§€ë¶„ë§¤ê°, ì‹¤ì  ê¸‰ë³€, ë ˆë²„ë¦¬ì§€ ê¸‰ì¦, ê³„ì—´ì‚¬ ìœ„í—˜ ì „ì´
3ì : ì¼ë°˜ì  ì‹¤ì  ê°œì„ /ì•…í™”, ì¤‘ê°„ ê·œëª¨ ì¡°ë‹¬
2ì : ë§ˆì¼€íŒ…/ì œíœ´ ë“± ì˜í–¥ ì œí•œì 
1ì : í™ë³´/í–‰ì‚¬/ESG ë“± ì‹ ìš© ë¬´ê´€

[ê¸°ì‚¬ ëª©ë¡]
{prompt_list}

ë¶„ì„ ì´ˆì ì€ "{comp}"ì´ë©° "{category}" ì‚°ì—… ì‹ ìš©ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”.

[ì§€ì‹œì‚¬í•­]
1. ê° ê¸°ì‚¬ ë²ˆí˜¸ë³„ ì ìˆ˜(1~5ì )
2. 5ì  ê¸°ì‚¬ë§Œ ì¤‘ìš” í›„ë³´
3. 5ì  ì¤‘ ìµœëŒ€ 2ê±´ ì„ ì •
4. 5ì  ì—†ìœ¼ë©´ 'ì—†ìŒ'

ì¶œë ¥(ì„¤ëª… ê¸ˆì§€):
[í‰ê°€]
1ë²ˆ: (ì ìˆ˜)
...

[ì„ ì •]
[ì¤‘ìš”1]: (ê¸°ì‚¬ë²ˆí˜¸ ë˜ëŠ” ì—†ìŒ)
[ì¤‘ìš”2]: (ê¸°ì‚¬ë²ˆí˜¸ ë˜ëŠ” ì—†ìŒ)
"""
            try:
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": guideline}],
                    max_tokens=900,
                    temperature=0.2,
                )
                answer = response.choices[0].message.content.strip()

                score_map = {}
                for line in answer.splitlines():
                    m = re.match(r"(\d+)ë²ˆ\s*:\s*([1-5])", line.strip())
                    if m:
                        no = int(m.group(1))
                        score_map[no] = int(m.group(2))

                sel1 = re.search(r"\[ì¤‘ìš” ?1\]\s*:\s*(\d+)", answer)
                sel2 = re.search(r"\[ì¤‘ìš” ?2\]\s*:\s*(\d+)", answer)

                raw_selected = []
                if sel1: raw_selected.append(int(sel1.group(1)))
                if sel2: raw_selected.append(int(sel2.group(1)))

                selected_idx0 = []
                for no in raw_selected:
                    idx0 = no - 1
                    if score_map.get(no) == 5 and 0 <= idx0 < len(target_articles):
                        if idx0 not in selected_idx0:
                            selected_idx0.append(idx0)

                if not selected_idx0:
                    continue

                for idx0 in selected_idx0:
                    a = target_articles[idx0]
                    result.append({
                        "í‚¤ì›Œë“œ": comp,
                        "ê¸°ì‚¬ì œëª©": a.get("title", ""),
                        "ë§í¬": a.get("link", ""),
                        "ë‚ ì§œ": a.get("date", ""),
                        "ì¶œì²˜": a.get("source", ""),
                        "ê°ì„±": "",
                        "ì‹œì‚¬ì ": ""
                    })
            except Exception:
                continue

    return result


def matched_filter_keywords(article, common_keywords, industry_keywords):
    text_candidates = [
        article.get("title", ""),
        article.get("description", ""),
        article.get("ìš”ì•½ë³¸", ""),
        article.get("ìš”ì•½", ""),
        article.get("full_text", ""),
        article.get("content", ""),
    ]
    text_long = " ".join([str(t) for t in text_candidates if t])
    matched_common = [kw for kw in common_keywords if kw in text_long]
    matched_industry = [kw for kw in industry_keywords if kw in text_long]
    return list(set(matched_common + matched_industry))


def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories, search_results):
    def clean_text(text):
        if not isinstance(text, str):
            text = str(text)
        text = text.replace('"', "'").replace('\n', ' ').replace('\r', '')
        return text[:200]

    sector_list = []
    for cat in favorite_categories:
        sector_list.extend(favorite_categories[cat])
    sector_list = list(dict.fromkeys(sector_list))

    excel_sector_list = []
    for cat in excel_company_categories:
        excel_sector_list.extend(excel_company_categories[cat])
    excel_sector_list = list(dict.fromkeys(excel_sector_list))

    if summary_data is None or len(summary_data) == 0:
        df_empty = pd.DataFrame(columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df_empty.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
            worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
            worksheet.set_column(0, 5, 30)
        output.seek(0)
        return output

    df = pd.DataFrame(summary_data)

    implication_col = None
    if "í•œì¤„ì‹œì‚¬ì " in df.columns:
        implication_col = "í•œì¤„ì‹œì‚¬ì "
    elif "ì‹œì‚¬ì " in df.columns:
        implication_col = "ì‹œì‚¬ì "
    elif "implication" in df.columns:
        implication_col = "implication"

    keyword_col = "í‚¤ì›Œë“œ" if "í‚¤ì›Œë“œ" in df.columns else (df.columns[0] if len(df.columns) else "ê¸°ì—…ëª…")

    rows = []
    for idx, company in enumerate(sector_list):
        search_articles = search_results.get(company, [])
        filtered_articles = [a for a in search_articles if article_passes_all_filters(a)]

        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)

        if st.session_state.get("use_llm_filter", False):
            top_k = st.session_state.get("llm_top_k", 10)
            already_llm = (
                len(filtered_articles) <= top_k and
                all(("llm_score" in a) for a in filtered_articles)
            )
            if not already_llm:
                filtered_articles = llm_filter_and_rank_articles(company, filtered_articles)

        total_count = len(filtered_articles)

        filtered_df = df[df.get(keyword_col, "") == company].sort_values(by='ë‚ ì§œ', ascending=False)

        hl_news = ["", ""]
        implications = ["", ""]
        for i, art in enumerate(filtered_df.itertuples()):
            if i > 1:
                break
            date_val = getattr(art, "ë‚ ì§œ", "") or ""
            title_val = getattr(art, "ê¸°ì‚¬ì œëª©", "") or getattr(art, "ì œëª©", "")
            link_val = getattr(art, "ë§í¬", "") or getattr(art, "link", "")
            display_text = f"({clean_text(date_val)}){clean_text(title_val)}"
            if title_val and link_val:
                hl_news[i] = f'=HYPERLINK("{clean_text(link_val)}", "{display_text}")'
            else:
                hl_news[i] = display_text or ""
            implications[i] = getattr(art, implication_col, "") if implication_col else ""

        merged_implication = ""
        if implications[0]:
            merged_implication += f"1. {implications[0]}"
        if implications[1]:
            merged_implication += f"\n2. {implications[1]}"

        rows.append({
            "ê¸°ì—…ëª…": company,
            "í‘œê¸°ëª…": excel_sector_list[idx] if idx < len(excel_sector_list) else "",
            "ê±´ìˆ˜": total_count,
            "ì¤‘ìš”ë‰´ìŠ¤1": hl_news[0],
            "ì¤‘ìš”ë‰´ìŠ¤2": hl_news[1],
            "ì‹œì‚¬ì ": merged_implication
        })

    result_df = pd.DataFrame(rows, columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])

    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        result_df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
        worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
        for i, col in enumerate(result_df.columns):
            worksheet.set_column(i, i, 30)
    output.seek(0)
    return output


# =========================================================
# 10. ì‹ ìš©í‰ê°€ ë¦¬í¬íŠ¸ ìˆ˜ì§‘(KIS/NICE/KIE)  (ê¸°ì¡´ ìœ ì§€)
# =========================================================
def extract_file_url(js_href: str) -> str:
    if not js_href or not js_href.startswith("javascript:fn_file"):
        return ""
    m = re.search(r"fn_file\((.*)\)", js_href)
    if not m:
        return ""
    args_str = m.group(1)
    args = [arg.strip().strip("'\"") for arg in args_str.split(",")]
    if len(args) < 4:
        return ""
    file_name = args[3]
    return f"https://www.kisrating.com/common/download.do?filename={file_name}"

def extract_credit_details(html_text):
    soup = BeautifulSoup(html_text, 'html.parser')
    results = []
    items = soup.select('div.list li')
    for item in items:
        key_tag = item.find('dt') or item.find('strong')
        kind = key_tag.get_text(strip=True) if key_tag else None
        if not kind:
            continue

        grade_tag = item.find('span', string='ë“±ê¸‰')
        grade_val = ""
        if grade_tag:
            grade_node = grade_tag.find_next(['a', 'strong'])
            grade_val = grade_node.get_text(strip=True) if grade_node else ""

        outlook_tag = item.find('span', string=lambda s: s and ('Outlook' in s or 'Watchlist' in s))
        outlook_val = outlook_tag.next_sibling.strip() if outlook_tag and outlook_tag.next_sibling else ""

        eval_date_tag = item.find('span', string='í‰ê°€ì¼')
        eval_date_val = eval_date_tag.next_sibling.strip() if eval_date_tag and eval_date_tag.next_sibling else ""

        eval_opinion_tag = item.find('span', string='í‰ê°€ì˜ê²¬')
        eval_opinion_val = ""
        if eval_opinion_tag:
            next_node = eval_opinion_tag.find_next('a')
            if next_node:
                eval_opinion_val = next_node.get_text(strip=True)
            else:
                eval_opinion_val = eval_opinion_tag.find_next(string=True).strip()

        results.append({
            "ì¢…ë¥˜": kind,
            "ë“±ê¸‰": grade_val,
            "Outlook/Watchlist": outlook_val,
            "í‰ê°€ì¼": eval_date_val,
            "í‰ê°€ì˜ê²¬": eval_opinion_val
        })
    return results

def extract_reports_and_research(html_text: str) -> dict:
    soup = BeautifulSoup(html_text, 'html.parser')
    result = {"í‰ê°€ë¦¬í¬íŠ¸": [], "ê´€ë ¨ë¦¬ì„œì¹˜": [], "ì‹ ìš©ë“±ê¸‰ìƒì„¸": []}

    tables = soup.select('div.table_ty1 > table')
    for table in tables:
        caption = table.find('caption')
        if not caption:
            continue
        caption_text = caption.text.strip()

        def get_download_url(tr):
            for a in tr.find_all('a'):
                js_href = (a.get("href") or "") or (a.get("onclick") or "")
                url = extract_file_url(js_href)
                if url:
                    return url
            return ""

        if caption_text == "í‰ê°€ë¦¬í¬íŠ¸":
            rows = table.select('tbody > tr')
            for tr in rows:
                tds = tr.find_all('td')
                if len(tds) < 4:
                    continue
                report_type = tds[0].text.strip()
                a_tag = tds[1].find('a')
                title = a_tag.text.strip() if a_tag else ''
                date = tds[2].text.strip()
                eval_type = tds[3].text.strip()
                download_url = get_download_url(tr)
                result["í‰ê°€ë¦¬í¬íŠ¸"].append({
                    "ì¢…ë¥˜": report_type,
                    "ë¦¬í¬íŠ¸": title,
                    "ì¼ì": date,
                    "í‰ê°€ì¢…ë¥˜": eval_type,
                    "ë‹¤ìš´ë¡œë“œ": download_url
                })

        elif caption_text == "ê´€ë ¨ ë¦¬ì„œì¹˜":
            rows = table.select('tbody > tr')
            for tr in rows:
                tds = tr.find_all('td')
                if len(tds) < 4:
                    continue
                category = tds[0].text.strip()
                a_tag = tds[1].find('a')
                title = a_tag.text.strip() if a_tag else ''
                date = tds[2].text.strip()
                download_url = get_download_url(tr)
                result["ê´€ë ¨ë¦¬ì„œì¹˜"].append({
                    "êµ¬ë¶„": category,
                    "ì œëª©": title,
                    "ì¼ì": date,
                    "ë‹¤ìš´ë¡œë“œ": download_url
                })

    result["ì‹ ìš©ë“±ê¸‰ìƒì„¸"] = extract_credit_details(html_text)
    return result

def fetch_and_display_reports(companies_map):
    def extract_table_after_marker(soup, marker_str):
        marker = None
        for tag in soup.find_all(['b', 'strong', 'h2', 'h3', 'span']):
            if marker_str in tag.get_text():
                marker = tag
                break
        return marker.find_next('table') if marker else None

    def parse_grade_table_html(table_tag):
        try:
            dfs = pd.read_html(str(table_tag), header=[0, 1])
            df = dfs[0]
            df.columns = [
                '_'.join([str(l) for l in col if str(l) not in ['nan', 'None']]).strip()
                for col in df.columns.values
            ]
            if all(('Unnamed' in col or col == '' or col.lower() == 'none') for col in df.columns):
                raise Exception("í—¤ë” íŒŒì‹± ì‹¤íŒ¨ - ë‹¨ì¼ë¼ì¸ í—¤ë” ì‹œë„")
            return df
        except Exception:
            try:
                dfs = pd.read_html(str(table_tag), header=0)
                df = dfs[0]
                df.columns = [str(col).strip() for col in df.columns]
                return df
            except Exception:
                try:
                    rows = [
                        [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]
                        for row in table_tag.find_all('tr')
                    ]
                    df = pd.DataFrame(rows[1:], columns=rows[0])
                    return df
                except Exception:
                    return pd.DataFrame()

    def table_to_list(table):
        rows = []
        if not table:
            return rows
        for row in table.find_all('tr'):
            cells = [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]
            if cells:
                rows.append(cells)
        return rows

    def fetch_nice_rating_data(cmpCd):
        if not cmpCd:
            return {"major_grade_df": pd.DataFrame(), "special_reports": []}
        url = f"https://www.nicerating.com/disclosure/companyGradeInfo.do?cmpCd={cmpCd}"
        try:
            resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=20)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
            major_grade_table_tag = extract_table_after_marker(soup, 'ì£¼ìš” ë“±ê¸‰ë‚´ì—­')
            special_report_table_tag = extract_table_after_marker(soup, 'ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸')
            major_grade_df = parse_grade_table_html(major_grade_table_tag) if major_grade_table_tag else pd.DataFrame()
            special_reports = table_to_list(special_report_table_tag) if special_report_table_tag else []
            return {"major_grade_df": major_grade_df, "special_reports": special_reports}
        except Exception as e:
            return {"major_grade_df": pd.DataFrame(), "special_reports": [], "error": f"ë‚˜ì´ìŠ¤ ì‹ ìš©í‰ê°€ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}"}

    st.markdown("---")
    st.markdown("### ğŸ“‘ ì‹ ìš©í‰ê°€ ë³´ê³ ì„œ ë° ê´€ë ¨ ë¦¬ì„œì¹˜")

    for cat in favorite_categories:
        for company in favorite_categories[cat]:
            kiscd = companies_map.get(company, "")
            cmpcd = config.get("cmpCD_map", {}).get(company, "")
            kr_compcd = kr_compcd_map.get(company, "")
            if not kiscd or not str(kiscd).strip():
                continue

            url_kis = f"https://www.kisrating.com/ratingsSearch/corp_overview.do?kiscd={kiscd}"
            url_nice = f"https://www.nicerating.com/disclosure/companyGradeInfo.do?cmpCd={cmpcd}"
            url_kie = f"https://www.korearatings.com/cms/frDisclosureCon/compView.do?MENU_ID=90&CONTENTS_NO=1&COMP_CD={kr_compcd}"

            with st.expander(f"{company} (KISCD: {kiscd} | CMP_CD: {cmpcd} | KIE_CD: {kr_compcd})", expanded=False):
                st.markdown(
                    f"- [í•œêµ­ì‹ ìš©í‰ê°€ (KIS)]({url_kis}) &nbsp;&nbsp; "
                    f"[ë‚˜ì´ìŠ¤ì‹ ìš©í‰ê°€ (NICE)]({url_nice}) &nbsp;&nbsp; "
                    f"[í•œêµ­ê¸°ì—…í‰ê°€ (KIE)]({url_kie})",
                    unsafe_allow_html=True
                )
                try:
                    resp = requests.get(url_kis, timeout=20, headers={"User-Agent": "Mozilla/5.0"})
                    if resp.status_code == 200:
                        html_text = resp.text
                        report_data = extract_reports_and_research(html_text)

                        if report_data.get("í‰ê°€ë¦¬í¬íŠ¸"):
                            with st.expander("í‰ê°€ë¦¬í¬íŠ¸", expanded=True):
                                df_report = pd.DataFrame(report_data["í‰ê°€ë¦¬í¬íŠ¸"])
                                st.dataframe(df_report)

                        if report_data.get("ê´€ë ¨ë¦¬ì„œì¹˜"):
                            with st.expander("ê´€ë ¨ë¦¬ì„œì¹˜", expanded=True):
                                df_research = pd.DataFrame(report_data["ê´€ë ¨ë¦¬ì„œì¹˜"])
                                df_research = df_research.drop(columns=["ë‹¤ìš´ë¡œë“œ"], errors="ignore")
                                st.dataframe(df_research)

                                nice_data = fetch_nice_rating_data(cmpcd)
                                special_reports = nice_data.get("special_reports", [])
                                st.markdown("#### ë‚˜ì´ìŠ¤ ì‹ ìš©í‰ê°€ ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸")
                                if special_reports and len(special_reports) > 1:
                                    header = special_reports[0]
                                    filtered_rows = [row for row in special_reports[1:] if len(row) == len(header)]
                                    if filtered_rows:
                                        df_special = pd.DataFrame(filtered_rows, columns=header)
                                        st.dataframe(df_special)
                                    else:
                                        st.info("í‘œ í˜•ì‹ì´ ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸)")
                                else:
                                    st.info("ìŠ¤í˜ì…œ ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                if nice_data.get("error"):
                                    st.warning(nice_data["error"])

                        credit_detail_list = extract_credit_details(html_text)
                        with st.expander("ì‹ ìš©ë“±ê¸‰ ìƒì„¸ì •ë³´", expanded=True):
                            if credit_detail_list:
                                df_credit_detail = pd.DataFrame(credit_detail_list)
                                st.dataframe(df_credit_detail)
                            else:
                                st.info("ì‹ ìš©ë“±ê¸‰ ìƒì„¸ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

                            st.markdown("#### ë‚˜ì´ìŠ¤ ì‹ ìš©í‰ê°€ ì£¼ìš” ë“±ê¸‰ë‚´ì—­")
                            nice_data = fetch_nice_rating_data(cmpcd)
                            major_grade_df = nice_data.get("major_grade_df", pd.DataFrame())
                            if not major_grade_df.empty:
                                st.dataframe(major_grade_df)
                            else:
                                st.info("ì£¼ìš” ë“±ê¸‰ë‚´ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            if nice_data.get("error"):
                                st.warning(nice_data["error"])
                    else:
                        st.warning("í•œêµ­ì‹ ìš©í‰ê°€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.warning(f"ì‹ ìš©í‰ê°€ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")
                time.sleep(1)


# =========================================================
# 11. UI / ì„¸ì…˜ init / CSS
# =========================================================
st.set_page_config(layout="wide")

def init_session_state():
    defaults = {
        "favorite_keywords": set(),
        "search_results": {},
        "show_limit": {},
        "search_triggered": False,
        "selected_articles": [],
        "cat_multi": [],
        "cat_major_autoset": [],
        "important_articles_preview": [],
        "important_selected_index": [],
        "industry_major_sub_map": {},
        "end_date": datetime.today().date(),
        "start_date": datetime.today().date() - timedelta(days=7),
        "remove_duplicate_articles": True,
        "require_exact_keyword_in_title_or_content": True,
        "filter_allowed_sources_only": False,
        "use_industry_filter": True,
        "show_sentiment_badge": False,
        "enable_summary": True,
        "keyword_input": "",
        "use_llm_filter": True,
        "llm_candidate_cap": 200,
        "llm_top_k": 10,
        "use_industry_issue_llm": True,
        "industry_issue_cap": 300,
        "industry_issue_top_k": 8,
        "search_run_id": 0,
        "industry_major_top_cache": {},
        "industry_major_top_cache_run_id": -1,

        # âœ… PATCH í•µì‹¬: ì²´í¬ ìƒíƒœë¥¼ ë‹¨ì¼ ë”•ì…”ë„ˆë¦¬ë¡œ í†µí•©
        # key = f"{company}_{uid}"  value = True/False
        "selected_news": {},

        # ì„ íƒ ë™ì‘ ì¤‘ë³µ rerun ë°©ì§€ìš©
        "last_toggle_key": "",
        "last_toggle_value": None,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

init_session_state()

st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] {
    background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold;
}
.sentiment-badge {
    display: inline-block; padding: 0.08em 0.6em; margin-left: 0.2em;
    border-radius: 0.8em; font-size: 0.85em; font-weight: bold; vertical-align: middle;
}
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.sentiment-neutral  { background: #6c757d; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em;
    padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
.news-title {
    word-break: break-all !important; white-space: normal !important; display: block !important; overflow: visible !important;
}
</style>
""", unsafe_allow_html=True)


# =========================================================
# 12. ìƒë‹¨ ì…ë ¥ UI
# =========================================================
col_title, col_option1, col_option2 = st.columns([0.5, 0.2, 0.3])
with col_title:
    st.markdown(
        "<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>"
        "<a href='https://credit-issue-monitoring-news-sending.onrender.com/' target='_blank' style='text-decoration:none; color:#1a1a1a;'>"
        "ğŸ“Š Credit Issue Monitoring</a></h1>",
        unsafe_allow_html=True
    )
with col_option1:
    st.checkbox("ê°ì„±ë¶„ì„ ë°°ì§€í‘œì‹œ", key="show_sentiment_badge")
with col_option2:
    st.checkbox("ìš”ì•½ ê¸°ëŠ¥", key="enable_summary")

col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input(label="", value="", key="keyword_input", label_visibility="collapsed")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

st.markdown("**â­ ì‚°ì—…êµ° ì„ íƒ**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        "",
        list(favorite_categories.keys()),
        key="cat_multi",
        label_visibility="collapsed"
    )
if selected_categories:
    auto_selected_majors = get_industry_majors_from_favorites(selected_categories)
    st.session_state.cat_major_autoset = auto_selected_majors.copy()
else:
    st.session_state.cat_major_autoset = []

with col_cat_btn:
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", key="cat_search_btn", help="ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰", use_container_width=True)

for cat in selected_categories:
    st.session_state.favorite_keywords.update(favorite_categories[cat])

date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼", value=st.session_state["start_date"], key="start_date_input")
    st.session_state["start_date"] = start_date
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼", value=st.session_state["end_date"], key="end_date_input")
    st.session_state["end_date"] = end_date

with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©ë¨)"):
    for major, subs in common_filter_categories.items():
        st.markdown(f"**{major}**: {', '.join(subs)}")

with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ëŒ€ë¶„ë¥˜ë³„ ì†Œë¶„ë¥˜ í•„í„°ë§)"):
    st.checkbox("ì´ í•„í„° ì ìš©", key="use_industry_filter")
    selected_major_map = get_industry_majors_from_favorites(selected_categories)

    updated_map = {}
    for major in selected_major_map:
        options = industry_filter_categories.get(major, [])
        default_selected = options if major not in st.session_state.industry_major_sub_map else st.session_state.industry_major_sub_map[major]
        selected_sub = st.multiselect(
            f"{major} ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ",
            options,
            default=default_selected,
            key=f"subfilter_{major}"
        )
        updated_map[major] = selected_sub
    st.session_state.industry_major_sub_map = updated_map

with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    st.checkbox("í‚¤ì›Œë“œê°€ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", key="require_exact_keyword_in_title_or_content")
    st.checkbox("ì¤‘ë³µ ê¸°ì‚¬ ì œê±°", key="remove_duplicate_articles", help="í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.")
    st.checkbox("íŠ¹ì • ì–¸ë¡ ì‚¬ë§Œ ê²€ìƒ‰", key="filter_allowed_sources_only", help="ì„ íƒëœ ë©”ì´ì € ì–¸ë¡ ì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.")

    st.checkbox(
        "LLM ì¤‘ìš”ë„ í•„í„° ì ìš©(ì „ì²´ ê¸°ì—…)",
        key="use_llm_filter",
        help="ê¸°ì—…ë³„ ìµœì‹  capê±´ì„ LLMì´ 1~5ì  í‰ê°€ í›„ ìƒìœ„ top_kë§Œ ë³´ì¡´"
    )
    st.number_input(
        "LLM í‰ê°€ í›„ë³´ cap(ìµœì‹ ìˆœ)",
        min_value=10, max_value=200, step=5,
        key="llm_candidate_cap"
    )
    st.number_input(
        "LLM ìƒìœ„ ê¸°ì‚¬ ê°œìˆ˜(top_k)",
        min_value=3, max_value=20, step=1,
        key="llm_top_k"
    )

    st.markdown("---")

    st.checkbox(
        "ì‚°ì—…êµ°ë³„ ì£¼ìš”ì´ìŠˆ LLM í•„í„° ì ìš©",
        key="use_industry_issue_llm",
        help="ê¸°ì—…ë³„ ìµœì¢… ê¸°ì‚¬ë“¤ì„ ì‚°ì—… ëŒ€ë¶„ë¥˜ë¡œ í•©ì³ top_k ì„ ì •"
    )
    st.number_input(
        "ì‚°ì—…êµ°ë³„ LLM í›„ë³´ cap(ìµœì‹ ìˆœ)",
        min_value=50, max_value=500, step=10,
        key="industry_issue_cap"
    )
    st.number_input(
        "ì‚°ì—…êµ°ë³„ LLM ìƒìœ„ ê¸°ì‚¬ ê°œìˆ˜(top_k)",
        min_value=3, max_value=20, step=1,
        key="industry_issue_top_k"
    )


# =========================================================
# 13. ê²€ìƒ‰ ì‹¤í–‰
# =========================================================
keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()] if keywords_input else []

if search_clicked and keyword_list:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        expanded = expand_keywords_with_synonyms(sorted(keyword_list))
        process_keywords_with_synonyms(
            expanded,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )
    st.session_state.search_run_id += 1

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])

        expanded = expand_keywords_with_synonyms(sorted(keywords))
        process_keywords_with_synonyms(
            expanded,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_exact_keyword_in_title_or_content", False)
        )
    st.session_state.search_run_id += 1


# =========================================================
# 14. ì¢Œì¸¡/ìš°ì¸¡ ë Œë” (PATCH)
#    - ì²´í¬ ìƒíƒœ ë‹¨ì¼í™”(selected_news)
#    - ìœ„ì ¯ keyë¥¼ í•­ëª©ë³„ ê³ ì •
#    - ë§ˆìŠ¤í„° í† ê¸€ 1íšŒë§Œ rerun
# =========================================================
def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True, enable_summary=True):
    SENTIMENT_CLASS = {"ê¸ì •": "sentiment-positive", "ë¶€ì •": "sentiment-negative"}

    col_list, col_summary = st.columns([1, 1])

    # ------------------------------
    # ì¢Œì¸¡: ë‰´ìŠ¤ ëª©ë¡ + ì²´í¬
    # ------------------------------
    with col_list:
        st.markdown("### ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼")

        # (A) ì‚°ì—… ëŒ€ë¶„ë¥˜ top_k
        if st.session_state.get("use_industry_issue_llm", True):
            if st.session_state.industry_major_top_cache_run_id != st.session_state.search_run_id:
                major_pool = build_industry_major_article_pool(results)
                cache = {}
                if major_pool:
                    with ThreadPoolExecutor(max_workers=min(8, len(major_pool))) as exe:
                        futures = {
                            exe.submit(llm_filter_and_rank_industry_major, major_name, major_articles): major_name
                            for major_name, major_articles in major_pool.items()
                        }
                        for fut in as_completed(futures):
                            major_name = futures[fut]
                            try:
                                cache[major_name] = fut.result()
                            except Exception:
                                cache[major_name] = []
                st.session_state.industry_major_top_cache = cache
                st.session_state.industry_major_top_cache_run_id = st.session_state.search_run_id

            cached_major_top = st.session_state.get("industry_major_top_cache", {})
            if cached_major_top:
                with st.expander("ğŸŸ£ ì‚°ì—…êµ°ë³„ ì£¼ìš” ì´ìŠˆ(top_k)", expanded=True):
                    for major_name, major_top in cached_major_top.items():
                        with st.expander(f"ğŸ­ {major_name} ({len(major_top)}ê±´)", expanded=False):
                            for art in major_top:
                                company_tag = (art.get("í‚¤ì›Œë“œ") or "").strip()
                                uid = make_uid(art["link"])
                                key = f"{company_tag}_{uid}" if company_tag else f"industry_{major_name}_{uid}"

                                # ê³ ì • ìœ„ì ¯ key
                                widget_key = f"major_chk_{key}"
                                checked = st.checkbox(
                                    "",
                                    value=st.session_state.selected_news.get(key, False),
                                    key=widget_key
                                )
                                st.session_state.selected_news[key] = checked

                                llm_info = f" | LLMì ìˆ˜:{art.get('llm_score')}ì " if art.get("llm_score") else ""
                                company_info = f" | ê¸°ì—…:{company_tag}" if company_tag else ""
                                st.markdown(
                                    f"<span class='news-title'><a href='{art['link']}' target='_blank'>{art['title']}</a></span> "
                                    f"{art['date']} | {art['source']}{company_info}{llm_info}",
                                    unsafe_allow_html=True
                                )
            else:
                st.info("ì‚°ì—…êµ°ë³„ ì£¼ìš” ì´ìŠˆë¥¼ ë§Œë“¤ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # (B) ê¸°ì—…/ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸
        for category_name, company_list in favorite_categories.items():
            companies_with_results = [c for c in company_list if c in results]
            if not companies_with_results:
                continue

            with st.expander(f"ğŸ“‚ {category_name}", expanded=True):
                for company in companies_with_results:
                    articles = results[company]
                    with st.expander(f"[{company}] ({len(articles)}ê±´)", expanded=False):

                        all_article_keys = []
                        for art in articles:
                            uid = make_uid(art["link"])
                            key = f"{company}_{uid}"
                            all_article_keys.append(key)

                        current_key_set = set(all_article_keys)

                        # stale key ì œê±°
                        for k in list(st.session_state.selected_news.keys()):
                            if k.startswith(f"{company}_") and k not in current_key_set:
                                st.session_state.selected_news.pop(k, None)

                        # ë§ˆìŠ¤í„° ì²´í¬ë°•ìŠ¤
                        slug = re.sub(r"\W+", "", f"{category_name}_{company}")
                        master_key = f"left_master_{slug}_select_all"

                        prev_value = all(st.session_state.selected_news.get(k, False) for k in all_article_keys)

                        select_all = st.checkbox(
                            f"ì „ì²´ ê¸°ì‚¬ ì„ íƒ/í•´ì œ ({company})",
                            value=prev_value,
                            key=master_key
                        )

                        if select_all != prev_value:
                            # ìƒíƒœ ë°˜ì˜
                            for k in all_article_keys:
                                st.session_state.selected_news[k] = select_all
                                # ê°œë³„ ìœ„ì ¯ë„ ì¦‰ì‹œ ë°˜ì˜ë˜ë„ë¡ key ê°’ ê°±ì‹ 
                                st.session_state[f"left_chk_{k}"] = select_all
                            st.rerun()

                        # ê°œë³„ ê¸°ì‚¬
                        for art in articles:
                            uid = make_uid(art["link"])
                            key = f"{company}_{uid}"
                            widget_key = f"left_chk_{key}"

                            checked = st.checkbox(
                                "",
                                value=st.session_state.selected_news.get(key, False),
                                key=widget_key
                            )
                            st.session_state.selected_news[key] = checked

                            cache_key = get_summary_key_from_url(art["link"], target_keyword=company)

                            sentiment = ""
                            if show_sentiment_badge and cache_key in st.session_state:
                                _, _, sentiment, _, _, _ = st.session_state[cache_key]

                            badge_html = (
                                f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment,'sentiment-neutral')}'>{sentiment}</span>"
                                if sentiment else ""
                            )

                            llm_info = f" | LLMì ìˆ˜:{art.get('llm_score')}ì " if art.get("llm_score") else ""
                            search_kw = f" | ê²€ìƒ‰ì–´:{art.get('ê²€ìƒ‰ì–´')}" if art.get('ê²€ìƒ‰ì–´') else ""

                            st.markdown(
                                f"<span class='news-title'><a href='{art['link']}' target='_blank'>{art['title']}</a></span> "
                                f"{badge_html} {art['date']} | {art['source']}{search_kw}{llm_info}",
                                unsafe_allow_html=True,
                            )

    # ------------------------------
    # ìš°ì¸¡: ì„ íƒ ê¸°ì‚¬ ìš”ì•½/ê°ì„±
    # ------------------------------
    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):

            industry_keywords_all = []
            if st.session_state.get("use_industry_filter"):
                for sublist in st.session_state.industry_major_sub_map.values():
                    industry_keywords_all.extend(sublist)

            grouped_selected = {}

            # ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€ ì„ íƒ ìˆ˜ì§‘
            for cat_name, comp_list in favorite_categories.items():
                for company in comp_list:
                    if company in results:
                        for art in results[company]:
                            uid = make_uid(art["link"])
                            key = f"{company}_{uid}"
                            if st.session_state.selected_news.get(key, False):
                                grouped_selected.setdefault(cat_name, {}).setdefault(company, []).append((company, uid, art))

            # ì‚°ì—…êµ° major topì—ì„œ ì„ íƒ ìˆ˜ì§‘
            cached_major_top = st.session_state.get("industry_major_top_cache", {})
            for major_name, major_top in cached_major_top.items():
                for art in major_top:
                    company = (art.get("í‚¤ì›Œë“œ") or "").strip()
                    if not company:
                        continue
                    uid = make_uid(art["link"])
                    key = f"{company}_{uid}"
                    if st.session_state.selected_news.get(key, False):
                        grouped_selected.setdefault("ì‚°ì—…êµ°ë³„ ì£¼ìš”ì´ìŠˆ", {}).setdefault(company, []).append((company, uid, art))

            # ìš”ì•½ ì²˜ë¦¬
            def process_article(item):
                company, uid, art = item
                cache_key = get_summary_key_from_url(art["link"], target_keyword=company)

                if cache_key in st.session_state:
                    one_line, summary, sentiment, implication, short_implication, full_text = st.session_state[cache_key]
                else:
                    one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(
                        art["link"],
                        art["title"],
                        do_summary=enable_summary,
                        target_keyword=company,
                        description=art.get("description"),
                    )
                    st.session_state[cache_key] = (one_line, summary, sentiment, implication, short_implication, full_text)

                filter_hits = matched_filter_keywords(
                    {"title": art["title"], "ìš”ì•½ë³¸": summary, "ìš”ì•½": one_line, "full_text": full_text},
                    ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords_all,
                )

                return {
                    "í‚¤ì›Œë“œ": company,
                    "í•„í„°íˆíŠ¸": ", ".join(filter_hits),
                    "ê¸°ì‚¬ì œëª©": safe_title(art["title"]),
                    "ìš”ì•½": one_line,
                    "ìš”ì•½ë³¸": summary,
                    "ê°ì„±": sentiment,
                    "ì‹œì‚¬ì ": implication,
                    "í•œì¤„ì‹œì‚¬ì ": short_implication,
                    "ë§í¬": art["link"],
                    "ë‚ ì§œ": art["date"],
                    "ì¶œì²˜": art["source"],
                    "full_text": full_text,
                }

            # ë³‘ë ¬ ìš”ì•½(ì„ íƒëœ ê²ƒë§Œ)
            for cat_name, comp_map in grouped_selected.items():
                for company, items in comp_map.items():
                    with ThreadPoolExecutor(max_workers=8) as exe:
                        grouped_selected[cat_name][company] = list(exe.map(process_article, items))

            # ì—‘ì…€ìš© ì €ì¥
            flattened = []
            for _cat, comp_map in grouped_selected.items():
                for _comp, arts in comp_map.items():
                    flattened.extend(arts)
            st.session_state.selected_articles = flattened

            # ë Œë”
            total_selected = 0
            for cat_name, comp_map in grouped_selected.items():
                with st.expander(f"ğŸ“‚ {cat_name}", expanded=True):
                    for company, arts in comp_map.items():
                        with st.expander(f"[{company}] ({len(arts)}ê±´)", expanded=True):
                            for art in arts:
                                total_selected += 1
                                st.markdown(
                                    f"#### <a href='{art['ë§í¬']}' target='_blank'>{art['ê¸°ì‚¬ì œëª©']}</a> "
                                    f"<span class='sentiment-badge {SENTIMENT_CLASS.get(art['ê°ì„±'],'sentiment-neutral')}'>{art['ê°ì„±']}</span>",
                                    unsafe_allow_html=True,
                                )
                                st.markdown(f"- **ê²€ìƒ‰ í‚¤ì›Œë“œ:** `{art['í‚¤ì›Œë“œ']}`")
                                st.markdown(f"- **í•„í„° íˆíŠ¸:** `{art['í•„í„°íˆíŠ¸'] or 'ì—†ìŒ'}`")
                                st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {art['ë‚ ì§œ']} | {art['ì¶œì²˜']}")
                                st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {art['ìš”ì•½']}")
                                st.markdown(f"- **í•œ ì¤„ ì‹œì‚¬ì :** {art['í•œì¤„ì‹œì‚¬ì ']}")
                                st.markdown(f"- **ì‹œì‚¬ì :** {art['ì‹œì‚¬ì ']}")
                                st.markdown("---")

            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {total_selected}")

            col_dl1, col_dl2 = st.columns([0.55, 0.45])
            with col_dl1:
                st.download_button(
                    label="ğŸ“¥ ë§ì¶¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=get_excel_download_with_favorite_and_excel_company_col(
                        st.session_state.selected_articles,
                        favorite_categories,
                        excel_company_categories,
                        st.session_state.search_results,
                    ).getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë§ì¶¤í˜•.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            with col_dl2:
                if st.button("ğŸ—‘ ì„ íƒ í•´ì œ (ì „ì²´)"):
                    for k in list(st.session_state.selected_news.keys()):
                        st.session_state.selected_news[k] = False
                    # ê°œë³„ ìœ„ì ¯ë„ ë™ê¸°í™”
                    for sk in list(st.session_state.keys()):
                        if sk.startswith("left_chk_") or sk.startswith("major_chk_"):
                            st.session_state[sk] = False
                    st.rerun()

        render_important_article_review_and_download()


# =========================================================
# 15. ì¤‘ìš”ê¸°ì‚¬ ë¦¬ë·°/ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€ + rerun í†µì¼)
# =========================================================
def render_important_article_review_and_download():
    import re
    from collections import defaultdict

    with st.container(border=True):
        st.markdown("### â­ ì¤‘ìš” ê¸°ì‚¬ ë¦¬ë·° ë° í¸ì§‘")

        auto_btn = st.button("ğŸš€ OpenAI ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ìë™ ì„ ì •")
        if auto_btn:
            with st.spinner("OpenAIë¡œ ì¤‘ìš” ë‰´ìŠ¤ ì„ ì • ì¤‘..."):
                filtered_results_for_important = {}
                for keyword, articles in st.session_state.search_results.items():
                    filtered_articles = [a for a in articles if article_passes_all_filters(a)]
                    if st.session_state.get("remove_duplicate_articles", False):
                        filtered_articles = remove_duplicates(filtered_articles)
                    if filtered_articles:
                        filtered_results_for_important[keyword] = filtered_articles

                industry_keywords_all = []
                if st.session_state.get("use_industry_filter", False):
                    for sublist in st.session_state.industry_major_sub_map.values():
                        industry_keywords_all.extend(sublist)

                important_articles = generate_important_article_list(
                    search_results=filtered_results_for_important,
                    common_keywords=ALL_COMMON_FILTER_KEYWORDS,
                    industry_keywords=industry_keywords_all,
                    favorites=favorite_categories
                )
                for i, art in enumerate(important_articles):
                    important_articles[i] = {
                        "í‚¤ì›Œë“œ": art.get("í‚¤ì›Œë“œ") or "",
                        "ê¸°ì‚¬ì œëª©": art.get("ê¸°ì‚¬ì œëª©") or "",
                        "ê°ì„±": art.get("ê°ì„±", ""),
                        "ë§í¬": art.get("ë§í¬") or "",
                        "ë‚ ì§œ": art.get("ë‚ ì§œ") or "",
                        "ì¶œì²˜": art.get("ì¶œì²˜") or "",
                        "ì‹œì‚¬ì ": art.get("ì‹œì‚¬ì ", "")
                    }
                st.session_state["important_articles_preview"] = important_articles
                st.session_state["important_selected_index"] = []

        articles = st.session_state.get("important_articles_preview", [])
        selected_indexes = st.session_state.get("important_selected_index", [])

        major_map = defaultdict(lambda: defaultdict(list))
        for art in articles:
            keyword = art.get("í‚¤ì›Œë“œ") or ""
            found_major = None
            for major, minors in favorite_categories.items():
                if keyword in minors:
                    found_major = major
                    break
            if found_major:
                major_map[found_major][keyword].append(art)

        st.markdown("ğŸ¯ **ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡ (êµì²´ ë˜ëŠ” ì‚­ì œí•  í•­ëª©ì„ ì²´í¬í•˜ì„¸ìš”)**")

        from concurrent.futures import ThreadPoolExecutor
        one_line_map = {}
        to_summarize = []

        for major, minor_map in major_map.items():
            for minor, arts in minor_map.items():
                for idx, article in enumerate(arts):
                    link = article.get("ë§í¬", "")
                    cache_key = get_summary_key_from_url(link, target_keyword=minor)
                    if cache_key in st.session_state and isinstance(st.session_state[cache_key], tuple):
                        one_line_map[(major, minor, idx)] = st.session_state[cache_key]
                    else:
                        if link:
                            to_summarize.append((major, minor, idx, link, article.get("ê¸°ì‚¬ì œëª©", "")))

        if to_summarize:
            with st.spinner("ì¤‘ìš” ê¸°ì‚¬ ìš”ì•½ ìƒì„± ì¤‘..."):
                def get_one_line(args):
                    major, minor, idx, link, title = args
                    one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(
                        link, title, do_summary=True, target_keyword=minor
                    )
                    cache_key = get_summary_key_from_url(link, target_keyword=minor)
                    st.session_state[cache_key] = (one_line, summary, sentiment, implication, short_implication, full_text)
                    return (major, minor, idx), (one_line, summary, sentiment, implication, short_implication, full_text)

                with ThreadPoolExecutor(max_workers=8) as executor:
                    for key, data_tuple in executor.map(get_one_line, to_summarize):
                        one_line_map[key] = data_tuple

        new_selection = []
        for major, minor_map in major_map.items():
            with st.expander(f"ğŸ“Š {major}", expanded=True):
                for minor, arts in minor_map.items():
                    with st.expander(f"{minor} ({len(arts)}ê±´)", expanded=False):
                        for idx, article in enumerate(arts):
                            uid = make_uid(article.get("ë§í¬",""))
                            check_key = f"important_chk_{major}_{minor}_{uid}"

                            cols = st.columns([0.06, 0.94])
                            with cols[0]:
                                checked = st.checkbox(
                                    "",
                                    key=check_key,
                                    value=((major, minor, idx) in selected_indexes),
                                )
                            with cols[1]:
                                st.markdown(
                                    f"{article.get('ê°ì„±','')} | "
                                    f"<a href='{article.get('ë§í¬','')}' target='_blank'>"
                                    f"{article.get('ê¸°ì‚¬ì œëª©','ì œëª©ì—†ìŒ')}</a>",
                                    unsafe_allow_html=True,
                                )

                                summary_data = one_line_map.get((major, minor, idx))
                                implication_text = ""
                                short_implication_text = ""

                                if summary_data and len(summary_data) == 6:
                                    implication_text = summary_data[3] or ""
                                    short_implication_text = summary_data[4] or ""
                                else:
                                    implication_text = article.get("ì‹œì‚¬ì ", "") or ""
                                    short_implication_text = article.get("í•œì¤„ì‹œì‚¬ì ", "") or ""

                                if implication_text:
                                    st.markdown(implication_text)
                                if short_implication_text:
                                    st.markdown(
                                        f"<span style='color:gray;font-style:italic;'>{short_implication_text}</span>",
                                        unsafe_allow_html=True,
                                    )

                                st.markdown(
                                    f"<span style='font-size:12px;color:#99a'>"
                                    f"{article.get('ë‚ ì§œ', '')} | {article.get('ì¶œì²˜', '')}</span>",
                                    unsafe_allow_html=True,
                                )
                                st.markdown("<div style='margin:0px;padding:0px;height:4px'></div>", unsafe_allow_html=True)

                            if checked:
                                new_selection.append((major, minor, idx))

        st.session_state["important_selected_index"] = new_selection

        col_add, col_del, col_rep = st.columns([0.3, 0.35, 0.35])

        def extract_keyword_from_link(search_results, article_link):
            for kw, arts in search_results.items():
                for art in arts:
                    if art.get("link") == article_link:
                        return kw
            return ""

        with col_add:
            if st.button("â• ì„ íƒ ê¸°ì‚¬ ì¶”ê°€"):
                left_selected_keys = [k for k, v in st.session_state.selected_news.items() if v]
                if not left_selected_keys:
                    st.warning("ì™¼ìª½ ë‰´ìŠ¤ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì ì–´ë„ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")
                else:
                    added_count = 0
                    important = st.session_state.get("important_articles_preview", [])

                    for from_key in left_selected_keys:
                        # from_key í˜•ì‹: company_uid
                        m = re.match(r"^([^_]+)_(.+)$", from_key)
                        if not m:
                            continue
                        company = m.group(1)
                        uid_tail = m.group(2)

                        selected_article = None
                        for kw, arts in st.session_state.search_results.items():
                            for art in arts:
                                if make_uid(art["link"]) == uid_tail:
                                    selected_article = art
                                    break
                            if selected_article:
                                break
                        if not selected_article:
                            continue

                        keyword = extract_keyword_from_link(st.session_state.search_results, selected_article["link"])
                        cache_key = get_summary_key_from_url(selected_article["link"], target_keyword=keyword)

                        if cache_key in st.session_state:
                            sentiment = st.session_state[cache_key][2]
                        else:
                            _, _, sentiment, _, _, _ = summarize_article_from_url(
                                selected_article["link"],
                                selected_article["title"],
                                target_keyword=keyword
                            )

                        new_article = {
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": selected_article["title"],
                            "ê°ì„±": sentiment or "",
                            "ë§í¬": selected_article["link"],
                            "ë‚ ì§œ": selected_article["date"],
                            "ì¶œì²˜": selected_article["source"],
                            "ì‹œì‚¬ì ": ""
                        }

                        if not any(a["ë§í¬"] == new_article["ë§í¬"] for a in important):
                            important.append(new_article)
                            added_count += 1

                        st.session_state.selected_news[from_key] = False

                    st.session_state["important_articles_preview"] = important
                    st.session_state["important_selected_index"] = []
                    if added_count > 0:
                        st.success(f"{added_count}ê±´ì˜ ê¸°ì‚¬ê°€ ì¤‘ìš” ê¸°ì‚¬ ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.info("ì¶”ê°€ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.rerun()

        with col_del:
            if st.button("ğŸ—‘ ì„ íƒ ê¸°ì‚¬ ì‚­ì œ"):
                important = st.session_state.get("important_articles_preview", [])
                remove_links = []
                for major, minor, idx in st.session_state["important_selected_index"]:
                    try:
                        link = major_map[major][minor][idx]["ë§í¬"]
                        remove_links.append(link)
                    except Exception:
                        continue
                important = [a for a in important if a.get("ë§í¬") not in remove_links]
                st.session_state["important_articles_preview"] = important
                st.session_state["important_selected_index"] = []
                st.rerun()

        with col_rep:
            if st.button("ğŸ” ì„ íƒ ê¸°ì‚¬ êµì²´"):
                left_selected_keys = [k for k, v in st.session_state.selected_news.items() if v]
                right_selected_indexes = st.session_state["important_selected_index"]
                if len(left_selected_keys) != 1 or len(right_selected_indexes) != 1:
                    st.warning("ì™¼ìª½ 1ê°œ, ì˜¤ë¥¸ìª½ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    return

                from_key = left_selected_keys[0]
                (target_major, target_minor, target_idx) = right_selected_indexes[0]

                m = re.match(r"^([^_]+)_(.+)$", from_key)
                if not m:
                    st.warning("ê¸°ì‚¬ ì‹ë³„ì íŒŒì‹± ì‹¤íŒ¨")
                    return
                uid_tail = m.group(2)

                selected_article = None
                for kw, art_list in st.session_state.search_results.items():
                    for art in art_list:
                        if make_uid(art["link"]) == uid_tail:
                            selected_article = art
                            break
                    if selected_article:
                        break
                if not selected_article:
                    st.warning("ì™¼ìª½ì—ì„œ ì„ íƒí•œ ê¸°ì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return

                keyword = extract_keyword_from_link(st.session_state.search_results, selected_article["link"])
                cache_key = get_summary_key_from_url(selected_article["link"], target_keyword=keyword)

                if cache_key in st.session_state:
                    sentiment = st.session_state[cache_key][2]
                else:
                    _, _, sentiment, _, _, _ = summarize_article_from_url(
                        selected_article["link"],
                        selected_article["title"],
                        target_keyword=keyword
                    )

                important = st.session_state.get("important_articles_preview", [])
                remove_link = major_map[target_major][target_minor][target_idx]["ë§í¬"]
                important = [a for a in important if a.get("ë§í¬") != remove_link]

                new_article = {
                    "í‚¤ì›Œë“œ": keyword,
                    "ê¸°ì‚¬ì œëª©": selected_article["title"],
                    "ê°ì„±": sentiment or "",
                    "ë§í¬": selected_article["link"],
                    "ë‚ ì§œ": selected_article["date"],
                    "ì¶œì²˜": selected_article["source"],
                    "ì‹œì‚¬ì ": ""
                }
                important.append(new_article)

                st.session_state["important_articles_preview"] = important
                st.session_state.selected_news[from_key] = False
                st.session_state["important_selected_index"] = []
                st.success("ì¤‘ìš” ê¸°ì‚¬ êµì²´ ì™„ë£Œ")
                st.rerun()

        st.markdown("---")
        st.markdown("ğŸ“¥ **ë¦¬ë·°í•œ ì¤‘ìš” ê¸°ì‚¬ë“¤ì„ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.**")

        articles_source = st.session_state.get("important_articles_preview", [])
        industry_keywords_all = []
        if st.session_state.get("use_industry_filter", False):
            for sublist in st.session_state.industry_major_sub_map.values():
                industry_keywords_all.extend(sublist)

        def enrich_article_for_excel(raw_article):
            link = raw_article.get("ë§í¬", "")
            keyword = raw_article.get("í‚¤ì›Œë“œ", "")
            cache_key = get_summary_key_from_url(link, target_keyword=keyword)
            if cache_key in st.session_state and isinstance(st.session_state[cache_key], tuple):
                one_line, summary, sentiment, implication, short_implication, full_text = st.session_state[cache_key]
            else:
                one_line, summary, sentiment, implication, short_implication, full_text = summarize_article_from_url(
                    link, raw_article.get("ê¸°ì‚¬ì œëª©", ""), target_keyword=keyword
                )
                st.session_state[cache_key] = (one_line, summary, sentiment, implication, short_implication, full_text)

            filter_hits = matched_filter_keywords(
                {"title": raw_article.get("ê¸°ì‚¬ì œëª©", ""), "ìš”ì•½ë³¸": summary,
                 "ìš”ì•½": one_line, "full_text": full_text},
                ALL_COMMON_FILTER_KEYWORDS,
                industry_keywords_all
            )
            return {
                "í‚¤ì›Œë“œ": keyword,
                "í•„í„°íˆíŠ¸": ", ".join(filter_hits),
                "ê¸°ì‚¬ì œëª©": safe_title(raw_article.get("ê¸°ì‚¬ì œëª©", "")),
                "ìš”ì•½": one_line,
                "ìš”ì•½ë³¸": summary,
                "ê°ì„±": sentiment,
                "ì‹œì‚¬ì ": implication,
                "í•œì¤„ì‹œì‚¬ì ": short_implication,
                "ë§í¬": link,
                "ë‚ ì§œ": raw_article.get("ë‚ ì§œ", ""),
                "ì¶œì²˜": raw_article.get("ì¶œì²˜", ""),
                "full_text": full_text or "",
            }

        summary_data = [enrich_article_for_excel(a) for a in articles_source]

        def get_excel_with_joined_implications(summary_data, favorite_categories, excel_company_categories, search_results):
            if not summary_data or len(summary_data) == 0:
                df_empty = pd.DataFrame(columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])
                output = BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    df_empty.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
                    worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
                    worksheet.set_column(0, 5, 30)
                output.seek(0)
                return output

            df = pd.DataFrame(summary_data)

            sector_list = []
            for cat in favorite_categories:
                sector_list.extend(favorite_categories[cat])
            sector_list = list(dict.fromkeys(sector_list))

            excel_sector_list = []
            for cat in excel_company_categories:
                excel_sector_list.extend(excel_company_categories[cat])
            excel_sector_list = list(dict.fromkeys(excel_sector_list))

            rows = []
            for idx, company in enumerate(sector_list):
                search_articles = search_results.get(company, [])
                filtered_articles = [a for a in search_articles if article_passes_all_filters(a)]
                if st.session_state.get("remove_duplicate_articles", False):
                    filtered_articles = remove_duplicates(filtered_articles)
                if st.session_state.get("use_llm_filter", False):
                    top_k = st.session_state.get("llm_top_k", 10)
                    already_llm = (
                        len(filtered_articles) <= top_k and
                        all(("llm_score" in a) for a in filtered_articles)
                    )
                    if not already_llm:
                        filtered_articles = llm_filter_and_rank_articles(company, filtered_articles)

                total_count = len(filtered_articles)

                filtered_df = df[df.get("í‚¤ì›Œë“œ", "") == company].sort_values(by='ë‚ ì§œ', ascending=False)

                hl_news = ["", ""]
                implications = ["", ""]
                short_imps = ["", ""]

                for i, art in enumerate(filtered_df.itertuples()):
                    if i > 1:
                        break
                    date_val = getattr(art, "ë‚ ì§œ", "") or ""
                    title_val = getattr(art, "ê¸°ì‚¬ì œëª©", "") or getattr(art, "ì œëª©", "")
                    link_val = getattr(art, "ë§í¬", "") or getattr(art, "link", "")
                    short_imp_val = getattr(art, "í•œì¤„ì‹œì‚¬ì ", "") or ""

                    display_text = f"({clean_excel_formula_text(date_val)}){clean_excel_formula_text(title_val)}"
                    if title_val and link_val:
                        hl_news[i] = f'=HYPERLINK("{clean_excel_formula_text(link_val)}", "{display_text}")'
                    else:
                        hl_news[i] = display_text or ""

                    implications[i] = getattr(art, "ì‹œì‚¬ì ", "") or ""
                    short_imps[i] = short_imp_val

                merged_implications = ""
                for n in range(2):
                    if implications[n]:
                        merged_implications += f"{n+1}. {implications[n]}\n"
                for n in range(2):
                    if short_imps[n]:
                        merged_implications += f"{n+1}. {short_imps[n]}\n"

                rows.append({
                    "ê¸°ì—…ëª…": company,
                    "í‘œê¸°ëª…": excel_sector_list[idx] if idx < len(excel_sector_list) else "",
                    "ê±´ìˆ˜": total_count,
                    "ì¤‘ìš”ë‰´ìŠ¤1": hl_news[0],
                    "ì¤‘ìš”ë‰´ìŠ¤2": hl_news[1],
                    "ì‹œì‚¬ì ": merged_implications.strip(),
                })

            result_df = pd.DataFrame(rows, columns=["ê¸°ì—…ëª…", "í‘œê¸°ëª…", "ê±´ìˆ˜", "ì¤‘ìš”ë‰´ìŠ¤1", "ì¤‘ìš”ë‰´ìŠ¤2", "ì‹œì‚¬ì "])

            output = BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                result_df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
                worksheet = writer.sheets['ë‰´ìŠ¤ìš”ì•½']
                for i, col in enumerate(result_df.columns):
                    worksheet.set_column(i, i, 30)
            output.seek(0)
            return output

        excel_data = get_excel_with_joined_implications(
            summary_data,
            favorite_categories,
            excel_company_categories,
            st.session_state.search_results
        )

        st.download_button(
            label="ğŸ“¥ ì¤‘ìš” ê¸°ì‚¬ ìµœì¢… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ë§ì¶¤ ì–‘ì‹)",
            data=excel_data.getvalue(),
            file_name=f"ì¤‘ìš”ë‰´ìŠ¤_ìµœì¢…ì„ ì •_ì–‘ì‹_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )


# =========================================================
# 16. ë Œë” ì§ì „ í•„í„°ë§ + ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# =========================================================
if st.session_state.get("search_results"):
    filtered_results = {}
    top_k = st.session_state.get("llm_top_k", 10)

    for keyword, articles in st.session_state["search_results"].items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]

        if st.session_state.get("remove_duplicate_articles", False):
            filtered_articles = remove_duplicates(filtered_articles)

        if st.session_state.get("use_llm_filter", False):
            already_llm = (
                len(filtered_articles) <= top_k and
                all(("llm_score" in a) for a in filtered_articles)
            )
            if not already_llm:
                filtered_articles = llm_filter_and_rank_articles(keyword, filtered_articles)

        if filtered_articles:
            filtered_results[keyword] = filtered_articles

    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )

    selected_companies = []
    for cat in st.session_state.get("cat_multi", []):
        selected_companies.extend(favorite_categories.get(cat, []))
    selected_companies = list(set(selected_companies))

    kiscd_filtered = {c: kiscd_map[c] for c in selected_companies if c in kiscd_map}
    fetch_and_display_reports(kiscd_filtered)

else:
    st.info("ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
