import nltk

# 'punkt' ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# 'punkt_tab' ë‹¤ìš´ë¡œë“œ (í™˜ê²½ì— ë”°ë¼ í•„ìš”)
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')


import streamlit as st
import requests
import re
import os
from datetime import datetime
import telepot
from openai import OpenAI
import newspaper  # newspaper4k
from google.cloud import language_v1

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def analyze_sentiment_google(text):
    lang = detect_lang(text)
    try:
        client_gc = language_v1.LanguageServiceClient()
        document = language_v1.Document(
            content=text,
            type_=language_v1.Document.Type.PLAIN_TEXT,
            language=lang
        )
        response = client_gc.analyze_sentiment(request={"document": document})
        score = response.document_sentiment.score
        if score > 0.05:
            return "ê¸ì •"
        elif score < -0.05:
            return "ë¶€ì •"
        else:
            return "ì¤‘ë¦½"
    except Exception as e:
        return f"ë¶„ì„ì‹¤íŒ¨: {e}"

# --- newspaper4kë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ---
def extract_article_text(url):
    try:
        article = newspaper.article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

def summarize_with_openai(text):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            "ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜.\n"
            "ë‹¨, ê¸°ì‚¬ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ ì—†ëŠ” ê´‘ê³ , ë°°ë„ˆ, ì¶”ì²œê¸°ì‚¬, ì„œë¹„ìŠ¤ ì•ˆë‚´, ì‚¬ì´íŠ¸ ê³µí†µ ë¬¸êµ¬ ë“±ì€ ëª¨ë‘ ìš”ì•½ì—ì„œ ì œì™¸í•´ì¤˜.\n"
            "ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•´ì¤˜.\n\n"
            f"[ê¸°ì‚¬ ë³¸ë¬¸]\n{text}"
        )
    else:
        prompt = (
            "Summarize the following news article in 3 sentences.\n"
            "Exclude any content that is not directly related to the article itself, such as advertisements, banners, recommended articles, service notices, or site-wide generic messages.\n"
            "Focus only on the main content of the article.\n\n"
            f"[ARTICLE]\n{text}"
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=256,
        temperature=0.3
    )
    summary = response.choices[0].message.content.strip()
    return summary, text

# --- ì´í•˜ ê¸°ì¡´ ì½”ë“œ ë™ì¼ ---
st.markdown("""
    <style>
        .block-container {padding-top: 2rem; padding-bottom: 2rem;}
        .stButton button {margin-top: 6px; margin-bottom: 6px; border-radius: 8px;}
        .stTextInput > div > div > input {font-size: 16px;}
        .stMultiSelect [data-baseweb="tag"] {
            background-color: #fff0f0 !important;
            color: #d60000 !important;
            border: 1px solid #d60000 !important;
        }
        .stMultiSelect label { color: #d60000 !important; font-weight: bold;}
        .stSelectbox, .stDateInput, .stMultiSelect {margin-bottom: 0.5rem;}
    </style>
""", unsafe_allow_html=True)

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

credit_keywords = ["ì‹ ìš©ë“±ê¸‰", "ì‹ ìš©í•˜í–¥", "ì‹ ìš©ìƒí–¥", "ë“±ê¸‰ì¡°ì •", "ë¶€ì •ì ", "ê¸ì •ì ", "í‰ê°€"]
finance_keywords = ["ì ì", "í‘ì", "ë¶€ì±„", "ì°¨ì…ê¸ˆ", "í˜„ê¸ˆíë¦„", "ì˜ì—…ì†ì‹¤", "ìˆœì´ìµ", "ë¶€ë„", "íŒŒì‚°"]
all_filter_keywords = sorted(set(credit_keywords + finance_keywords))
default_credit_issue_patterns = [
    "ì‹ ìš©ë“±ê¸‰", "ì‹ ìš©í‰ê°€", "í•˜í–¥", "ìƒí–¥", "ê°•ë“±", "ì¡°ì •", "ë¶€ë„",
    "íŒŒì‚°", "ë””í´íŠ¸", "ì±„ë¬´ë¶ˆì´í–‰", "ì ì", "ì˜ì—…ì†ì‹¤", "í˜„ê¸ˆíë¦„", "ìê¸ˆë‚œ",
    "ì¬ë¬´ìœ„í—˜", "ë¶€ì •ì  ì „ë§", "ê¸ì •ì  ì „ë§", "ê¸°ì—…íšŒìƒ", "ì›Œí¬ì•„ì›ƒ", "êµ¬ì¡°ì¡°ì •", "ìë³¸ì ì‹"
]

favorite_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": ["í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ"],
    "5ëŒ€ì‹œì¤‘ì€í–‰": ["ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"],
    "ì¹´ë“œì‚¬": ["KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"],
    "ìºí”¼íƒˆ": ["í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": ["SKì´ë…¸ë² ì´ì…˜", "GSì—ë„ˆì§€", "SK", "GS"],
    "ì—ë„ˆì§€": ["SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "SKì•¤ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"],
    "ë°œì „": ["GSíŒŒì›Œ", "GSEPS", "ì‚¼ì²œë¦¬"],
    "ìë™ì°¨": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´"],
    "ì „ê¸°/ì „ì": ["SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "LSì¼ë ‰íŠ¸ë¦­"],
    "ì†Œë¹„ì¬": ["ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"],
    "ë¹„ì² /ì² ê°•": ["í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"],
    "ì„ìœ í™”í•™": ["LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"],
    "ê±´ì„¤": ["í¬ìŠ¤ì½”ì´ì•¤ì”¨"],
    "íŠ¹ìˆ˜ì±„": ["ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"]
}

if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False

for category_keywords in favorite_categories.values():
    st.session_state.favorite_keywords.update(category_keywords)

st.markdown("**ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬ ì„ íƒ**")
cat_col, btn_col = st.columns([5, 1])
with cat_col:
    selected_categories = st.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ì¦ê²¨ì°¾ê¸° í‚¤ì›Œë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.", list(favorite_categories.keys()))
    for cat in selected_categories:
        st.session_state.favorite_keywords.update(favorite_categories[cat])
with btn_col:
    st.write("")
    category_search_clicked = st.button("ğŸ” ê²€ìƒ‰", use_container_width=True)

def filter_by_issues(title, desc, selected_keywords, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    if enable_credit_filter and not is_credit_risk_news(title + " " + desc, credit_filter_keywords):
        return False
    return True

def is_credit_risk_news(text, keywords):
    return any(kw in text for kw in keywords)

def fetch_naver_news(query, start_date=None, end_date=None, enable_credit_filter=True, credit_filter_keywords=None, limit=100, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 6):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 10,
            "start": (page - 1) * 10 + 1,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], enable_credit_filter, credit_filter_keywords, require_keyword_in_title):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
    return articles[:limit]

def fetch_gnews_news(query, enable_credit_filter=True, credit_filter_keywords=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], enable_credit_filter, credit_filter_keywords, require_keyword_in_title):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, enable_credit_filter, credit_filter_keywords, require_keyword_in_title=require_keyword_in_title)
        st.session_state.search_results[k] = articles
        st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def summarize_article_from_url(article_url, title):
    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None
        summary, _ = summarize_with_openai(full_text)
        return summary, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None

def render_articles_with_single_summary_and_telegram(results, show_limit):
    all_articles = []
    article_keys = []
    for keyword, articles in results.items():
        for idx, article in enumerate(articles[:show_limit.get(keyword, 5)]):
            all_articles.append(f"[{keyword}] {article['title']} ({article['date']} | {article['source']})")
            article_keys.append((keyword, idx))

    if not all_articles:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected_idx = st.radio("ìš”ì•½/ê°ì„±ë¶„ì„/í…”ë ˆê·¸ë¨ ì „ì†¡í•  ê¸°ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.", range(len(all_articles)), format_func=lambda i: all_articles[i], key="article_selector")
    selected_keyword, selected_article_idx = article_keys[selected_idx]
    selected_article = st.session_state.search_results[selected_keyword][selected_article_idx]

    st.markdown(f"""
    <div style='margin-bottom: 10px; padding: 10px; border: 1px solid #eee; border-radius: 10px; background-color: #fafafa;'>
        <div style='font-weight: bold; font-size: 15px; margin-bottom: 4px;'>
            <a href="{selected_article['link']}" target="_blank" style='text-decoration: none; color: #1155cc;'>
                {selected_article['title']}
            </a>
        </div>
        <div style='font-size: 12px; color: gray;'>
            {selected_article['date']} | {selected_article['source']}
        </div>
    </div>
    """, unsafe_allow_html=True)

    if st.button("ğŸ” ì„ íƒ ê¸°ì‚¬ ìš”ì•½ ë° ê°ì„±ë¶„ì„"):
        with st.spinner("ê¸°ì‚¬ ìš”ì•½ ì¤‘..."):
            summary, full_text = summarize_article_from_url(selected_article['link'], selected_article['title'])
            if full_text:
                st.markdown("<div style='font-size:14px; font-weight:bold;'>ğŸ” ë³¸ë¬¸ ìš”ì•½:</div>", unsafe_allow_html=True)
                st.write(summary)
                sentiment = analyze_sentiment_google(full_text)
                st.markdown(f"<div style='font-size:14px; font-weight:bold;'>ğŸ§­ ê°ì„± ë¶„ì„: <span style='color:#d60000'>{sentiment}</span></div>", unsafe_allow_html=True)
            else:
                st.warning(summary)

    if st.button("âœˆï¸ ì„ íƒ ê¸°ì‚¬ í…”ë ˆê·¸ë¨ ì „ì†¡"):
        try:
            msg = f"*[{selected_article['title']}]({selected_article['link']})*\n{selected_article['date']} | {selected_article['source']}"
            Telegram().send_message(msg)
            st.success("í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except Exception as e:
            st.warning(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜¤ë¥˜: {e}")

st.set_page_config(layout="wide")
st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)

col1, col2, col3 = st.columns([6, 1, 1])
with col1:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="", on_change=lambda: st.session_state.__setitem__('search_triggered', True))
with col2:
    st.write("")
    search_clicked = st.button("ê²€ìƒ‰", use_container_width=True)
with col3:
    st.write("")
    fav_add_clicked = st.button("â­ ì¦ê²¨ì°¾ê¸° ì¶”ê°€", use_container_width=True)
    if fav_add_clicked:
        new_keywords = {kw.strip() for kw in keywords_input.split(",") if kw.strip()}
        st.session_state.favorite_keywords.update(new_keywords)
        st.success("ì¦ê²¨ì°¾ê¸°ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼")
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼")

with st.expander("ğŸ›¡ï¸ ì‹ ìš©ìœ„í—˜ í•„í„° ì˜µì…˜", expanded=True):
    enable_credit_filter = st.checkbox("ì‹ ìš©ìœ„í—˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§", value=False)
    credit_filter_keywords = st.multiselect(
        "ì‹ ìš©ìœ„í—˜ ê´€ë ¨ í‚¤ì›Œë“œ (í•˜ë‚˜ ì´ìƒ ì„ íƒ)",
        options=default_credit_issue_patterns,
        default=default_credit_issue_patterns,
        key="credit_filter"
    )

with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜", expanded=True):
    require_keyword_in_title = st.checkbox("ê¸°ì‚¬ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë³´ê¸°", value=True)

fav_col1, fav_col2 = st.columns([5, 1])
with fav_col1:
    fav_selected = st.multiselect("â­ ì¦ê²¨ì°¾ê¸°ì—ì„œ ê²€ìƒ‰", sorted(st.session_state.favorite_keywords))
with fav_col2:
    st.write("")
    fav_search_clicked = st.button("ì¦ê²¨ì°¾ê¸°ë¡œ ê²€ìƒ‰", use_container_width=True)

search_clicked = False

if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, enable_credit_filter, credit_filter_keywords)
    st.session_state.search_triggered = False

if fav_search_clicked and fav_selected:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        process_keywords(fav_selected, start_date, end_date, enable_credit_filter, credit_filter_keywords)

if category_search_clicked and selected_categories:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        keywords = set()
        for cat in selected_categories:
            keywords.update(favorite_categories[cat])
        process_keywords(
            sorted(keywords),
            start_date,
            end_date,
            enable_credit_filter,
            credit_filter_keywords,
            require_keyword_in_title
        )

if st.session_state.search_results:
    render_articles_with_single_summary_and_telegram(st.session_state.search_results, st.session_state.show_limit)
